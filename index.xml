<?xml version="1.0"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Factual Audio</title>
    <link>https://factualaudio.com/</link>
    <description>Recent content on Factual Audio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>etienne@edechamps.fr (Etienne Dechamps)</managingEditor>
    <webMaster>etienne@edechamps.fr (Etienne Dechamps)</webMaster>
    <copyright>Etienne Dechamps</copyright>
    <lastBuildDate>Sun, 25 Mar 2018 20:35:00 +0100</lastBuildDate>
    
        <atom:link href="https://factualaudio.com/index.xml" rel="self" type="application/rss+xml"/>
    
    
    <item>
      <title>Acoustical interference</title>
      <link>https://factualaudio.com/post/interference/</link>
      <pubDate>Sun, 25 Mar 2018 20:35:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/interference/</guid>
      <description>
&lt;p&gt;Previously, I described how audio signals can &lt;a href="https://factualaudio.com/post/sum/"&gt;sum together&lt;/a&gt; and produce various results depending on &lt;a href="https://factualaudio.com/post/anatomy/"&gt;frequency&lt;/a&gt;, &lt;a href="https://factualaudio.com/post/amplitude/"&gt;amplitude&lt;/a&gt; and &lt;a href="https://factualaudio.com/post/phase/"&gt;phase&lt;/a&gt;. We’ve seen that these rules apply in any &lt;a href="https://factualaudio.com/post/life/"&gt;realm&lt;/a&gt; — digital, analog, or acoustic — as long as the medium is &lt;a href="https://en.wikipedia.org/wiki/Linear_system"&gt;linear&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is pretty much the end of the story for the digital and analog realms, where the signal can simply be represented as a variation of amplitude over time, i.e. a single &lt;a href="https://en.wikipedia.org/wiki/Waveform"&gt;waveform&lt;/a&gt;. The acoustic realm, on the other hand, doesn’t just have a &lt;em&gt;time&lt;/em&gt; dimension, it has &lt;em&gt;spatial&lt;/em&gt; dimensions as well. So far, when discussing wave summation and interference we’ve been ignoring these spatial dimensions, instead focusing on &lt;a href="https://factualaudio.com/post/life/#the-acoustic-realm"&gt;sound pressure&lt;/a&gt; at a &lt;em&gt;single point in space&lt;/em&gt; only. Just like the &lt;a href="https://factualaudio.com/post/distance/"&gt;previous post&lt;/a&gt; focused on how distance in space can affect the amplitude of a signal, it seems appropriate to discuss the concepts of summation and interference in a spatial context.&lt;/p&gt;
&lt;p&gt;In this post we’ll look at the interaction between two spatially separated sound sources that, for the sake of simplicity, are both radiating the same 250&amp;#160;Hz sine wave. We’ll use a 2D projection because 3D space doesn’t matter too much for this post and it makes things easier to visualize. Furthermore, just like in the previous post, we’re going keep things clear and focused by assuming &lt;a href="https://en.wikipedia.org/wiki/Free_field_(acoustics)"&gt;free field&lt;/a&gt; conditions, i.e. sound propagates unimpeded without meeting any obstacles such as walls.&lt;/p&gt;
&lt;h1 id="two-sources-one-listener"&gt;Two sources, one listener&lt;/h1&gt;
&lt;p&gt;Let’s set the stage as, say, a 10-meter square area with some arbitrary reference listening position equidistant from both sources. Something like this:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/point-sources.fingerprint-e03e2d6.png" alt="Two point sources" data-svg-alternative="https://factualaudio.com/plots/point-sources.fingerprint-2ab47d5.svg"&gt;&lt;/span&gt;
&lt;p&gt;As one might expect, the sound from sources A and B is going to sum at the listening position. To understand what’s going to happen there, we can refer back to the &lt;a href="https://factualaudio.com/post/sum/"&gt;summation rules&lt;/a&gt; we’ve discussed in a previous post. Since the two initial waves are sine waves, we know the result is going to be a sine wave too, and the resulting amplitude and phase will depend on the amplitude and phase of the two waves.&lt;/p&gt;
&lt;p&gt;So what is the amplitude and phase of the two constituent waves? Well, for starters, it depends on the characteristics of the sources. For the sake of simplicity we could assume the two sources are identical; for example they could be two closely-matched units of the same loudspeaker model. In fact, we’re going to go one step further and assume that the sources are not only identical but also &lt;em&gt;omnidirectional&lt;/em&gt;, i.e. they radiate sound in the same way in all directions. This way we don’t have to worry about changes in amplitude and phase depending on the angle at which the listener is facing the source. In a real-world situation we wouldn’t be able to get away with these assumptions, but they don’t really affect the basic principles described in this post; they just make it easier to calculate the results.&lt;/p&gt;
&lt;p&gt;Now that the source characteristics are out of the way, we have to consider what happens between the sources and the listener. Here the situation is simple — since the two waves are propagating through the same medium and the listener is sitting at equal distance from both sources, there is no reason to believe their &lt;em&gt;relative&lt;/em&gt; amplitude and phase would be affected.&lt;/p&gt;
&lt;p&gt;From there we can deduce that both waves will arrive at the listener with the same amplitude and phase, and we can apply the &lt;a href="https://factualaudio.com/post/sum/#same-frequency-same-phase"&gt;appropriate rule&lt;/a&gt; to conclude that the two waves are going to &lt;em&gt;interfere constructively&lt;/em&gt; at the listening position.&lt;/p&gt;
&lt;p&gt;So far so good. What if we move the listener to the side?&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/point-sources-lateral.fingerprint-8ea14c8.png" alt="Two point sources, lateral listener" data-svg-alternative="https://factualaudio.com/plots/point-sources-lateral.fingerprint-6599e74.svg"&gt;&lt;/span&gt;
&lt;p&gt;In that case the listener is not sitting at equal distance from A and B anymore. This has two immediate consequences. First, due to the &lt;a href="https://factualaudio.com/post/distance/#three-dimensions"&gt;inverse square law&lt;/a&gt;, the wave from B will have higher amplitude than the wave from A at the listening position. However, because the distances are still somewhat similar in this example, the difference is benign — a mere ~1&amp;#160;dB. For this reason, we’re going to ignore this effect for now.&lt;/p&gt;
&lt;p&gt;More importantly, we should point out that the &lt;a href="https://en.wikipedia.org/wiki/Speed_of_sound"&gt;speed of sound&lt;/a&gt; is not infinite, which means sound from B will arrive &lt;em&gt;earlier&lt;/em&gt; than sound from A at the listening position. In the conditions of a typical room, air travels at around 343&amp;#160;m/s. In our example, sound from A has to travel an additional 0.7&amp;#160;m, which means it will arrive around 2&amp;#160;milliseconds late.&lt;/p&gt;
&lt;p&gt;Now, such a small delay might seem inconsequential at first, but consider this: 2&amp;#160;ms is exactly &lt;em&gt;half&lt;/em&gt; the &lt;a href="https://en.wikipedia.org/wiki/Periodic_function"&gt;period&lt;/a&gt; of the 250&amp;#160;Hz sine wave we’re using in our example. Which means that when the wave from A arrives at the listening position, its &lt;a href="https://factualaudio.com/post/phase/"&gt;phase&lt;/a&gt; is exactly opposite (180°) relative to the wave from B. If we look at the &lt;a href="https://factualaudio.com/post/sum/#same-frequency-opposite-phase"&gt;summation rules&lt;/a&gt; for this scenario, we deduce that &lt;em&gt;destructive interference&lt;/em&gt; will occur, and since the amplitudes of the two waves are (almost) the same, we can conclude that there is no sound at the listening position!&lt;/p&gt;
&lt;h1 id="interference-patterns"&gt;Interference patterns&lt;/h1&gt;
&lt;p&gt;One observation that we can make at this point is that the nature of the interference depends on the difference in propagation delays between the two waves (&lt;em&gt;path difference&lt;/em&gt;). if the delay is negligible compared to the period, or if it’s close to an integer multiple of the period, we get constructive interference; at the other extreme, if it falls in the middle, we get destructive interference. We’ve also seen that, in space, due to the speed of sound, delay and distance are intricately linked. Thus our two example sources interact to create &lt;em&gt;interference patterns&lt;/em&gt; where sound amplitude and phase varies wildly from one point in space to the next. In other words, we have a complex &lt;em&gt;sound &lt;a href="https://en.wikipedia.org/wiki/Field_(physics)"&gt;field&lt;/a&gt;&lt;/em&gt;:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/point-sources-amplitude-nospreading.fingerprint-4bca97d.png" alt="Two point sources, amplitude map" data-svg-alternative="https://factualaudio.com/plots/point-sources-amplitude-nospreading.fingerprint-0d2cde9.svg"&gt;&lt;/span&gt;
&lt;p&gt;In our particular example, we can observe an alternating pattern of constructive and destructive interference. In this context, the areas of destructive interference are called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Node_(physics)"&gt;nodes&lt;/a&gt;&lt;/em&gt;, and the areas of constructive interference are called &lt;em&gt;antinodes&lt;/em&gt;. Because there is no sound amplitude at the nodes, it looks like the wave is vibrating in place at the antinodes without moving in space; hence the term &lt;em&gt;stationary&lt;/em&gt; or &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Standing_wave"&gt;standing wave&lt;/a&gt;&lt;/em&gt; to describe this phenomenon.&lt;/p&gt;
&lt;p&gt;The above plot assumes each of the two sources individually produces 0&amp;#160;dB at every point in space and ignores the inverse square law. If we instead assume that each source can produce 0&amp;#160;dB at a distance of 1&amp;#160;m and apply the inverse square law, we get a more realistic result:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/point-sources-amplitude.fingerprint-08d0901.png" alt="Two point sources, amplitude map with spreading" data-svg-alternative="https://factualaudio.com/plots/point-sources-amplitude.fingerprint-6b0d74e.svg"&gt;&lt;/span&gt;
&lt;h1 id="transient-sound-field"&gt;Transient sound field&lt;/h1&gt;
&lt;p&gt;In the example above, we’ve determined that interference occurs because the sound from one of the sources arrives 2&amp;#160;milliseconds later than the sound from the other source. I then showed the resulting sound field &lt;em&gt;after&lt;/em&gt; the sound from both sources has traversed the whole area, and the sound field has stabilized: this is called the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Steady_state"&gt;steady state&lt;/a&gt;&lt;/em&gt;. This begs the question: what about the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Transient_state"&gt;transient state&lt;/a&gt;&lt;/em&gt;, i.e. the window of time just before the sound from the second source arrives at the listener?&lt;/p&gt;
&lt;p&gt;In the post where we discussed &lt;a href="https://factualaudio.com/post/sum/"&gt;wave summation&lt;/a&gt;, I carefully avoided that question by assuming that the waves were &lt;a href="https://en.wikipedia.org/wiki/Continuous_wave"&gt;continuous&lt;/a&gt;; that is, they have no beginning and no end. Let’s depart from that assumption for a moment and assume that our sound wave has a well-defined start time: our &lt;a href="https://en.wikipedia.org/wiki/Initial_condition"&gt;initial condition&lt;/a&gt; is that the sound field is completely silent, and our two ideal 250&amp;#160;Hz sine wave sources are switched on simultaneously at t = 0. To make things more obvious, I also rearranged the sources to increase the path difference:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/point-sources-amplitude-transient.fingerprint-fb9d268.png" alt="Two point sources, one closer" data-svg-alternative="https://factualaudio.com/plots/point-sources-amplitude-transient.fingerprint-41454a3.svg"&gt;&lt;/span&gt;
&lt;p&gt;Initially, the listener is not hearing anything because the sound from the first source has not reached it yet. At around 8&amp;#160;ms, the sound from the first source has arrived and the listener starts hearing a wave of about -9&amp;#160;dB amplitude. At around 18&amp;#160;ms, the sound from the second source arrives, interferes with the sound from the first source, and the sound level at the listening position drops to about -14&amp;#160;dB. Shown differently, this is the sound that reaches the listener:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/point-sources-amplitude-transient-waveform.fingerprint-5d420d3.png" alt="Two point sources, one closer (listener waveform)" data-svg-alternative="https://factualaudio.com/plots/point-sources-amplitude-transient-waveform.fingerprint-0bd88a6.svg"&gt;&lt;/span&gt;
&lt;p&gt;If our listener was a microphone, the above shows the waveform that it would capture. But in the real world we don’t look at sounds — we hear them. This immediately raises a number of very important questions. What if the listener is not a microphone but a human being? &lt;em&gt;What would they hear&lt;/em&gt; in this scenario? Would they notice the transient state at all? Would they hear a difference if we only used a single source of similar steady-state amplitude?&lt;/p&gt;
&lt;p&gt;These questions are not about physics anymore — they’re about &lt;em&gt;perception&lt;/em&gt;. In other words, we are stepping into the field of &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Psychoacoustics"&gt;psychoacoustics&lt;/a&gt;&lt;/em&gt;. The answers to these questions depend on many factors, such as frequency, transient state duration, and relative amplitude. They also depend on the shape of the signal: if, in our example, we used a very short, &lt;a href="https://en.wikipedia.org/wiki/Dirac_delta_function"&gt;impulse&lt;/a&gt;-like signal instead of a sine wave, then the sound from the first source would have ended before the sound from the second source reaches the listener. In such a scenario there is no interference at all, the listener hears the same signal twice, and there is no steady state to speak of! As if that wasn’t enough, perception is also affected by the &lt;em&gt;angle&lt;/em&gt; at which sound arrives at the listener. &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Toole, Floyd E., &lt;em&gt;&lt;a href="https://books.google.co.uk/books?id=sGmz0yONYFcC"&gt;Sound Reproduction: Loudspeakers and Rooms&lt;/a&gt;&lt;/em&gt;, chapter 7.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; Needless to say, this is a vast topic to which we will come back in more detail in future posts.&lt;/p&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; When making your own acoustical measurements using a microphone and an analyzer, you should always keep in mind that, by default, most of the results that are displayed (in particular the frequency response) &lt;em&gt;are only valid for the steady state&lt;/em&gt;. For this reason you should exercise extreme caution when interpreting these measurements and drawing conclusions from them with regard to how the system will sound like to a human listener. This can be quite misleading, especially at high frequencies, and especially if the system under test has long transient states (such as a room, where sound travels distances measured in meters). More details in later posts.&lt;/p&gt;
&lt;h1 id="the-wavelength"&gt;The wavelength&lt;/h1&gt;
&lt;p&gt;I mentioned above that, in the acoustic realm, &lt;em&gt;delay&lt;/em&gt; (or duration) and &lt;em&gt;distance&lt;/em&gt; are two sides of the same coin: distance differences cause delay differences which, relative to the period of the wave, cause interference patterns. We can simplify this story by sidestepping the concept of delay entirely; instead, we could choose to only deal with distances.&lt;/p&gt;
&lt;p&gt;In order to do that, we need to convert the wave period, which is a duration, into a distance. Using the speed of sound, we can compute the distance that sound travels in the duration of one period; this is called the &lt;em&gt;spatial period&lt;/em&gt;, or more commonly, the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Wavelength"&gt;wavelength&lt;/a&gt;&lt;/em&gt;. The formula is very straightforward: the wavelength is the period multiplied by speed of sound (or speed of sound divided by frequency). As usual, &lt;a href="http://www.sengpielaudio.com/calculator-waves.htm"&gt;calculators&lt;/a&gt; are available.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/wavelength.fingerprint-7ec1514.png" alt="Wavelength" data-svg-alternative="https://factualaudio.com/plots/wavelength.fingerprint-87adf25.svg"&gt;&lt;/span&gt;
&lt;p&gt;Using the wavelength, we can rephrase the interference phenomena described in the previous section purely in terms of distances. For example, we can say that if the distance difference between the listener and the two sources is close to an integer multiple of the wavelength, then constructive interference will occur. Furthermore, the wavelength of a 250&amp;#160;Hz wave is ~1.4&amp;#160;m, which explains why, in our initial example, destructive interference occurs when the distance difference approaches half that wavelength (0.7&amp;#160;m).&lt;/p&gt;
&lt;h1 id="distance-between-sources"&gt;Distance between sources&lt;/h1&gt;
&lt;p&gt;We’ve observed above that sound from two spatially separated sources can combine to form an alternating constructive and destructive interference pattern (i.e. a standing wave). This is true for the specific example that I’ve chosen, but things do not necessarily turn out that way in the general case. Indeed the sound field might adopt a different shape if any of our initial assumptions (i.e. &lt;em&gt;identical&lt;/em&gt;, &lt;em&gt;omnidirectional&lt;/em&gt; sources) are broken.&lt;/p&gt;
&lt;p&gt;More interestingly, the distance between our two sources also matters. Remember that this particular interference pattern is inherently caused by the path difference between the sources and the listener — if the path difference approaches half a wavelength, destructive interference occurs. Consider this: basic geometry tells us that the path difference cannot be more than the distance between the sources themselves. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This maximum is reached if the listener sits on the line that goes through both sources, such that one source lies behind the other from the perspective of the listener.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; If that distance is less than a quarter of a wavelength, then destructive interference simply &lt;em&gt;cannot&lt;/em&gt; occur anywhere in space, and we are left with only constructive interference:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/point-sources-amplitude-close.fingerprint-6f99905.png" alt="Two close point sources" data-svg-alternative="https://factualaudio.com/plots/point-sources-amplitude-close.fingerprint-d77bc63.svg"&gt;&lt;/span&gt;
&lt;p&gt;From this we can conclude that interference patterns tend to disappear if the sources are moved closer to each other. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This is why two &lt;a href="https://en.wikipedia.org/wiki/Subwoofer"&gt;subwoofers&lt;/a&gt; (low frequency, long wavelength) will act just like a single subwoofer with twice the output if they are positioned right next to each other. Things are not that simple at higher frequencies, because the wavelengths are shorter.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Since wavelength is inversely proportional to frequency, lowering the frequency has the same effect.&lt;/p&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; Two sound sources can interact in very different ways depending on the wavelengths (frequencies) involved. This is important because the audible range spans a huge range of wavelengths: from 17&amp;#160;meters (20&amp;#160;Hz) down to 17&amp;#160;millimeters (20&amp;#160;kHz). This in turn means that the shape of the sound field can vary wildly depending on which part of the frequency spectrum we are looking at. For this reason it is &lt;em&gt;imperative&lt;/em&gt; to clearly state the frequency range of interest when discussing acoustical phenomena in real-world situations — otherwise the discussion makes no sense.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Keeping our distance</title>
      <link>https://factualaudio.com/post/distance/</link>
      <pubDate>Wed, 28 Feb 2018 22:53:00 +0000</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/distance/</guid>
      <description>
&lt;p&gt;This post will focus on a specific aspect of the &lt;a href="https://factualaudio.com/post/life/#the-acoustic-realm"&gt;acoustic realm&lt;/a&gt;: specifically, how the &lt;a href="https://factualaudio.com/post/amplitude/"&gt;amplitude&lt;/a&gt; of a sound wave is altered as it travels through free space. This is useful to gain a better understanding of how loudspeaker systems behave; naturally, it is not as useful when discussing headphones, as in this case there is not much space for sound to propagate into.&lt;/p&gt;
&lt;p&gt;To make things easier to understand, I will use a simple unrealistic scenario as a basic starting point. Then, I will elaborate on this scenario so that we get closer and closer to a realistic model of how sound propagates in the real world. Here are our initial assumptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Sound is propagating through a perfect lossless medium.&lt;/em&gt; In other words, the medium (in our case, air) opposes no resistance to the passage of sound, and there is no dissipation of sound energy, such as in the form of heat. This is not completely true in practice, and we will revisit this assumption later in this post.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sound is coming from a single, small point in space&lt;/em&gt;, not from multiple points at the same time. Such a source is called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Point_source"&gt;point source&lt;/a&gt;&lt;/em&gt;. Perfect point sources don’t exist in the real world, but are a good approximation as long the listener sits in the &lt;em&gt;far field&lt;/em&gt;; that is to say, the distance to the source is large compared to the size of the source itself. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;If we can’t make that assumption, then an alternative is to decompose a complex source into a number of individual point sources, the sound from which &lt;a href="https://factualaudio.com/post/sum/"&gt;sum&lt;/a&gt; together at the listening position; this approach is known as the &lt;a href="https://en.wikipedia.org/wiki/Huygens%E2%80%93Fresnel_principle"&gt;Huygens-Fresnel principle&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Sound does not reflect off nearby surfaces.&lt;/em&gt; Think of the sound source as levitating in mid-air, with no floor, no walls, no ceiling, basically nothing for sound to bounce off of. This situation is known as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Free_field_(acoustics)"&gt;free field&lt;/a&gt;&lt;/em&gt; or &lt;em&gt;free space&lt;/em&gt; conditions. Strictly speaking this is implied by the previous assumption, since after all a reflection is itself a sound source — thus the sound would not be coming from a single point anymore.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The latter assumption is of course egregiously wrong when sound propagates in a room, which is the main reason why the scenarios described in this post are somewhat unrealistic. The rules described below should be seen as building blocks to gain a better understanding of how sound propagates through space; they can’t be applied as-is for sound waves propagating through a room where reflections are present. We will tackle that problem in a later post.&lt;/p&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; Keep in mind that &lt;a href="https://en.wikipedia.org/wiki/Sound_pressure"&gt;sound pressure&lt;/a&gt;, the metric used to quantify the amplitude of sound in the acoustic realm, refers to pressure variations &lt;em&gt;at a given point in space&lt;/em&gt;. It does &lt;em&gt;not&lt;/em&gt; refer to some overall amount of &lt;a href="https://en.wikipedia.org/wiki/Sound_power"&gt;sound power&lt;/a&gt; that is being &lt;em&gt;radiated away&lt;/em&gt; from a source. Local sound pressure depends on where the listener is; in contrast, total sound power is a property of the source only. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;When a specific sound pressure level is mentioned in the specifications of a sound source (e.g. a loudspeaker), it is always relative to a specific distance from the source (typically 1&amp;#160;meter), at a specific angle (typically &lt;em&gt;on-axis&lt;/em&gt;, i.e. 0°), under specific conditions (typically &lt;em&gt;half-space&lt;/em&gt;, i.e. with the loudspeaker against a wall). Such a number is valid for this point only.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="one-dimension"&gt;One dimension&lt;/h1&gt;
&lt;p&gt;To start with, let’s consider the case where sound is propagating down a narrow tube. In fact, as an approximation we will assume for the sake of the discussion that the tube is &lt;em&gt;infinitely&lt;/em&gt; narrow, such that we don’t have to deal with any reflections off the sides of the tube. (This might seem like a strange place to start, but bear with me for a moment.) We could simulate such a scenario by attaching a tube to a loudspeaker, like so:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/diagrams/inverse-square-law-line.fingerprint-f508015.png" alt="Distance on a line"&gt;&lt;/span&gt;
&lt;p&gt;A more entertaining analogy is the &lt;a href="https://en.wikipedia.org/wiki/Tin_can_telephone"&gt;tin can telephone&lt;/a&gt;, where the propagation medium is not air, but a piece of string.&lt;/p&gt;
&lt;p&gt;In such a setup, the problem is one-dimensional: there is nowhere the sound can go but down the line. Furthermore, since we’ve assumed earlier that the medium is lossless, there really is no reason for any energy to get lost as sound propagates down the tube. Therefore the sound pressure at points A and B (as well as any other point along the tube) is equal. It’s as simple as that.&lt;/p&gt;
&lt;h1 id="two-dimensions"&gt;Two dimensions&lt;/h1&gt;
&lt;p&gt;Let’s go one step further and add a second spatial dimension into the mix. One could imagine that our loudspeaker is sandwiched between two parallel planes, where the space between the two planes is infinitesimal:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/diagrams/inverse-square-law-circle.fingerprint-ee1def0.png" alt="Distance over a circle"&gt;&lt;/span&gt;
&lt;p&gt;Another possible analogy is a vibrating piece of paper — a two-dimensional upgrade to the tin can telephone from the previous section.&lt;/p&gt;
&lt;p&gt;In this setup things get more complicated, because the loudspeaker is radiating sound in many directions at once. This can be conceptualized as expanding circles called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Wavefront"&gt;wavefronts&lt;/a&gt;&lt;/em&gt;. As a wavefront gets farther and farther away from the sound source, its circumference increases. However, energy does not magically appear out of thin air as this happens, so the same amount of sound energy gets spread over a larger circle. In the diagram above, the wavefronts represented by the inner and outer circles carry the same amount of energy &lt;em&gt;in total&lt;/em&gt;, but since the outer circle is larger, the sound pressure &lt;em&gt;at a given point&lt;/em&gt; on the circle is lower. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This reasoning does not require that the source be &lt;em&gt;omnidirectional&lt;/em&gt;, i.e. that it radiates sound equally in all directions. In other words, sound energy is not necessarily &lt;em&gt;evenly&lt;/em&gt; distributed along the circle. In general, loudspeakers are &lt;em&gt;not&lt;/em&gt; omnidirectional; at higher frequencies, they will typically radiate more sound out the front than out the back. Directivity makes no difference as far as this post is concerned, but it is an important concept in other contexts.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We can easily quantify the rate at which sound pressure decreases as the wavefront gets wider. In the above example, the distance between the source and point A is exactly half the distance to point B. In other words, the radius of the outer circle is double the radius of the inner circle. The circumference of a circle is proportional to its radius, so doubling the radius is equivalent to doubling the circumference. Doubling the circumference means that a given amount of sound power gets spread over twice the area, which in turn means that the sound power per unit area (i.e. the &lt;a href="https://en.wikipedia.org/wiki/Sound_intensity"&gt;sound intensity&lt;/a&gt;) is halved. Sound intensity varies with the square of sound pressure, &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This is because sound intensity is the product of sound pressure and &lt;a href="https://en.wikipedia.org/wiki/Particle_velocity"&gt;particle velocity&lt;/a&gt;, and particle velocity is proportional to sound pressure. As a result sounds pressure is multiplied with itself. This is analogous to electric power dissipated in a resistor being proportional to the square of voltage.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; therefore the ratio of sound pressure between the two circles is the square root of 2, or 3&amp;#160;dB.&lt;/p&gt;
&lt;p&gt;This leads us to conclude that, when sound propagates along two spatial dimensions, and keeping in mind the important assumptions we’ve made so far, &lt;em&gt;a doubling of the distance to the sound source results in a 3&amp;#160;dB drop in sound pressure level&lt;/em&gt;.&lt;/p&gt;
&lt;h1 id="three-dimensions"&gt;Three dimensions&lt;/h1&gt;
&lt;p&gt;Finally, let’s enter the real world and in its three spatial dimensions. As one would expect, our circles are now spheres:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/diagrams/inverse-square-law-sphere.fingerprint-c487806.png" alt="Distance over a sphere"&gt;&lt;/span&gt;
&lt;p&gt;From this point forward the reasoning is very similar to the two-dimensional case, except this time we’re dealing with surfaces, not lines. The surface of a sphere is proportional to the &lt;em&gt;square&lt;/em&gt; of the radius, so doubling the radius is equivalent to quadrupling the surface. Quadrupling the surface means that a given amount of sound power gets spread over an area four times as large, which in turn means the sound intensity is divided by 4. Therefore, the ratio of sound pressure between the two spheres is the square root of 4, which is, well, 2, or 6&amp;#160;dB.&lt;/p&gt;
&lt;p&gt;This leads us to conclude that, keeping in mind the important assumptions we’ve made so far, &lt;em&gt;a doubling of the distance to the sound source results in a 6&amp;#160;dB drop in sound pressure level (SPL)&lt;/em&gt;. This fundamental result, which applies not just to sound waves but to various other physical phenomena, is called the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Inverse-square_law"&gt;inverse-square law&lt;/a&gt;&lt;/em&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This is because intensity decreases with the square of the distance. Strictly speaking, since we’re talking about pressure this should be called the &lt;a href="https://en.wikipedia.org/wiki/Sound_pressure#Inverse-proportional_law"&gt;inverse-proportional law&lt;/a&gt;, but that term is seldom used in practice.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; &lt;a href="http://www.sengpielaudio.com/calculator-distance.htm"&gt;Calculators&lt;/a&gt; exist to compute the drop for any arbitrary distance ratio.&lt;/p&gt;
&lt;h1 id="air-attenuation"&gt;Air attenuation&lt;/h1&gt;
&lt;p&gt;At the beginning of this post, we’ve made the assumption that sound is propagating through a lossless medium. But &lt;a href="https://factualaudio.com/post/life/#the-acoustic-realm"&gt;remember&lt;/a&gt; that sound is a pressure wave; by definition, its propagation involves the movement of particles that comprise the medium. In the case of air, that’s mostly oxygen and nitrogen. Because such movement goes against the natural equilibrium of the medium, it requires an expenditure of energy, which is dissipated as heat. As a result, the total energy carried by a sound wave tends to decrease as it travels through the air, &lt;em&gt;in addition&lt;/em&gt; to the loss of sound pressure caused by the inverse-square law described in the previous section. &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;em&gt;Engineering Acoustics, &lt;a href="https://en.wikibooks.org/wiki/Engineering_Acoustics/Attenuation_of_Sound_Waves"&gt;“Attenuation of sound waves”&lt;/a&gt;&lt;/em&gt;; Smith, Julius O., &lt;em&gt;Physical Audio Signal Processing, &lt;a href="https://ccrma.stanford.edu/~jos/pasp/Air_Absorption.html"&gt;“Air Absorption”&lt;/a&gt;&lt;/em&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Contrary to what some may believe, this doesn’t seem to have anything to do with “loops of energy” that somehow &lt;a href="https://www.quora.com/If-energy-cannot-be-destroyed-then-where-does-sound-energy-go-after-we-speak/answer/Abhishek-Bisht-12"&gt;“supply the body with vital life force”&lt;/a&gt;, however inspiring that might be.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The physical phenomena at play here are non-trivial. The precise amount of sound absorption per unit of distance traveled in air depends on ambient pressure, temperature and humidity, requiring the use of a &lt;a href="http://www.sengpielaudio.com/calculator-air.htm"&gt;calculator&lt;/a&gt;. Most importantly, it increases with frequency, which means, somewhat shockingly, that &lt;em&gt;air itself&lt;/em&gt; exhibits &lt;a href="https://factualaudio.com/post/distortion/#frequency-response-distortion"&gt;frequency response distortion&lt;/a&gt;. Here’s how things look like for various atmospheric conditions: &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Calculated according to &lt;a href="https://www.iso.org/standard/17426.html"&gt;ISO 9613–1:1993&lt;/a&gt;, &lt;em&gt;Acoustics — Attenuation of sound during propagation outdoors — Part 1: Calculation of the absorption of sound by the atmosphere&lt;/em&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt;&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/air-attenuation.fingerprint-d109341.png" alt="Air attenuation" data-svg-alternative="https://factualaudio.com/plots/air-attenuation.fingerprint-b31c23c.svg"&gt;&lt;/span&gt;
&lt;p&gt;Immediately, we can observe that below a few kHz, there is not much attenuation to speak of; it is practically negligible at low and medium frequencies. This should dispel the common misconception that attenuation of sound with distance is due to absorption by the air — in reality, that phenomenon is dwarfed by the inverse-square law that we’ve discussed above.&lt;/p&gt;
&lt;p&gt;On the other hand, absorption cannot be neglected for high frequencies. In conditions typical of a living room (20 °C, 50% relative humidity), a 10&amp;#160;kHz sound wave will be down ~1.5&amp;#160;dB after 10&amp;#160;meters in addition to the inverse square law and other phenomena. It’s small, but it’s not zero, and it tends to go downhill fast over the last octave of the audible frequency range.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Mixing waves</title>
      <link>https://factualaudio.com/post/sum/</link>
      <pubDate>Tue, 30 Jan 2018 22:48:00 +0000</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/sum/</guid>
      <description>
&lt;p&gt;It is very common in real-life systems to see signals being combined in a process known as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Electronic_mixer#Additive_mixers"&gt;additive mixing&lt;/a&gt;&lt;/em&gt;, sometimes referred to as simply &lt;em&gt;mixing&lt;/em&gt;, &lt;em&gt;summation&lt;/em&gt;, &lt;em&gt;addition&lt;/em&gt; or &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Interference_(wave_propagation)"&gt;interference&lt;/a&gt;&lt;/em&gt;. Examples include your computer playing a notification sound on top of your music or reducing the number of channels from an audio stream before playback (&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Audio_mixing_(recorded_music)#Mixdown"&gt;downmixing&lt;/a&gt;&lt;/em&gt;); however the most complex — and most intriguing — examples come from the &lt;a href="https://factualaudio.com/post/life/#the-acoustic-realm"&gt;acoustic realm&lt;/a&gt;, in which sound from multiple sources combine at a given point in space. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;In real-world home audio systems this happens &lt;em&gt;literally all the time&lt;/em&gt;. If you’re not convinced, consider that sound bounces off walls, and the interference that this produces has huge implications for acoustics. But that’s a story for another post.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the sake of this discussion, we’ll assume that the system that is combining these signals is, for all intents and purposes, &lt;a href="https://en.wikipedia.org/wiki/Linear_system"&gt;linear&lt;/a&gt;. In the digital and analog realms this a safe assumption to make as long as &lt;a href="https://factualaudio.com/post/distortion/#non-linear-distortion"&gt;non-linear distortion&lt;/a&gt; is kept in check. In particular the peak amplitude of the resulting signal needs to be low enough to avoid &lt;a href="https://en.wikipedia.org/wiki/Clipping_(signal_processing)"&gt;clipping&lt;/a&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;In fact, you might be compelled to use the calculations described below precisely to ensure that you &lt;em&gt;won’t&lt;/em&gt; drive your system into clipping; indeed this is a common concern when mixing signals digitally or electronically.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; To a much lesser extent, this also applies to the acoustic realm: air can be assumed to be linear insofar as we’re not dealing with extreme eardrum-busting pressures. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;The textbook case where that assumption breaks is when large amounts of sound energy are forced down a narrow tube. One example is the throat (i.e. the loudspeaker-facing part) of the &lt;a href="https://en.wikipedia.org/wiki/Horn_(acoustic)"&gt;horn&lt;/a&gt; in a &lt;a href="https://en.wikipedia.org/wiki/Horn_loudspeaker"&gt;horn loudspeaker&lt;/a&gt;, which is subject to enormous dynamic pressures from the &lt;a href="https://en.wikipedia.org/wiki/Compression_driver"&gt;compression driver&lt;/a&gt;. (See &lt;a href="https://books.google.com/books?id=7lUFu9NgM00C&amp;amp;pg=PA392&amp;amp;lpg=PA392"&gt;Kleiner&lt;/a&gt;, &lt;a href="https://books.google.co.uk/books?id=z6_hBwAAQBAJ&amp;amp;pg=PA200&amp;amp;lpg=PA200"&gt;Eargle&lt;/a&gt;, &lt;a href="https://archive.org/details/bstj14-1-159"&gt;Thuras&lt;/a&gt;.)&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What’s nice about linear systems is that they follow the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Superposition_principle"&gt;superposition principle&lt;/a&gt;&lt;/em&gt;: summing two signals can be done, quite literally, by taking the values of both &lt;a href="https://factualaudio.com/post/anatomy/"&gt;waveforms&lt;/a&gt; at a given point in time and adding one to the other. It’s as simple as that (just don’t forget about the sign). That being said, it would be nice to have a general idea of what to expect when summing waves of specific shapes. This is especially important in cases where the two signals being mixed might be related to each other, as is often the case. I’ll start with very simple scenarios and make my way to the less trivial cases.&lt;/p&gt;
&lt;h1 id="same-frequency-same-phase"&gt;Same frequency, same phase&lt;/h1&gt;
&lt;p&gt;Let’s say we want to sum two pure sine waves of identical &lt;em&gt;frequency&lt;/em&gt; and &lt;em&gt;&lt;a href="https://factualaudio.com/post/phase/"&gt;phase&lt;/a&gt;&lt;/em&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;It is important that the frequencies match exactly, otherwise you will end up with a surprising (and counter-intuitive) pattern called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Beat_(acoustics)"&gt;beat&lt;/a&gt;&lt;/em&gt;. Fortunately this is rarely a problem in practice because in a typical home audio system all signals use a consistent timing reference (often originating from a single &lt;a href="https://en.wikipedia.org/wiki/Clock_signal"&gt;digital clock signal&lt;/a&gt;), which means their frequencies are always in precise lockstep.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-constructive-interference.fingerprint-b20172a.png" alt="Constructive interference" data-svg-alternative="https://factualaudio.com/plots/sine-wave-constructive-interference.fingerprint-05bcb3c.svg"&gt;&lt;/span&gt;
&lt;p&gt;The above plot illustrates the fact that when two sines of identical frequency are mixed together, the result is &lt;em&gt;always&lt;/em&gt; a sine of that same frequency. &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;This follows from &lt;a href="https://en.wikipedia.org/wiki/Phasor#Addition"&gt;mathematical properties of sines&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; Furthermore, in this particular example, the peaks of the two sine waves are aligned because they have the same phase. Since they’re aligned, it follows naturally that the peak &lt;a href="https://factualaudio.com/post/amplitude/"&gt;amplitude&lt;/a&gt; of the resulting signal is simply the sum of the peak amplitudes of the two original signals.&lt;/p&gt;
&lt;p&gt;We’ve seen &lt;a href="https://factualaudio.com/post/amplitude/"&gt;previously&lt;/a&gt; that the RMS amplitude of a sine wave is simply its peak amplitude divided by a constant, so we can in turn deduce that the RMS amplitude of the resulting signal is the sum of the RMS amplitudes of the original signals. The resulting amplitude is higher than the amplitudes we started with, which is why this is called &lt;em&gt;constructive interference&lt;/em&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;The general concepts of constructive and destructive interference are not limited to sine waves. They can be used to describe the interaction between any waveforms as long as they are &lt;a href="https://en.wikipedia.org/wiki/Coherence_(physics)"&gt;coherent&lt;/a&gt;. Two sine waves of identical frequency are always coherent. Note that there seems to be a lot of confusion around the terms “coherent” and “in phase”. According to the Wikipedia definition two coherent waveforms do not necessarily have the same phase, yet the term “coherent sum” is often thrown around to specifically describe perfect constructive interference.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The sum of two sine waves that have identical frequency and phase is itself a sine wave of that same frequency and phase. The resulting amplitude (peak or RMS) is simply the sum of the amplitudes.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id="same-frequency-opposite-phase"&gt;Same frequency, opposite phase&lt;/h1&gt;
&lt;p&gt;Now let’s take the same scenario as above, but this time one of the two waves is 180° out of phase, i.e. it is inverted.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-destructive-interference.fingerprint-1fc122b.png" alt="Destructive interference" data-svg-alternative="https://factualaudio.com/plots/sine-wave-destructive-interference.fingerprint-d986bf6.svg"&gt;&lt;/span&gt;
&lt;p&gt;This time the peaks of one wave are aligned with the valleys of the other wave. This means their amplitudes subtract instead of add, which is why this is called &lt;em&gt;destructive interference&lt;/em&gt; or &lt;em&gt;cancellation&lt;/em&gt;. In the special case where the two waves have exactly the same amplitude, they cancel each other out perfectly, and the signal vanishes — the result is silence. &lt;a href="https://en.wikipedia.org/wiki/Active_noise_control"&gt;Active noise control&lt;/a&gt;, the technology used in noise-canceling headphones, is an example of this phenomenon.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The sum of two sine waves that have identical frequency and opposite phase is itself a sine wave of that same frequency. The resulting amplitude (peak or RMS) is the difference between the two amplitudes. The resulting phase is the phase of the wave with the strongest amplitude.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id="same-frequency-arbitrary-phase"&gt;Same frequency, arbitrary phase&lt;/h1&gt;
&lt;p&gt;When dealing with two sine waves of identical frequency, but neither identical nor opposite phase, we find ourselves deprived of a simple rule of thumb; there’s no avoiding the &lt;a href="http://2000clicks.com/mathhelp/GeometryTrigEquivPhaseShift.aspx"&gt;math&lt;/a&gt; in this case. However, the following plots will give you an idea as to what to expect. They describe the effect of summing two sine waves that differ by a particular amplitude ratio (line color) and phase (horizontal axis). The “same phase” case that we’ve seen in the first section is located in the middle of the horizontal axis, while its sides illustrate the “opposite phase” case that we’ve seen in the previous section.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/interference.fingerprint-352fdc7.png" alt="Interference plots" data-svg-alternative="https://factualaudio.com/plots/interference.fingerprint-2ddb3b0.svg"&gt;&lt;/span&gt;
&lt;p&gt;&lt;em&gt;The sum of two sine waves that have identical frequency is itself a sine wave of that same frequency. The resulting amplitude and phase depend on the amplitude ratio and phase of the two waves.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id="different-frequencies"&gt;Different frequencies&lt;/h1&gt;
&lt;p&gt;So far we’ve only been dealing with pure sine waves of identical frequency. What about the sum of sine waves of different frequencies?&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-sum.fingerprint-cd2f1ec.png" alt="Sum of different frequencies" data-svg-alternative="https://factualaudio.com/plots/sine-wave-sum.fingerprint-0a420ca.svg"&gt;&lt;/span&gt;
&lt;p&gt;In this case, as illustrated by the above plot, the result is &lt;em&gt;not&lt;/em&gt; a sine wave. We can see that the peak amplitude is the sum of the peak amplitudes of the two waves — here, 8 (5+3). This makes sense, because two sine waves of different frequency will, given enough time, eventually line up with each other such that their peaks are aligned, even briefly. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Here I’m assuming that the signal is of infinite duration, or at least sufficiently long that it makes little difference. If the signal is of limited duration, the peaks will not necessarily have the opportunity to line up, and the peak amplitude will therefore be lower.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RMS amplitude is less trivial. We can’t use the same trick as before because we’re not dealing with a single sine wave anymore — we’re dealing with a different wave shape. Mathematically there are many ways to derive the answer, but perhaps the most straightforward is to use the &lt;a href="https://en.wikipedia.org/wiki/Parseval's_theorem"&gt;Parseval theorem&lt;/a&gt;, which basically states that the sum of squares of a waveform is equal to the sum of the squares of the sine waves (of different frequencies) that comprise it. In the above example, the RMS amplitudes of the original sine waves are approximately 3.5&amp;#160;and 2.1, so the &lt;a href="https://en.wikipedia.org/wiki/Root_mean_square#In_waveform_combinations"&gt;RMS total&lt;/a&gt; is the square root of 12.5+4.5=17 — which is approximately 4.1.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The sum of two sine waves of different frequencies is not a sine wave. The peak amplitude of the resulting wave is the sum of the peak amplitudes of both sine waves. The RMS amplitude of the resulting wave is the RMS total of the amplitudes of both sine waves.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id="arbitrary-signals"&gt;Arbitrary signals&lt;/h1&gt;
&lt;p&gt;Since any signal can always be decomposed into a sum of sine waves using the &lt;a href="https://en.wikipedia.org/wiki/Fourier_transform"&gt;Fourier transform&lt;/a&gt;, the above section &lt;em&gt;de facto&lt;/em&gt; provides a general solution that can be applied to any pair of signals: decompose both signals, add the sine waves at each frequency using the rules from the first three sections above, sum them back using the rule from the fourth section, and you’re done. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Granted, that’s not really easier than just adding the two waveforms directly, is it — especially since computing the Fourier transform without the help of a computer is, well, extremely hard. The point was to demonstrate the variety of approaches that can be used to think about these sums. Depending on the situation, some of these techniques may be easier to apply than others.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; If you know the amplitude spectrum for the two signals that you want to sum, you might be tempted to simply add them together in the frequency domain, by summing the amplitude at each frequency. In most cases that’s incorrect, because at each frequency you are summing sine waves without knowing their respective phase — the actual result might be less than the sum of the amplitudes. The correct result can only be obtained by summing the raw complex output of the Fourier transform, which includes phase information, before extracting the amplitude information.&lt;/p&gt;
&lt;p&gt;That said, there are cases where we can get away with a simpler approach. Consider a scenario in which the two signals are completely and utterly unrelated to each other; for example, it could be two noise signals, or two separates pieces of music, or the voice of a commentator on top of a soundtrack, or a notification sound playing on top of other content. The trick in that case is to see the two signals as two independent &lt;a href="https://en.wikipedia.org/wiki/Stochastic_process"&gt;random processes&lt;/a&gt;, in the mathematical sense of the term. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;The usual caveats apply, including the simplifying assumptions that the signals are truly unpredictable and extend infinitely in time. This is an approximation, albeit one that works well in practice.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From that perspective, peak amplitude is intuitively the sum of peak amplitudes, since, given enough time, two peaks will eventually align. As for RMS, the trick is to see that the sum of squares is just another name for the &lt;a href="https://en.wikipedia.org/wiki/Variance"&gt;variance&lt;/a&gt;, of which RMS amplitude is the square root (which makes it the &lt;a href="https://en.wikipedia.org/wiki/Standard_deviation"&gt;standard deviation&lt;/a&gt;). The variance of the sum of two independent random variables is the &lt;a href="https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)"&gt;sum of the variances&lt;/a&gt;, so we can directly deduce that the RMS amplitude of the sum of two unrelated signals is the root of the sum of the squared RMS amplitudes — i.e. the &lt;a href="https://en.wikipedia.org/wiki/Root_mean_square#In_waveform_combinations"&gt;RMS total&lt;/a&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;The RMS total is also known as the &lt;em&gt;power sum&lt;/em&gt;. This makes sense because power is proportional to the square of the amplitude.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; In that sense, summing two unrelated signals is similar to summing two sine waves of different frequencies as described in the previous section.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The peak amplitude of the sum of two unrelated signals is the sum of the peak amplitudes. The RMS amplitude of the sum is the RMS total of the amplitudes.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id="a-word-about-decibels"&gt;A word about decibels&lt;/h1&gt;
&lt;p&gt;At this point, I should warn you about &lt;a href="https://factualaudio.com/post/decibel/"&gt;decibels&lt;/a&gt;. Due to their logarithmic nature, decibels are great for manipulating ratios and multiplying things, but they tend to get in the way when doing additions. As a gentle reminder, the sum of, say, ×2 (6&amp;#160;dB) and ×5 (14&amp;#160;dB) is ×7, which is 17&amp;#160;dB, &lt;em&gt;not&lt;/em&gt; 20&amp;#160;dB.&lt;/p&gt;
&lt;p&gt;In fact, it gets even more confusing when you realize that what I just did in this example was directly summing two values, which is only a good idea in &lt;em&gt;some&lt;/em&gt; of the cases that I’ve described above: namely, perfect constructive interference, or when dealing with peak amplitudes. It is not the correct approach when dealing with RMS totals. Which is really unfortunate, because decibel values are typically expressed in RMS, so the intuitive direct approach is wrong in many cases.&lt;/p&gt;
&lt;p&gt;Fortunately, decibels can also help us in their own way, as they make it easier to express the amplitude of the sum when the ratio of the amplitudes of the original signals is known. For example, ×2&amp;#160;is +6&amp;#160;dB, which can help when summing signals of identical amplitude. What’s much more interesting, though, is that the RMS total of two signals of identical amplitude turns out to be their RMS amplitude multiplied by the square root of 2… which, conveniently, happens to be approximately +3&amp;#160;dB. If we combine that with the knowledge we gained in the earlier sections, we can say, for example, that summing two unrelated signals of equal amplitude results in a peak amplitude of +6&amp;#160;dB and an RMS amplitude of +3&amp;#160;dB above the original signals. Isn’t that nice?&lt;/p&gt;
&lt;p&gt;In fact, we can summarize what we’ve just learned as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Sum of two signals of equal amplitude&lt;/th&gt;
&lt;th&gt;Peak&lt;/th&gt;
&lt;th&gt;RMS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sine waves of identical frequency and phase&lt;/td&gt;
&lt;td&gt;+6&amp;#160;dB&lt;/td&gt;
&lt;td&gt;+6&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sine waves of identical frequency and opposite phase&lt;/td&gt;
&lt;td&gt;-∞ dB&lt;/td&gt;
&lt;td&gt;-∞ dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sine waves of different frequencies&lt;/td&gt;
&lt;td&gt;+6&amp;#160;dB&lt;/td&gt;
&lt;td&gt;+3&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Unrelated signals&lt;/td&gt;
&lt;td&gt;+6&amp;#160;dB&lt;/td&gt;
&lt;td&gt;+3&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These results can be extended to arbitrary ratios of amplitudes, and calculators exist to help you do just that, be it for the &lt;a href="http://www.sengpielaudio.com/calculator-leveladding.htm"&gt;RMS total&lt;/a&gt; case or the &lt;a href="http://www.sengpielaudio.com/calculator-coherentsources.htm"&gt;direct (peak) case&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ll leave you with one last rule of thumb: decibels being a logarithmic unit, the contribution of a given term to the sum falls quickly as the decibels decrease. For example, the direct sum of 0&amp;#160;dB and -20&amp;#160;dB is only 0.8&amp;#160;dB. This can be used to dismiss the contribution of the smaller terms as negligible. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;It’s also why low-level &lt;a href="https://factualaudio.com/post/distortion/#noise"&gt;noise&lt;/a&gt;, which normally sits at -80&amp;#160;dB or less relative to the signal, has a negligible effect on the overall amplitude.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Reasoning about phase</title>
      <link>https://factualaudio.com/post/phase/</link>
      <pubDate>Sun, 21 Jan 2018 23:18:15 +0000</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/phase/</guid>
      <description>
&lt;p&gt;So far, when discussing the various frequency components of a signal — i.e. the sine waves that &lt;a href="https://factualaudio.com/post/anatomy/#the-frequency-domain"&gt;together make up&lt;/a&gt; the signal — I’ve mostly focused on the &lt;em&gt;&lt;a href="https://factualaudio.com/post/amplitude/"&gt;amplitude&lt;/a&gt;&lt;/em&gt; of these sine waves. In order to keep things simple and short, I’ve been purposefully evading the subject of &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_(waves)"&gt;phase&lt;/a&gt;&lt;/em&gt;, an often-overlooked property of waveforms. Now it’s time to face the music (pun intended) and fill this gap, as this concept will be useful in future posts.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz-phase.fingerprint-7b9b4cf.png" alt="Four sine waves with different phases" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz-phase.fingerprint-eb3f893.svg"&gt;&lt;/span&gt;
&lt;p&gt;The above plot shows four different sine waves. Contrary to what I’ve discussed previously on this blog, these waves don’t differ in frequency nor amplitude. Instead, their cycles are offset from each other in time. In other words, they have different phase.&lt;/p&gt;
&lt;p&gt;Phase indicates what part of the wave cycle is occurring at a given point in time. When not specified, that moment is conventionally defined as the origin of time (i.e. t=0). For example, in the above plot, the phase of the solid blue sine wave is zero, because its cycle starts at the origin. In contrast, at the same instant, the other sine waves are at a quarter, a half, and three quarters of their cycles, respectively.&lt;/p&gt;
&lt;p&gt;When a sine wave reaches the end of its cycle, it’s right back when it started and a new cycle begins again. In that way, progressing through the cycles of a sine wave is a bit like running around in circles. Hence it is no surprise that the terminology used when reasoning about phase is that of turns and angles. It seems natural in that context to define a &lt;a href="https://en.wikipedia.org/wiki/Turn_(geometry)"&gt;full cycle&lt;/a&gt; as 360 &lt;a href="https://en.wikipedia.org/wiki/Degree_(angle)"&gt;degrees&lt;/a&gt; — or, alternatively, 2π &lt;a href="https://en.wikipedia.org/wiki/Radian"&gt;radians&lt;/a&gt; — and from there, the other sine waves shown above could be defined as fractions of a full cycle: 90° (½π rad), 180° (π rad) and 270° (³⁄₂π rad). &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;From a mathematical perspective, the analogy goes &lt;a href="https://commons.wikimedia.org/wiki/File:ComplexSinInATimeAxe.gif"&gt;much farther&lt;/a&gt; than this. These concepts are all variations on the same common theme.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="phase-in-the-frequency-domain"&gt;Phase in the frequency domain&lt;/h1&gt;
&lt;p&gt;We’ve &lt;a href="https://factualaudio.com/post/anatomy/#the-frequency-domain"&gt;seen previously&lt;/a&gt; that the Fourier transform can be used to decompose any signal into a number of constituent sine waves — one per discrete frequency. In addition to the amplitude of each sine wave, the output of the Fourier transform (i.e. the spectrum) also contains their phase. For example, here is the phase information that the Fourier transform produces for each of the four above signals: &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;For the sake of consistency and to avoid confusion, I cheated a bit here — the real components of the Fourier transform are cosines, not sines, so strictly speaking the output should be offset by 90°. This is purely arbitrary and doesn’t matter one bit, though.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-spectrum-1khz-phase.fingerprint-89bac21.png" alt="Four sine waves with different phases" data-svg-alternative="https://factualaudio.com/plots/sine-spectrum-1khz-phase.fingerprint-74af671.svg"&gt;&lt;/span&gt;
&lt;p&gt;We’ve also &lt;a href="https://factualaudio.com/post/distortion/#frequency-response-distortion"&gt;seen previously&lt;/a&gt; that a linear system can alter the amplitude of the frequency components that flow through it. In the same way, a linear system can also alter their phase (which is &lt;em&gt;not&lt;/em&gt; the same thing as delaying them — see below), and the extent of these alterations can be shown on a plot, called the &lt;a href="https://en.wikipedia.org/wiki/Phase_response"&gt;phase response&lt;/a&gt;. Here is the phase response of a system that is similar to the example from that previous post:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/peak-phase-response.fingerprint-bb050bb.png" alt="Four sine waves with different phases" data-svg-alternative="https://factualaudio.com/plots/peak-phase-response.fingerprint-87f3197.svg"&gt;&lt;/span&gt;
&lt;h1 id="phase-and-delay"&gt;Phase and delay&lt;/h1&gt;
&lt;p&gt;The sine waves I’m using as examples have a frequency of 1&amp;#160;kHz, which means that a cycle completes in 1&amp;#160;millisecond. From that perspective, it is tempting to think about phase as a time offset; for example, one might say that a 1&amp;#160;kHz wave with a phase of 90° is offset by one quarter of a millisecond relative to the reference. This quantity is known as the &lt;a href="https://en.wikipedia.org/wiki/Group_delay_and_phase_delay"&gt;phase delay&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is where things get tricky and misleading, though. One subtlety that is often overlooked when dealing with such concepts is that, mathematically speaking, phase is a property of a &lt;a href="https://en.wikipedia.org/wiki/Periodic_function"&gt;periodic signal&lt;/a&gt;, which implies a &lt;a href="https://en.wikipedia.org/wiki/Continuous_wave"&gt;continuous wave&lt;/a&gt; of infinite duration with no beginning and no end. Real-world signals of course do not meet these criteria. For most intents and purposes this does not really matter, but in some cases it does, and this is one of those. Specifically, it makes it very easy to misinterpret the meaning of phase delay.&lt;/p&gt;
&lt;p&gt;Imagine that, in the above plot, the sine waves went on forever on both sides of the figure. What does “delay” even mean in that context? I could say that the 90° wave is delayed by 0.25&amp;#160;ms relative to the 0° wave, but I could just as well flip things around and say that the 0° wave is delayed by 0.75&amp;#160;ms relative to the 90° wave. Since both signals extend infinitely to the left, it makes no sense to imply that one &lt;em&gt;started&lt;/em&gt; before the other. I could even go one step further and say that the 90° wave is delayed by, say, 10.25&amp;#160;ms (10&amp;#160;full cycles, plus a quarter cycle) and it would still mean the same thing. For this reason, the word “delay” needs to be handled very carefully in this context. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;For an illustration of what can happen when people get confused about these concepts, see this &lt;a href="https://groups.google.com/d/topic/comp.soft-sys.matlab/lBSRLMUV7nE/discussion"&gt;epic trainwreck of a debate&lt;/a&gt; where 46&amp;#160;participants spend 456&amp;#160;posts fighting to the death over the deep philosophical meaning of the word “delay”. The math is easy; it’s interpreting the results that’s hard.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Following the same logic, in terms of the phase itself, 90° is equivalent to -270°, 450°, -630°, etc. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;In fact, when using &lt;a href="https://en.wikipedia.org/wiki/Complex_number#Signal_analysis"&gt;complex numbers&lt;/a&gt; to do signal analysis, these are not just equivalent: they are the exact same number, landing in the same spot on the &lt;a href="https://en.wikipedia.org/wiki/Complex_plane"&gt;complex plane&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;You might come across phase response plots where the range of values exceed 360°. This is called &lt;em&gt;unwrapped phase&lt;/em&gt; and is meant mostly as a visual aid, making the graph more readable by avoiding sudden jumps at the boundaries of the range, and making some calculations easier. The underlying data is the same.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Then again, one might still be tempted to argue that this is a mostly theoretical distinction: after all, any real-world device that changes the phase of the signal that passes through it &lt;em&gt;has&lt;/em&gt; to apply some kind of delay, right?&lt;/p&gt;
&lt;p&gt;Well, not necessarily. As a counter-example, consider the case of a very basic device that reverts the &lt;a href="https://en.wikipedia.org/wiki/Electrical_polarity"&gt;polarity&lt;/a&gt; of the signal in the &lt;a href="https://factualaudio.com/post/life/#the-analog-realm"&gt;analog realm&lt;/a&gt;. In other words, it changes the sign of the voltage, which could be as simple as swapping two wires. The opposite of a sine wave is that same sine wave shifted by 180°, so in effect, this device shifts the phase of every frequency component of the input signal by 180°. One can even say that it has a phase delay of 0.5&amp;#160;ms at 1&amp;#160;kHz (and 1&amp;#160;ms at 500&amp;#160;Hz, and 0.25&amp;#160;ms at 2&amp;#160;kHz, and so on). Yet it would be impossible to say that this device &lt;em&gt;delays&lt;/em&gt; the signal for any reasonable definition of “delay” (i.e. in terms of information or energy propagation). It physically can’t, since there is no memory (buffer) for it to hold the signal in, and energy is coming out as soon as it gets in. This device exhibits phase shift and thus phase delay, yet &lt;em&gt;there is no actual delay&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For these reasons, in general, it is not possible to know the true delay of a device by making a single phase shift measurement at a single frequency. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;A pure delay produces a phase shift at every frequency equal to frequency times delay — which, incidentally, is very effective at making a mess in phase response plots. But delay cannot be directly recovered from the phase shift at a single frequency, because, as explained above, there is loss of information — the phase shift is constrained to a 360° range, making its interpretation ambiguous.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; It is, however, often possible to get more information about delay if a number of phase shift measurements are taken at various frequencies, such as by looking at a phase response plot like the one from the previous section. More specifically, this can be used to compute the &lt;a href="https://en.wikipedia.org/wiki/Group_delay_and_phase_delay"&gt;group delay&lt;/a&gt; of the device, albeit with a number of caveats related to measurement accuracy. Some devices, especially those that exhibit amplitude frequency response distortion like the above example, have a group delay that itself varies with frequency, which makes interpretation even trickier. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;If you’re not convinced how tricky this is, consider that the example I’ve used above (which is quite mundane, really) has &lt;em&gt;negative&lt;/em&gt; group delay in the low frequencies. Let that sink in for a moment. That doesn’t seem physically possible, &lt;a href="https://www.dsprelated.com/showarticle/54.php"&gt;but it is&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Noise and distortion</title>
      <link>https://factualaudio.com/post/distortion/</link>
      <pubDate>Tue, 21 Nov 2017 21:05:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/distortion/</guid>
      <description>
&lt;p&gt;As the audio signal makes its way through the different &lt;a href="https://factualaudio.com/post/life/"&gt;realms&lt;/a&gt; of the system, it travels through various digital, analog, and acoustic components that alter the signal in various ways. Some of these alterations might or might not be audible, or might only be audible under certain conditions. In most scenarios relevant to this blog they are undesirable side effects from limitations in the components that make up the system, though in some specific cases they can be deliberately introduced in pursuit of a specific goal (e.g. &lt;a href="https://en.wikipedia.org/wiki/Equalization_(audio)"&gt;equalization&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In order to build a high quality audio system, it is necessary to keep signal degradation (i.e. unwanted alteration) under control, and this requires a good understanding of what these alterations might be, what causes them, and how to avoid or alleviate them. Ever since the advent of sound reproduction more than a century ago this topic has been the subject of great debate, some thoughtful and innovative, some misguided or downright counter-productive. Hopefully this blog will do more of the former and less of the latter — but for now, this post serves as a brief introduction to the issues at hand.&lt;/p&gt;
&lt;p&gt;Signal alterations can be divided into three broad categories: &lt;em&gt;noise&lt;/em&gt;, &lt;em&gt;frequency response distortion&lt;/em&gt;, and &lt;em&gt;non-linear distortion&lt;/em&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;When used by itself without qualification, the term “distortion” can refer to some or all of these categories, depending on context.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Real-world systems exhibit all three kinds in varying amounts. What follows is a brief overview of the issues at hand; future posts will look at each of them more closely.&lt;/p&gt;
&lt;h1 id="noise"&gt;Noise&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Noise"&gt;Noise&lt;/a&gt;&lt;/em&gt; describes an alteration of the signal in which a separate, &lt;em&gt;unrelated&lt;/em&gt; signal is added (i.e. mixed in, superposed) to the original signal. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This is the definition I’ll use throughout this blog, pursuant to &lt;a href="https://webstore.iec.ch/publication/1219"&gt;IEC 60268–2&lt;/a&gt;. In other contexts noise might be used in a more specific way (e.g. broadband noise only), or in a more general way (e.g. signal differences introduced by non-linear distortion are considered to be part of the noise signal).&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; That additional signal has its own &lt;a href="https://factualaudio.com/post/anatomy/"&gt;characteristics&lt;/a&gt; including amplitude and frequency content (spectrum), which are combined with the characteristics of the original signal. Noise is often quantified by comparing the amplitude of the noise to the amplitude of the signal, a metric known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio"&gt;signal-to-noise ratio&lt;/a&gt;&lt;/em&gt; (the higher the better).&lt;/p&gt;
&lt;p&gt;Noise can appear in all three of the audio realms. In the digital realm it can take the form of &lt;a href="https://en.wikipedia.org/wiki/Dither#Digital_audio"&gt;dithering noise&lt;/a&gt; for example, though modern digital systems provide good enough performance that noise sits comfortably below the threshold of audibility. This is not always the case in the analog realm, where noise problems are the most common, the most objectionable, and the most pernicious — often the result of complex &lt;a href="https://en.wikipedia.org/wiki/Electromagnetic_interference"&gt;electromagnetic interference&lt;/a&gt; phenomena, subtle hardware defects, or compatibility issues. Finally, the acoustic realm is rife with often overlooked sources of noise from ordinary life, from the rumbling of an air conditioning unit to the occasional car driving down the street.&lt;/p&gt;
&lt;p&gt;Depending on amplitude and frequency content, noise might or might not constitute a problem in practice. For example, low-level broadband &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Colors_of_noise"&gt;colored noise&lt;/a&gt;&lt;/em&gt; (such as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/White_noise"&gt;white noise&lt;/a&gt;&lt;/em&gt;) will often go unnoticed because its spectrum is roughly similar to typical ambient noise that we are all continuously subjected to in our daily lives. &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;See Albert Donald G., Decato Stephen N., “&lt;a href="http://www.sciencedirect.com/science/article/pii/S0003682X16306120"&gt;Acoustic and seismic ambient noise measurements in urban and rural areas&lt;/a&gt;”, Applied Acoustics, 119, 135–143, (2017) for examples of ambient noise spectra.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; The same cannot be said of narrowband noise concentrated in specific frequencies. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Unfortunately that distinction is lost when noise measurements are condensed into a single number (such as signal-to-noise ratio), discarding spectral distribution in the process.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Furthermore, narrowband noise is more likely to affect the perception of minute detail in the original signal, due to an auditory phenomenon known as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Auditory_masking"&gt;masking&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/noisy-sine-wave.fingerprint-0c9d828.png" alt="Noisy sine waveform" data-svg-alternative="https://factualaudio.com/plots/noisy-sine-wave.fingerprint-8008abb.svg"&gt;&lt;/span&gt; &lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/noisy-sine-spectrum.fingerprint-8556bc4.png" alt="Noisy sine spectrum" data-svg-alternative="https://factualaudio.com/plots/noisy-sine-spectrum.fingerprint-6db4a30.svg"&gt;&lt;/span&gt;
&lt;p class="legend"&gt;&lt;em&gt;Waveform and spectrum of a sine wave affected by white noise. The noise spectrum, circled in red, is often called the “noise floor”. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;From that spectrum plot you might be tempted to conclude that the signal-to-noise ratio is about 50&amp;#160;dB. That would be wrong — it’s actually much worse, around 17&amp;#160;dB. You can’t read it directly from the graph because the noise is spread across multiple frequencies. This is a very common mistake when reading spectrum plots, which I might describe in more detail in a future post.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In a real-world scenario, noise is only really noticeable when the original signal is relatively quiet, such as when there is a break in a piece of music, because all that remains is the noise itself. Conversely, noise is inaudible when a significantly loud signal is playing, again because of masking (but this time in reverse). In practice, I tend to abide by the following rule of thumb: if you can’t hear anything when playing a silent signal, then your system is probably fine as far as noise is concerned.&lt;/p&gt;
&lt;h1 id="frequency-response-distortion"&gt;Frequency response distortion&lt;/h1&gt;
&lt;p&gt;In the first post on this site, I &lt;a href="https://factualaudio.com/post/anatomy/#the-frequency-domain"&gt;explained&lt;/a&gt; that an audio signal can be decomposed into a number of constituent signals of various frequencies. One way an audio component can alter the signal is by changing the amplitude (i.e. applying &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Gain_(electronics)"&gt;gain&lt;/a&gt;&lt;/em&gt;) on some of these frequencies more than others. This relationship between frequency and gain is known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Frequency_response"&gt;frequency response&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If this relationship is constant, i.e. the same amplitude multiplier is always applied to a given frequency regardless of the shape of the signal, then we are dealing with a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Linear_time-invariant_theory"&gt;linear time-invariant system&lt;/a&gt;&lt;/em&gt;. We can plot the frequency response on a graph, known as a &lt;em&gt;frequency response graph&lt;/em&gt; (or, more technically, a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Bode_plot"&gt;Bode plot&lt;/a&gt;&lt;/em&gt;). &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;One thing that I’ve omitted to keep things simple is that a linear time-invariant system is not just allowed to change the amplitude of individual frequency components, it can also change their &lt;a href="https://en.wikipedia.org/wiki/Phase_(waves)"&gt;phase&lt;/a&gt;. This is conveyed through the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_response"&gt;phase response&lt;/a&gt;&lt;/em&gt;. Technically the term “frequency response” encompasses both &lt;em&gt;magnitude response&lt;/em&gt; and &lt;em&gt;phase response&lt;/em&gt;, though the latter is often dismissed for lack of relevance in most audio discussions. More on the phase response in the &lt;a href="https://factualaudio.com/post/phase/#phase-in-the-frequency-domain"&gt;next post&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/peak-frequency-response.fingerprint-039fc50.png" alt="Frequency response with 1&amp;#160;kHz 6&amp;#160;dB resonance" data-svg-alternative="https://factualaudio.com/plots/peak-frequency-response.fingerprint-9d34e77.svg"&gt;&lt;/span&gt;
&lt;p class="legend"&gt;&lt;em&gt;The frequency response of a system that amplifies frequencies around 1&amp;#160;kHz by about 6&amp;#160;dB. This particular type of distortion is called a &lt;a href="https://en.wikipedia.org/wiki/Resonance"&gt;resonance&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How is this useful? Well, &lt;a href="https://factualaudio.com/post/anatomy/#the-frequency-domain"&gt;remember&lt;/a&gt; that the frequency domain is often more useful than the time domain when it comes to understanding how audio signals are perceived. Frequency response tells us how a system alters the frequency components of the signal that flows through it. That makes it one the most powerful tools in the audio engineer’s toolbox.&lt;/p&gt;
&lt;p&gt;Studies show that frequency response is extremely important when it comes to assessing audio quality in real-world scenarios. For example it has been shown that the human auditory system is capable of detecting frequency response variations as tiny as 0.1&amp;#160;dB &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Toole, F. and Olive. S., “&lt;a href="http://www.aes.org/e-lib/browse.cfm?elib=5163"&gt;The Modification of Timbre by Resonances: Perception and Measurement&lt;/a&gt;”, J. Audio Eng. Soc., 36(3), 122–141, (1988). From Fig. 9: coherent sum of 0&amp;#160;dB and -40&amp;#160;dB sources is ~0.1&amp;#160;dB.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; , and that it is by far the most important criterion when it comes to assessing the quality of a loudspeaker &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Olive, Sean, “&lt;a href="http://www.aes.org/e-lib/browse.cfm?elib=12794"&gt;A Multiple Regression Model For Predicting Loudspeaker Preference Using Objective Measurements: Part 1 — Listening Test Results&lt;/a&gt;”, presented at the 116th AES Convention, Berlin, Germany, preprint 6113, (May 2004).&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; . Make no mistake: this metric is a &lt;em&gt;huge deal&lt;/em&gt;, and I expect most posts on this blog will make use of it in one way or another.&lt;/p&gt;
&lt;p&gt;The digital and analog realms typically contribute very little in the way of frequency response distortion. Virtually all of it is found in the acoustic realm, as even the best loudspeakers and headphones struggle to keep their frequency response within a few dB of the intended target &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Many examples can be found in the &lt;a href="http://www.soundstagenetwork.com/index.php?option=com_content&amp;amp;view=article&amp;amp;id=16&amp;amp;Itemid=18"&gt;SoundStage database&lt;/a&gt; (loudspeakers) and the &lt;a href="https://www.innerfidelity.com/headphone-measurements"&gt;InnerFidelity database&lt;/a&gt; (headphones).&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; . It gets worse: the acoustics of home listening rooms (&lt;a href="https://en.wikipedia.org/wiki/Room_modes"&gt;room modes&lt;/a&gt; especially) can result in low frequency alterations in the order of 10&amp;#160;dB or more! &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Literally any frequency response measurement made in a small room will show this phenomenon. One example can be found in Leduc Michel, 2009, “&lt;a href="http://www.audioholics.com/room-acoustics/listening-room-acoustics#ftn3"&gt;How Does Listening Room Acoustics Affect Sound Quality?&lt;/a&gt;“, Audioholics (graph under “Standing waves” section). A series of representative examples can be found in Toole, Floyd E., &lt;em&gt;&lt;a href="https://books.google.co.uk/books?id=sGmz0yONYFcC"&gt;Sound Reproduction: Loudspeakers and Rooms&lt;/a&gt;&lt;/em&gt;, figure 13.9.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; No wonder why these are often said to be the “weakest links” of the audio chain…&lt;/p&gt;
&lt;h1 id="non-linear-distortion"&gt;Non-linear distortion&lt;/h1&gt;
&lt;p&gt;Ideally, audio components should meet the definition of a linear system as described above; that is, they should be able to accurately track the input signal, such that the output is precisely proportional to the input. Of course that is not the case in practice. Besides noise (which we’ve already covered), consider, for example, that the driver inside a loudspeaker is made from imperfect materials that have imperfect physical properties, so its movement will not perfectly track the signal, instead giving rise to non-linear distortion. One example in the analog realm is &lt;a href="https://en.wikipedia.org/wiki/Crossover_distortion"&gt;crossover distortion&lt;/a&gt; that can occur in certain types of amplifiers.&lt;/p&gt;
&lt;p&gt;Fortunately, any reasonable audio system will be at least &lt;em&gt;approximately&lt;/em&gt; linear, so we can still use the frequency response to reason about the system. At the same time, we need to keep ourselves honest and account for any remaining non-linear behavior that might alter the signal in ways that frequency response and noise measurements will not predict. This is appropriately called &lt;em&gt;non-linear distortion&lt;/em&gt;, also known as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Amplitude_distortion"&gt;amplitude distortion&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This definition makes non-linear distortion a very open-ended category, as indeed a signal can be distorted in an infinite variety of ways. Since it is not possible to run an infinite number of tests, measuring and quantifying the non-linear behavior of a system can be quite challenging. That said, most non-linear distortion comes in well-known shapes and forms, so a few standard measurements are usually good enough to characterize the performance of an audio system.&lt;/p&gt;
&lt;p&gt;One important aspect of non-linear distortion is that, contrary to frequency response distortion, it can cause new frequencies to appear that weren’t present in the original signal. By far the most well-known type of non-linear distortion is &lt;em&gt;harmonic distortion&lt;/em&gt;, where new frequencies appear that are whole multiples (&lt;a href="https://en.wikipedia.org/wiki/Harmonic"&gt;harmonics&lt;/a&gt;) of the frequencies in the original signal. It is often summarized in a single number, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Total_harmonic_distortion"&gt;total harmonic distortion&lt;/a&gt;&lt;/em&gt; (THD), which indicates the total amplitude of the introduced harmonics relative to the original signal (more precisely, the &lt;a href="https://en.wikipedia.org/wiki/Fundamental_frequency"&gt;fundamental&lt;/a&gt;). A related phenomenon is &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Intermodulation"&gt;intermodulation distortion&lt;/a&gt;&lt;/em&gt;, where multiple frequency components in the signal interact with each other to create new frequencies in a specific pattern.&lt;/p&gt;
&lt;p&gt;At the beginning of this section I described examples of non-linear distortion that can occur during normal system operation. However, the most egregious, obvious, and audible non-linear distortion issues occur when the system is driven beyond its limits; that is, signal amplitude is pushed too far and the system is unable to keep up. When this happens the peaks of the waveform cannot be reproduced faithfully and the system “bottoms out”, a phenomenon known as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Clipping_(audio)"&gt;clipping&lt;/a&gt;&lt;/em&gt;. To mention a few examples, this can happen in the digital realm (which is otherwise immune from most other forms of non-linear distortion) due to overflowing the largest possible sample value; in the analog realm due to exceeding the power limits of an amplifier; or in the acoustic realm due to exceeding the movement limits of a driver (&lt;a href="https://en.wikipedia.org/wiki/Excursion_(audio)"&gt;excursion&lt;/a&gt;).&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/clipped-sine-wave.fingerprint-e1151d6.png" alt="Clipped sine waveform" data-svg-alternative="https://factualaudio.com/plots/clipped-sine-wave.fingerprint-ffb5f12.svg"&gt;&lt;/span&gt; &lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/clipped-sine-spectrum.fingerprint-1f3315a.png" alt="Clipped sine spectrum" data-svg-alternative="https://factualaudio.com/plots/clipped-sine-spectrum.fingerprint-2a7fd86.svg"&gt;&lt;/span&gt;
&lt;p class="legend"&gt;&lt;em&gt;Waveform and spectrum of a 1&amp;#160;kHz sine wave showing symptoms of “hard clipping”, which produces odd-order harmonics (circled in red). The THD in this example is about 7%. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Here’s how this number is calculated. First, take the &lt;a href="https://factualaudio.com/post/amplitude/"&gt;RMS amplitude&lt;/a&gt; of the harmonics: 3&amp;#160;kHz -26&amp;#160;dB (0.051) 5&amp;#160;kHz -32&amp;#160;dB (0.025) 7&amp;#160;kHz -46&amp;#160;dB (0.0047) 9kHz -47&amp;#160;dB (0.0043)… the &lt;a href="https://en.wikipedia.org/wiki/Root_mean_square#In_waveform_combinations"&gt;total RMS amplitude&lt;/a&gt; of the harmonics is 0.057. The RMS amplitude of the fundamental is 0.78. The THD is the ratio of these two numbers, which is 0.073, or 7.3%.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So how audible is non-linear distortion? Well, that depends. Because non-linear distortion can take many forms, there is no simple answer. For example, distortion in the lower frequencies is less audible than in higher frequencies, and it is less audible if the newly introduced frequencies are close to the fundamental (thanks, once again, to masking). &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Temme, Steve, “&lt;a href="https://www.bksv.com/media/doc/BO0385.pdf"&gt;Application Note: Audio Distortion Measurements&lt;/a&gt;“, Brüel &amp; Kjær, 1992.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; Condensing non-linear distortion measurements into a single, simplistic THD number, as is often done, certainly doesn’t help. With that in mind, real-world examples tend to suggest that a THD of 10% is likely to be audible, 1% is borderline, and 0.1% is unlikely to be audible. &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Audioholics, ”&lt;a href="http://www.audioholics.com/room-acoustics/human-hearing-distortion-audibility-part-3"&gt;Human Hearing - Distortion Audibility Part 3&lt;/a&gt;”, 2005.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt;&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Amplitude, quantified</title>
      <link>https://factualaudio.com/post/amplitude/</link>
      <pubDate>Tue, 21 Nov 2017 21:04:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/amplitude/</guid>
      <description>
&lt;p&gt;In the last series of posts I’ve been focusing on the concept of amplitude, first &lt;a href="https://factualaudio.com/post/anatomy/"&gt;defining it&lt;/a&gt; as the strength of an audio signal, then &lt;a href="https://factualaudio.com/post/life/"&gt;describing its physical manifestations&lt;/a&gt; as the signal makes its way through the audio playback chain, and lastly explaining how &lt;a href="https://factualaudio.com/post/decibel/"&gt;decibels&lt;/a&gt; are used as a way to manipulate these numbers.&lt;/p&gt;
&lt;p&gt;The attentive reader might have observed that, back in that very first post, I deliberately left out an important part for later. The issue that still needs to be addressed is exactly how to &lt;em&gt;quantify&lt;/em&gt; amplitude, i.e. how do we calculate its value for a given audio signal. Let’s go back to our original example of a pure tone (sine) waveform, but with a vertical scale this time:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-amplitude.fingerprint-bf62927.png" alt="Sine wave with amplitude scale" data-svg-alternative="https://factualaudio.com/plots/sine-wave-amplitude.fingerprint-456cf34.svg"&gt;&lt;/span&gt;
&lt;p&gt;Now let me ask you a question: what is the amplitude of that signal?&lt;/p&gt;
&lt;h1 id="peak-amplitude"&gt;Peak amplitude&lt;/h1&gt;
&lt;p&gt;You might be tempted to answer that question with “1.0” &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Well, unless you are an audio engineer and you know what’s up. But in that case, what are you doing here?&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; , because that’s the as far as the curve travels from its middle point. Or perhaps you might answer “2.0”, because that’s the total height of the waveform (-1.0&amp;#160;to 1.0).&lt;/p&gt;
&lt;p&gt;These are not the only possible answers (as we’ll see below), but they are valid answers nonetheless. The former answer (“1.0”) is called the &lt;em&gt;peak amplitude&lt;/em&gt; of the signal. The latter answer (“2.0”) is called the &lt;em&gt;peak-to-peak amplitude&lt;/em&gt; of the signal.&lt;/p&gt;
&lt;p&gt;Peak-to-peak amplitude is twice the peak amplitude. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Strictly speaking that’s a bit of an oversimplification, because it assumes that audio signals are symmetrical, but they often aren’t — see Hetrich, Wayne L., “&lt;a href="http://www.aes.org/e-lib/browse.cfm?elib=2221"&gt;Real-World Audio Wave Form Asymmetries and the Effect on the Audio Chain&lt;/a&gt;”, presented at the 55th AES Convention, New York, NY, USA, preprint 1193, (October 1976). However this rarely matters in practice.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; In practice peak amplitude is more widely used than peak-to-peak amplitude. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;One notable exception is marketing material (including manufacturer-provided specifications), where peak-to-peak amplitude is often used because the number looks bigger — don’t be fooled!&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="the-problem-with-peak-amplitude"&gt;The problem with peak amplitude&lt;/h1&gt;
&lt;p&gt;You might be wondering why we can’t simply stop there. After all, we just came up with a quantitative definition of amplitude, so our job here is done, right?&lt;/p&gt;
&lt;p&gt;Not so fast. Let’s not forget that amplitude is used in a variety of contexts for a variety of calculations and comparisons. We need to make sure that the definition we came up with (peak amplitude) is appropriate for what we’re going to use it for.&lt;/p&gt;
&lt;p&gt;Peak amplitude is appropriate in &lt;em&gt;some&lt;/em&gt; contexts. For example, if you’re trying to determine whether a digital signal is going to clip, peak amplitude is definitely the metric you should use to make that determination. But in most cases, what we’re most interested in is the amount of &lt;em&gt;energy&lt;/em&gt; that is being conveyed in that audio signal &lt;em&gt;on average&lt;/em&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;“Energy” is used here in a generic sense, as physically speaking there is no “energy” in a digital signal for example. However it does map directly to the physical definition of energy when the signal enters the analog or acoustic realms, and since the acoustic realm is really all that matters in the end, it makes sense to use that term to describe audio signals in general.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Peak amplitude fares poorly in that scenario. To understand why, let’s look at an extreme example of a signal that is very different from a sine wave:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sinh-wave-amplitude.fingerprint-24ec7ae.png" alt="Peaky wave with amplitude scale" data-svg-alternative="https://factualaudio.com/plots/sinh-wave-amplitude.fingerprint-c194792.svg"&gt;&lt;/span&gt;
&lt;p&gt;That signal has the same peak amplitude as the previous example. Yet, it’s easy to see that it conveys less energy: it’s mostly silence only interrupted by a train of narrow peaks. That makes peak amplitude ill-suited for estimating the overall strength of the signal.&lt;/p&gt;
&lt;h1 id="rms-to-the-rescue"&gt;RMS to the rescue&lt;/h1&gt;
&lt;p&gt;To solve this problem, we need a different metric. Ideally, we want to compute some kind of &lt;em&gt;average&lt;/em&gt; of the signal. We can’t use the &lt;a href="https://en.wikipedia.org/wiki/Arithmetic_mean"&gt;mean&lt;/a&gt; — that would just amount to zero, since the positive and negative parts of the signal would cancel each other out.&lt;/p&gt;
&lt;p&gt;As it turns out, there is a standard way to compute the average value of an audio signal (or any alternative signal for that matter): the &lt;a href="https://en.wikipedia.org/wiki/Root_mean_square"&gt;root mean square&lt;/a&gt; (RMS). It’s a simple formula: we square the signal values, sum the squares, divide the result by the number of values, and then finally we take the square root of that number. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;For the sake of example, I’m assuming a discrete-time signal here.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Because the values are squared, the positive and negative parts of the signal add up instead of canceling each other. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This discussion assumes that the signal is centered on zero — i.e. that there is no &lt;a href="https://en.wikipedia.org/wiki/DC_bias"&gt;DC offset&lt;/a&gt;. In audio this is almost always a safe assumption to make. Incidentally, RMS when used in this context is the same thing as &lt;a href="https://en.wikipedia.org/wiki/Standard_deviation"&gt;standard deviation&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we apply that formula to the first example, we end up with ~0.707. More generally, for a pure sine wave (and &lt;em&gt;only&lt;/em&gt; for a pure sine wave!), the math tells us RMS amplitude is equal to peak amplitude divided by the square root of two (√2). Or, when working in decibels, that’s peak amplitude minus ~3&amp;#160;dB.&lt;/p&gt;
&lt;p&gt;When applied to the second example, we end up with ~0.424. As expected, we get a lower value as the signal conveys less energy. In other words, the ratio of peak amplitude to RMS amplitude — known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Crest_factor"&gt;crest factor&lt;/a&gt;&lt;/em&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Sometimes informally — and somewhat incorrectly — referred to as &lt;a href="https://en.wikipedia.org/wiki/Dynamic_range#Music"&gt;dynamic range&lt;/a&gt; in some contexts.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;  — is different because the shape of the waveform is different.&lt;/p&gt;
&lt;h1 id="closing-thoughts"&gt;Closing thoughts&lt;/h1&gt;
&lt;p&gt;&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-amplitude-detail.fingerprint-c62c872.png" alt="Sinewave with detailed amplitude annotations" data-svg-alternative="https://factualaudio.com/plots/sine-wave-amplitude-detail.fingerprint-e2f78de.svg"&gt;&lt;/span&gt; &lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sinh-wave-amplitude-detail.fingerprint-f5fa04a.png" alt="Peaky wave with detailed amplitude annotations" data-svg-alternative="https://factualaudio.com/plots/sinh-wave-amplitude-detail.fingerprint-130fed1.svg"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ve seen that there is more than one way to quantify the amplitude of a signal, and that different approaches will produce different results depending on the shape of the waveform. Depending on the context in which the numbers are used, some approaches might be more appropriate than others.&lt;/p&gt;
&lt;p&gt;In practice, the method used to calculate the amplitude is often stated near the unit. For example, “Vrms”, “Vp”, “Vpp”. Otherwise, it is often assumed that RMS was used. In particular, levels expressed in decibels (e.g. “dBV”), are virtually always RMS &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;a href="https://webstore.iec.ch/publication/94"&gt;IEC 60027–3:2002&lt;/a&gt;, &lt;em&gt;Letter symbols to be used in electrical technology — Part 3: Logarithmic and related quantities, and their units&lt;/em&gt;, §4.1&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; (with the possible exception of dBFS, which sadly is &lt;a href="https://en.wikipedia.org/wiki/DBFS"&gt;ambiguous&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;What about &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Loudness"&gt;loudness&lt;/a&gt;&lt;/em&gt;? One could see loudness as the way us humans measure the amplitude of the sounds that reach our ears. Because the human auditory system is extremely complex, it is not easy to estimate how loud a given signal will be perceived in general. Of the two approaches that I’ve described, RMS is the one that approximates loudness best, but it is still a very crude estimation. Nonetheless, in practice, RMS amplitude is often used as a poor man’s proxy for loudness due to its simplicity. More perceptually accurate ways to estimate loudness would typically involve &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Weighting"&gt;weighting&lt;/a&gt;&lt;/em&gt; or even more advanced processes such as those described in &lt;a href="https://www.itu.int/rec/R-REC-BS.1770/en"&gt;ITU-R BS.1770&lt;/a&gt;. But that’s a story for another post.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Decibels, explained</title>
      <link>https://factualaudio.com/post/decibel/</link>
      <pubDate>Tue, 21 Nov 2017 21:03:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/decibel/</guid>
      <description>
&lt;p&gt;In the &lt;a href="https://factualaudio.com/post/life/"&gt;previous post&lt;/a&gt;, I introduced a number of physical quantities that are used to describe the &lt;em&gt;&lt;a href="https://factualaudio.com/post/anatomy/"&gt;amplitude&lt;/a&gt;&lt;/em&gt; of an audio signal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the digital domain, it is a &lt;em&gt;sample value&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;In the analog domain, it is voltage in &lt;em&gt;volts&lt;/em&gt; (V);&lt;/li&gt;
&lt;li&gt;In the acoustic domain, it is a pressure difference in &lt;em&gt;pascals&lt;/em&gt; (Pa).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in the audio literature, marketing materials and equipment specifications, these are not the units that are typically used. Instead, one often finds such quantities expressed in &lt;em&gt;decibels&lt;/em&gt; (dB) or a related unit. There are good reasons for this, and they have to do with how we humans perceive loudness.&lt;/p&gt;
&lt;h1 id="on-the-usefulness-of-ratios"&gt;On the usefulness of ratios&lt;/h1&gt;
&lt;p&gt;In audio, we care more about &lt;em&gt;ratios&lt;/em&gt; of quantities (e.g. “×2”) than absolute values (e.g. “3&amp;#160;V”). To understand why, here’s an example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The difference in loudness between 0.1&amp;#160;Pa and 0.2&amp;#160;Pa is very noticeable.&lt;/li&gt;
&lt;li&gt;The difference in loudness between 1.0&amp;#160;Pa and 1.1&amp;#160;Pa is barely noticeable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases the absolute difference is the same: 0.1&amp;#160;Pa. However, the ratio is very different; in the former case it’s ×2, in the latter case it’s ×1.1. This makes a compelling case for using ratios, not differences, when comparing amplitudes.&lt;/p&gt;
&lt;p&gt;There is another advantage to using ratios. Remember that all three quantities (sample value, voltage, and sound pressure) are &lt;em&gt;proportional&lt;/em&gt; to each other when the audio signal moves from one realm to the next. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Assuming ideal conditions, i.e. no &lt;a href="https://factualaudio.com/post/distortion/"&gt;noise or distortion&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; This means that a given ratio applies in all three domains: twice the sample value is also twice the voltage and twice the sound pressure. This is very convenient because it means that a given ratio (often called &lt;a href="https://en.wikipedia.org/wiki/Gain_%28electronics%29"&gt;gain&lt;/a&gt;) has the same meaning regardless of context.&lt;/p&gt;
&lt;p&gt;Now, if only there was a unit that made working with ratios easier…&lt;/p&gt;
&lt;h1 id="enter-the-decibel"&gt;Enter the decibel&lt;/h1&gt;
&lt;p&gt;The &lt;em&gt;decibel&lt;/em&gt; (dB) is, in its purest form, a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Dimensionless_quantity"&gt;dimensionless unit&lt;/a&gt;&lt;/em&gt; that represents a ratio between two quantities, just like “×” or “%”. What sets the decibel apart is that it is also a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Logarithmic_scale"&gt;logarithmic unit&lt;/a&gt;&lt;/em&gt;, meaning that its value is proportional to the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Logarithm"&gt;logarithm&lt;/a&gt;&lt;/em&gt; of the ratio, not the ratio itself. This will be clearer with examples:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Amplitude ratio&lt;/th&gt;
&lt;th&gt;Decibel equivalent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;×0.01&lt;/td&gt;
&lt;td&gt;-40&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;×0.1&lt;/td&gt;
&lt;td&gt;-20&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;×0.5&lt;/td&gt;
&lt;td&gt;-6&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;×1&lt;/td&gt;
&lt;td&gt;0&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;×2&lt;/td&gt;
&lt;td&gt;+6&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;×4&lt;/td&gt;
&lt;td&gt;+12&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;×8&lt;/td&gt;
&lt;td&gt;+18&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;×10&lt;/td&gt;
&lt;td&gt;+20&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;×100&lt;/td&gt;
&lt;td&gt;+40&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because the decibel is a logarithmic unit, it behaves differently from more conventional linear units. The trick is to not fight their logarithmic nature, but embrace it. Here’s how:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The most useful ratios to keep in mind are +6&amp;#160;dB (×2) and +20&amp;#160;dB (×10).&lt;/li&gt;
&lt;li&gt;To invert the ratio, just change the sign: for example, -6&amp;#160;dB is the same as dividing by 2.&lt;/li&gt;
&lt;li&gt;When combining ratios, decibels don’t multiply; they add up. For example, ×4 (2×2) is 12&amp;#160;dB (6+6), not 36 (6×6). This might seem strange, but this property makes decibels very convenient to use in practice — it’s easier to add than to multiply.&lt;/li&gt;
&lt;li&gt;0&amp;#160;dB means ×1, i.e. no change in amplitude.&lt;/li&gt;
&lt;li&gt;×0&amp;#160;is -∞ dB (negative infinity). You might have seen this as the “mute” position on some volume knobs.&lt;/li&gt;
&lt;li&gt;For less trivial cases, &lt;a href="http://www.sengpielaudio.com/calculator-db.htm"&gt;online calculators&lt;/a&gt; are available.&lt;/li&gt;
&lt;/ul&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; All of the above assumes decibels are used to express ratios of &lt;a href="https://en.wikipedia.org/wiki/Field,_power,_and_root-power_quantities"&gt;field quantities&lt;/a&gt;. Digital sample value, voltage, and sound pressure are examples of field quantities. When dealing with &lt;em&gt;power&lt;/em&gt; quantities (i.e. watts), however, there is a catch: in that case, +6&amp;#160;dB is ×4, not ×2, which is +3&amp;#160;dB. This might seem strange and confusing, but once again there is an explanation: in practice power is proportional to the &lt;em&gt;square&lt;/em&gt; of the field quantity. So, for example, when voltage is doubled, power quadruples. Or, said differently, if voltage is increased by 6&amp;#160;dB, then power increases by… 6&amp;#160;dB — which is why the rules makes sense.&lt;/p&gt;
&lt;h1 id="using-decibels-for-absolute-values"&gt;Using decibels for absolute values&lt;/h1&gt;
&lt;p&gt;In theory, one could stick with linear units when dealing with absolute values (e.g. 2&amp;#160;V), and use decibels when dealing with ratios (e.g. ×2). However, when a calculation involves both, the mental gymnastics can be challenging. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Not convinced? Try calculating 2&amp;#160;V × -11&amp;#160;dB by hand.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It would make more sense to use decibels for everything, including absolute quantities. Fortunately, that’s easy: we just need to agree on a reference, and then express in decibels the ratio between that reference and the quantity we wish to convey. The resulting decibel value is called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Level_%28logarithmic_quantity%29"&gt;level&lt;/a&gt;&lt;/em&gt;. For example, if the reference is 1&amp;#160;V, then the level of 2&amp;#160;V is +6&amp;#160;dB. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;And the calculation from the previous note becomes 6&amp;#160;dB - 11&amp;#160;dB = -5&amp;#160;dB. Much easier!&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In practice, the reference value is indicated by a suffix affixed to the unit. Here are some references commonly used in the three audio realms:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Quantity&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Equivalent level&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sound pressure&lt;/td&gt;
&lt;td&gt;20&amp;#160;µPa&lt;/td&gt;
&lt;td&gt;0 &lt;a href="https://en.wikipedia.org/wiki/Sound_pressure#Sound_pressure_level"&gt;dB SPL&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Voltage&lt;/td&gt;
&lt;td&gt;1&amp;#160;V&lt;/td&gt;
&lt;td&gt;0 &lt;a href="https://en.wikipedia.org/wiki/Decibel#Voltage"&gt;dBV&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Voltage&lt;/td&gt;
&lt;td&gt;~0.77&amp;#160;V&lt;/td&gt;
&lt;td&gt;0 &lt;a href="https://en.wikipedia.org/wiki/Decibel#Voltage"&gt;dBu&lt;/a&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Yes, there are two different references in common use for voltages. That’s sad, but that’s the way it is. To convert from one to the other, add or subtract ~2.2&amp;#160;dB.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample value&lt;/td&gt;
&lt;td&gt;Full scale&lt;/td&gt;
&lt;td&gt;0 &lt;a href="https://en.wikipedia.org/wiki/DBFS"&gt;dBFS&lt;/a&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;In other words, 0&amp;#160;dBFS is the maximum level before digital clipping (truncation) occurs. Thus valid amplitudes have negative dBFS values. Or at least that’s how it’s supposed to work. The definition of dBFS can be quite fuzzy and ambiguous, as &lt;a href="https://en.wikipedia.org/wiki/DBFS#RMS_levels"&gt;explained&lt;/a&gt; in that Wikipedia page. Caveat emptor.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Sometimes the suffix is omitted and has to be inferred from the context. For example, it is fairly common to find “dB SPL” written as simply “dB”, especially in mainstream publications. This is best avoided as it can lead to confusion between ratios and levels. Conversely, “dBr” (“relative”) is sometimes used to make it explicit that decibels are used to express a ratio, not a level.&lt;/p&gt;
&lt;p&gt;Additional suffixes and variants are often used to convey additional information. The most common examples include “peak” or “RMS”, which denote &lt;a href="https://factualaudio.com/post/amplitude/"&gt;different ways&lt;/a&gt; of looking at the amplitude of the signal, and “dBA”, which indicates the use of &lt;a href="https://en.wikipedia.org/wiki/A-weighting"&gt;A-weighting&lt;/a&gt;. More on these in later posts.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>From bytes to your eardrum</title>
      <link>https://factualaudio.com/post/life/</link>
      <pubDate>Tue, 21 Nov 2017 21:02:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/life/</guid>
      <description>
&lt;p&gt;In the &lt;a href="https://factualaudio.com/post/anatomy/"&gt;previous post&lt;/a&gt;, I described how an audio signal is represented. Now, let’s discuss the various physical forms that audio signals take as they travel through each stage of an audio playback system.&lt;/p&gt;
&lt;p&gt;For the sake of this discussion, I am going to assume the most common and straightforward use case: playing a &lt;em&gt;digital stream&lt;/em&gt; over loudspeakers (or headphones). By “digital stream” I mean any audio signal that is processed by a computer or computer-like system; that can be anything including a MP3&amp;#160;file, online video, online music streaming, the soundtrack of a Blu-ray disc, etc. This does not include analogue media such as vinyl discs or cassette tapes.&lt;/p&gt;
&lt;p&gt;Before this digital content can reach your eardrums, it has to go through a series of steps. Between these steps, the audio signal is materialized in different ways depending on which part of the audio “pipeline” we are looking at. In this post I refer to these concrete representations as &lt;em&gt;realms&lt;/em&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;“realm” is not a widely used term — the term “domain” is normally used. However, I felt that this could create confusion with &lt;em&gt;time domain&lt;/em&gt; and &lt;em&gt;frequency domain&lt;/em&gt;, which are completely unrelated concepts.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . I am going to start at the source and then make my way through to the listener.&lt;/p&gt;
&lt;p&gt;To keep things clear and simple, the example signal I’ll use throughout this post is a monophonic (one channel) 1&amp;#160;kHz sine wave. For all intents of purposes, each additional channel can be assumed to act like a completely separate audio signal that takes a similar path through the system.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/diagrams/realms.fingerprint-0ee5447.png" alt="Audio pipeline diagram" data-svg-alternative="https://factualaudio.com/diagrams/realms.fingerprint-346c4fa.svg"&gt;&lt;/span&gt;
&lt;h1 id="the-digital-realm"&gt;The digital realm&lt;/h1&gt;
&lt;p&gt;It all starts within the digital device, which can be any computer or computer-like gadget (PC, smartphone, Bluetooth receiver, etc.). Most devices read or receive audio data in &lt;em&gt;digitally compressed&lt;/em&gt; form. Popular digital compression algorithms include &lt;a href="https://en.wikipedia.org/wiki/MP3"&gt;MP3&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Advanced_Audio_Coding"&gt;AAC&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/FLAC"&gt;FLAC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Data_compression"&gt;Digital compression&lt;/a&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Not to be confused with &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Dynamic_range_compression"&gt;dynamic range compression&lt;/a&gt;&lt;/em&gt;, which is completely unrelated.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; is a complicated subject, which I won’t dig into further in this post. In any case, the data first goes through a &lt;em&gt;decoder&lt;/em&gt; which converts the compressed signal into uncompressed form, which looks like this:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz-digital.fingerprint-7b40d5a.png" alt="Digitally sampled 1kHz sine waveform" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz-digital.fingerprint-a06ebe2.svg"&gt;&lt;/span&gt;
&lt;p&gt;This plot shows that, in the digital realm, audio is not represented by a continuous, smoothly changing signal — instead, all we have are regularly-spaced “snapshots” that indicate what the signal amplitude is at some point in time. This is called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Discrete-time_signal"&gt;discrete signal&lt;/a&gt;&lt;/em&gt;, and the “snapshots” are called &lt;em&gt;samples&lt;/em&gt;. In this example, we have 44100&amp;#160;samples every second, or more formally, the &lt;em&gt;sample rate&lt;/em&gt; is 44.1&amp;#160;kHz. Such a sample rate is standard for music — other types of content, such as movies or games, use a slightly higher rate, 48&amp;#160;kHz, for mostly historical reasons.&lt;/p&gt;
&lt;p&gt;Because memory is not infinite, each sample value has a finite precision. In practice, each sample is typically converted to a signed integer with a precision, or &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Audio_bit_depth"&gt;bit depth&lt;/a&gt;&lt;/em&gt;, of 16&amp;#160;bits. This process is called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Quantization_%28signal_processing%29"&gt;quantization&lt;/a&gt;&lt;/em&gt;. A 16-bit signed integer can take a value from -32768&amp;#160;to +32767. Samples outside of this range cannot be represented, and will be clamped to the nearest possible value; this is called &lt;em&gt;digital &lt;a href="https://en.wikipedia.org/wiki/Clipping_%28signal_processing%29"&gt;clipping&lt;/a&gt;&lt;/em&gt;, and is best avoided as it sounds quite bad. A signal that peaks at the highest possible amplitude without clipping is called a &lt;em&gt;full-range&lt;/em&gt; or &lt;em&gt;full-scale&lt;/em&gt; signal &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;a href="https://webstore.iec.ch/publication/5664"&gt;IEC 61606–1:2009&lt;/a&gt;, &lt;em&gt;Digital audio parts — Basic measurement methods of audio characteristics — General&lt;/em&gt;, §3.1.10&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; .&lt;/p&gt;
&lt;p&gt;Finally, the signal is physically represented simply by transmitting the value of each point, or &lt;em&gt;sample&lt;/em&gt;, one after the other. For example, the above signal is transmitted as 4653, 9211, 13583, etc. in the form of binary numbers. This way of transmitting the signal is called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Pulse-code_modulation"&gt;pulse-code modulation&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This section just skirted the surface of how digital audio works. The details of how sampled signals behave in practice are often counter-intuitive; as a result, misrepresentation of digital audio phenomena is quite commonplace in the audiophile community, leading to confusion and misguided advice. Digital audio expert &lt;a href="https://en.wikipedia.org/wiki/Chris_Montgomery"&gt;Chris Montgomery&lt;/a&gt; produced a &lt;a href="https://xiph.org/video/"&gt;series of videos&lt;/a&gt; that explains these complex phenomena with very clear and straightforward examples — it is a highly recommended resource if you wish to learn more about the digital realm.&lt;/p&gt;
&lt;h1 id="the-analog-realm"&gt;The analog realm&lt;/h1&gt;
&lt;p&gt;Loudspeakers and headphones cannot receive a digital signal; it has to be converted to an &lt;em&gt;analog&lt;/em&gt; signal first. This conversion is done in an electronic circuit appropriately named the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Digital-to-analog_converter"&gt;digital-to-analog converter&lt;/a&gt;&lt;/em&gt;, or DAC. This is where computer engineering ends and electrical engineering begins. The main task of the DAC is to take each sample value and convert it to some electrical &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Voltage"&gt;voltage&lt;/a&gt;&lt;/em&gt; on its output pins. The resulting signal looks like the following:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz-analog.fingerprint-263e374.png" alt="1kHz sine wave with voltage scale" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz-analog.fingerprint-ebd32f7.svg"&gt;&lt;/span&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; In the plot above, the unit used for the vertical scale is the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Volt"&gt;volt&lt;/a&gt;&lt;/em&gt;. In other words, the amplitude of the audio signal in the analog domain is defined by its &lt;em&gt;voltage&lt;/em&gt;. It is &lt;em&gt;not&lt;/em&gt; defined by &lt;a href="https://en.wikipedia.org/wiki/Electric_current"&gt;current&lt;/a&gt; nor &lt;a href="https://en.wikipedia.org/wiki/Electric_power"&gt;power&lt;/a&gt;. Even when the signal is used as the input of a loudspeaker, it is still voltage that determines the sound that comes out; power dissipation is a &lt;em&gt;consequence&lt;/em&gt;, not a &lt;em&gt;cause&lt;/em&gt;, of the audio signal flowing through the loudspeaker. As Pat Brown &lt;a href="http://www.prosoundtraining.com/site/author/pat-brown/meaningful-metrics-the-use-and-abuse-of-loudspeaker-power-ratings/"&gt;elegantly puts it&lt;/a&gt;: “power is &lt;em&gt;drawn&lt;/em&gt;, not applied”. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Another way to state this is to say that properly engineered analog audio devices act as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Voltage_source"&gt;voltage sources&lt;/a&gt;&lt;/em&gt; (or sinks), which are connected to each other by way of &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Impedance_bridging"&gt;impedance bridging&lt;/a&gt;&lt;/em&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The DAC took our discrete signal and converted it into a continuous electrical signal, whose voltage is (hopefully) &lt;em&gt;proportional&lt;/em&gt; to the digital sample value. The central (mean) value of the signal, called the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/DC_bias"&gt;DC offset&lt;/a&gt;&lt;/em&gt;, is zero volts; the signal swings around that central value, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Alternating_current"&gt;alternating&lt;/a&gt;&lt;/em&gt; between positive and negative voltage. In this example, our full-scale digital stream was converted to an analog signal that swings between -1.41&amp;#160;V and +1.41&amp;#160;V. Depending on the specific model of DAC used, its volume control setting (if any) and the signals involved, these numbers can vary — typical peak amplitude can go as low as 0.5&amp;#160;V &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Wikipedia, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Line_level#Nominal_levels"&gt;Nominal levels&lt;/a&gt;&lt;/em&gt;, peak amplitude for consumer audio&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; or as high as 2.8&amp;#160;V &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;a href="https://webstore.iec.ch/publication/6142"&gt;IEC 61938:2013&lt;/a&gt;, &lt;em&gt;Guide to the recommended characteristics of analogue interfaces to achieve interoperability&lt;/em&gt;, §8.2.1&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; .&lt;/p&gt;
&lt;p&gt;The amount of &lt;em&gt;current&lt;/em&gt; or &lt;em&gt;power&lt;/em&gt; transferred from the source of an analog signal (e.g. a DAC) to the equipment plugged in at the other end of the cable (the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Electrical_load"&gt;load&lt;/a&gt;&lt;/em&gt;, e.g. a loudspeaker) is determined by the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Electrical_impedance"&gt;impedance&lt;/a&gt;&lt;/em&gt; of the load, also known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Input_impedance"&gt;input impedance&lt;/a&gt;&lt;/em&gt;. According to &lt;a href="https://en.wikipedia.org/wiki/Ohm's_law"&gt;Ohm’s law&lt;/a&gt;, the lower the impedance, the more current, and therefore power, will be required to sustain a given voltage.&lt;/p&gt;
&lt;p&gt;DACs, as well as most other types of analog audio equipment (such as filters or mixers), are not designed to provide significant amounts of power. Instead, they are meant to be connected to a high-impedance load, normally 20&amp;#160;kΩ or higher &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;a href="https://webstore.iec.ch/publication/6142"&gt;IEC 61938:2013&lt;/a&gt;, &lt;em&gt;Guide to the recommended characteristics of analogue interfaces to achieve interoperability&lt;/em&gt;, §8.2.1&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; . This means that the load is acting much like a &lt;a href="https://en.wikipedia.org/wiki/Voltmeter"&gt;voltmeter&lt;/a&gt; or oscilloscope — it is “peeking” at the input voltage without drawing significant power from it. Such a signal that carries some voltage but very little power is called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Line_level"&gt;line-level&lt;/a&gt;&lt;/em&gt; signal.&lt;/p&gt;
&lt;p&gt;On the other hand, loudspeakers (and headphones to a lesser extent) are low-impedance devices — often between 4&amp;#160;Ω and 8&amp;#160;Ω in the case of speakers. This is because they operate under a relatively low voltage, but require a lot of power. For example, most speakers will happily produce comfortably loud sound with as little as 6&amp;#160;V, but might consume as much as 9 &lt;a href="https://en.wikipedia.org/wiki/Watt"&gt;watts&lt;/a&gt; while doing so &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;From the numbers given a keen eye will &lt;a href="http://www.sengpielaudio.com/calculator-ohm.htm"&gt;deduce&lt;/a&gt; that this example speaker has an impedance of 4&amp;#160;Ω. One thing to note is that loudspeaker impedance is highly dependent on the frequency of the signal, making the use of one number an oversimplification. The impedance that manufacturers advertise, called the &lt;em&gt;rated impedance&lt;/em&gt;, is 1.25&amp;#160;times the &lt;em&gt;minimum&lt;/em&gt; impedance of the speaker across its rated frequency range. (see &lt;a href="https://webstore.iec.ch/publication/1223"&gt;IEC 60268–5:2003&lt;/a&gt;, &lt;em&gt;Loudspeakers&lt;/em&gt;, §16.1)&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . Line-level equipment is not designed to provide such a large amount of power.&lt;/p&gt;
&lt;p&gt;This problem is solved by using a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Audio_power_amplifier"&gt;power amplifier&lt;/a&gt;&lt;/em&gt;. This is a component that conveniently provides a high-impedance input for connecting line-level equipment, while exposing an output that is capable of providing large amounts of power, such as 10W or more, to a low-impedance load. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;In practice, most amplifiers are also capable of increasing the voltage (amplitude) of the signal; this is called the &lt;a href="https://en.wikipedia.org/wiki/Gain_%28electronics%29"&gt;gain&lt;/a&gt; of the amplifier. This is because most loudspeakers require voltages that are somewhat higher than line level in order to play loud enough. Still, the primary goal of a power amplifier is to provide power, not to increase voltage.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Such outputs provide so-called &lt;em&gt;speaker-level signals&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In some home audio systems, the DAC and the amplifier are integrated into one single device, which is called an &lt;em&gt;integrated amplifier&lt;/em&gt; or more commonly an &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/AV_receiver"&gt;AV receiver&lt;/a&gt;&lt;/em&gt; (AVR).&lt;/p&gt;
&lt;h1 id="the-acoustic-realm"&gt;The acoustic realm&lt;/h1&gt;
&lt;p&gt;Finally, in order to reach your ears, the analog signal must be converted to an &lt;em&gt;acoustic&lt;/em&gt; signal, that is, actual &lt;a href="https://en.wikipedia.org/wiki/Sound"&gt;sound waves&lt;/a&gt;. This is accomplished using a device called an &lt;em&gt;electroacoustic &lt;a href="https://en.wikipedia.org/wiki/Transducer"&gt;transducer&lt;/a&gt;&lt;/em&gt;, or &lt;em&gt;driver&lt;/em&gt;. The output of a driver when excited with our example signal, as measured at some point in front of it, might look like the following:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz-acoustic.fingerprint-675cab2.png" alt="1kHz sine wave with pressure scale" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz-acoustic.fingerprint-0e739d8.svg"&gt;&lt;/span&gt;
&lt;p&gt;Note the change of vertical scale. We’re not dealing with voltage anymore — amplitude takes the form of &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Sound_pressure"&gt;sound pressure&lt;/a&gt;&lt;/em&gt; instead. Indeed, sound is a physical phenomenon in which transient changes in pressure (&lt;em&gt;compression&lt;/em&gt;, &lt;em&gt;rarefaction&lt;/em&gt;) produced by the vibration of a &lt;em&gt;sound source&lt;/em&gt; propagate through the space around it. In other words, it is a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Longitudinal_wave"&gt;longitudinal wave&lt;/a&gt;&lt;/em&gt;. Sound pressure, expressed in &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Pascal_%28unit%29"&gt;Pascals&lt;/a&gt;&lt;/em&gt; (Pa), quantifies the difference between normal atmospheric pressure and some local, dynamic change in pressure, at a given point in time and space. The human ear is equipped to detect these changes, which are then — finally! — perceived as sound by the human brain.&lt;/p&gt;
&lt;p&gt;An ideal transducer will produce sound pressure &lt;em&gt;proportional&lt;/em&gt; to the voltage applied to it, like in the above waveform. However, it is difficult to design a driver that is capable of doing that across the entire range of audible frequencies. Consequently, a number of transducer types are available, which are commonly referred to as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Subwoofer"&gt;subwoofers&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Woofer"&gt;woofers&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Mid-range_speaker"&gt;midranges&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Tweeter"&gt;tweeters&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In order to reproduce the entire range of human hearing, several of these drivers — often two or three — are assembled inside a single “box”, called the &lt;em&gt;enclosure&lt;/em&gt;. In most designs the drivers are mounted flush with one side of the box, which is called the &lt;em&gt;front baffle&lt;/em&gt;. An electrical circuit called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Audio_crossover"&gt;crossover&lt;/a&gt;&lt;/em&gt; splits the input signal into the frequency ranges appropriate for each driver. The resulting device is called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Loudspeaker"&gt;loudspeaker&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;What I’ve described here is called a &lt;em&gt;passive&lt;/em&gt; loudspeaker, which is the most common type in consumer “Hi-Fi” systems. Sometimes the amplifier and speaker are integrated into the same device; this is called an &lt;em&gt;active&lt;/em&gt; or &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Powered_speakers"&gt;powered&lt;/a&gt;&lt;/em&gt; speaker. Examples include professional “studio monitor” speakers, which have line-level inputs. Other products, such as “Bluetooth speakers”, go one step further and throw in a DAC as well for a completely integrated solution.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Headphones"&gt;Headphones&lt;/a&gt;&lt;/em&gt; are a special case and typically only have one driver per channel, which makes them simpler. Conceptually, a headphone is akin to a miniature loudspeaker. Because of their proximity to the ear, they don’t have to produce as much sound pressure; therefore they require much less power to operate (often less than 1&amp;#160;mW).&lt;/p&gt;
&lt;p&gt;One notable aspect of the acoustic realm is that sound propagates in all three dimensions — the audio signal (sound pressure) is not the same at every point in space. In particular, speakers exhibit &lt;em&gt;radiation patterns&lt;/em&gt; that vary with angle and frequency, and the sound they emit can bounce off surfaces (&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Reflection_%28physics%29#Sound_reflection"&gt;reflection&lt;/a&gt;&lt;/em&gt;). This in turn means that they interact with their environment (the listening room, or, in the case of headphones, the listener’s head) in ways that are complex and difficult to predict but nonetheless have an enormous impact on how the radiated sound will be perceived by a human listener. This makes choosing and configuring a speaker system quite the challenge. Hopefully, future posts on this blog will provide some pointers.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Anatomy of an audio signal</title>
      <link>https://factualaudio.com/post/anatomy/</link>
      <pubDate>Tue, 21 Nov 2017 21:01:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/anatomy/</guid>
      <description>
&lt;p&gt;This inaugural Factual post starts from first principles, by laying down some of the fundamental foundations necessary to start reasoning about audio signals. I will then build on these principles in the posts to follow.&lt;/p&gt;
&lt;h1 id="the-time-domain"&gt;The time domain&lt;/h1&gt;
&lt;p&gt;An audio signal is an oscillating phenomenon: it is defined by a quantity that alternatively increases and decreases over time while keeping in the vicinity of some central value.&lt;/p&gt;
&lt;p&gt;Out of the infinity of shapes that an audio signal can take, probably the simplest is a &lt;em&gt;pure tone&lt;/em&gt;, also called a &lt;em&gt;sine wave&lt;/em&gt; from the name of the &lt;a href="https://en.wikipedia.org/wiki/Sine"&gt;mathematical function&lt;/a&gt; that it describes. Here is an example of a sine wave:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz.fingerprint-5b89774.png" alt="1&amp;#160;kHz pure tone waveform" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz.fingerprint-3686aa5.svg"&gt;&lt;/span&gt;
&lt;p&gt;The horizontal axis is time, which is why it is often said that this representation shows the signal in the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Time_domain"&gt;time domain&lt;/a&gt;&lt;/em&gt; (another term is &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Waveform"&gt;waveform&lt;/a&gt;&lt;/em&gt;). The above signal oscillates around the central value represented by the horizontal line. According to the horizontal scale, this particular signal repeats once every millisecond: this is its &lt;em&gt;period&lt;/em&gt;, also known as the &lt;em&gt;cycle&lt;/em&gt;. Said differently, the signal repeats one thousand times per second: it has a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Frequency"&gt;frequency&lt;/a&gt;&lt;/em&gt; of 1000 &lt;a href="https://en.wikipedia.org/wiki/Hertz"&gt;hertz&lt;/a&gt;. In order to be audible, the frequency of the signal must sit between 20&amp;#160;Hz and 20&amp;#160;kHz: this interval is known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Hearing_range"&gt;audible range&lt;/a&gt;&lt;/em&gt; of the human hearing system.&lt;/p&gt;
&lt;p&gt;In the image above, the height of the signal is known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Amplitude"&gt;amplitude&lt;/a&gt;&lt;/em&gt; (the term &lt;em&gt;magnitude&lt;/em&gt; is also used). I deliberately left out the vertical scale and unit because they depend on the context — more on this to follow in the post about &lt;a href="https://factualaudio.com/post/life/"&gt;audio realms&lt;/a&gt;. Another complication is that the “height” of the curve can be defined in multiple different ways — which are explored in a &lt;a href="https://factualaudio.com/post/amplitude/"&gt;separate post&lt;/a&gt; as well.&lt;/p&gt;
&lt;p&gt;Amplitude is related to &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Loudness"&gt;loudness&lt;/a&gt;&lt;/em&gt;, in the sense that if we take a signal and increase its amplitude (by &lt;em&gt;amplifying&lt;/em&gt; its oscillations), the human hearing system will perceive the signal to be louder. Likewise, if we decrease its amplitude (by &lt;em&gt;attenuating&lt;/em&gt;), it will be perceived as being quieter.&lt;/p&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; This relationship between amplitude and loudness does not necessarily hold when comparing signals that have differing frequencies. This is due to the fact that the human hearing system does not perceive all frequencies as being equally loud &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;The effect can be quantified using &lt;a href="https://en.wikipedia.org/wiki/Equal-loudness_contour"&gt;equal-loudness contours&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . For example, if you listen to a 30&amp;#160;Hz tone and then to a 2&amp;#160;kHz tone of equal amplitude, the latter will sound much louder than the former.&lt;/p&gt;
&lt;p&gt;Of course, most audio content is not a pure tone. In practice, an audio signal for, say, music, might look like this:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/piano-c5-wave.fingerprint-7700c23.png" alt="Piano C5&amp;#160;waveform" data-svg-alternative="https://factualaudio.com/plots/piano-c5-wave.fingerprint-ab40946.svg"&gt;&lt;/span&gt;
&lt;p&gt;As the above image shows, a musical signal is way more complex than a pure tone. And that’s not even a complicated musical piece — this is pianist Joohyun Park, solo, playing a single note &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Specifically, this is one of the first notes played at the beginning of the &lt;em&gt;Allegro&lt;/em&gt; track from &lt;em&gt;&lt;a href="http://www.bearmccreary.com/blog/blog/battlestar-galactica-3/battlestar-galactica-solo-piano-cd/"&gt;The Music of Battlestar Galactica for Solo Piano&lt;/a&gt;&lt;/em&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . What’s really problematic, however, is that this representation doesn’t seem to relate to our perception at all — to the naked eye, it doesn’t look like a musical note played on a piano.&lt;/p&gt;
&lt;h1 id="the-frequency-domain"&gt;The frequency domain&lt;/h1&gt;
&lt;p&gt;In order to make sense of such complex signals, we need a better way to look at the data. Fortunately, the above signal can be decomposed into a number of pure tones of various frequencies and amplitudes, thanks to the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Superposition_principle"&gt;superposition principle&lt;/a&gt;&lt;/em&gt;. The mathematical tool used to do the decomposition is called the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Fourier_transform"&gt;Fourier transform&lt;/a&gt;&lt;/em&gt;. For example, if we were to apply the Fourier transform to our first pure tone example, the result could be represented as follows:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-spectrum-1khz.fingerprint-5516420.png" alt="1&amp;#160;kHz pure tone spectrum" data-svg-alternative="https://factualaudio.com/plots/sine-spectrum-1khz.fingerprint-814fb1f.svg"&gt;&lt;/span&gt;
&lt;p&gt;The vertical axis is still amplitude, but the horizontal axis has changed — it now represents frequency. This representation shows the signal in the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Frequency_domain"&gt;frequency domain&lt;/a&gt;&lt;/em&gt;, or, in other words, it shows the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Spectral_density"&gt;spectral density&lt;/a&gt;&lt;/em&gt; (often simply called &lt;em&gt;spectrum&lt;/em&gt;) of the signal.&lt;/p&gt;
&lt;p&gt;A keen eye might have noticed that the horizontal axis is using a &lt;a href="https://en.wikipedia.org/wiki/Logarithmic_scale#Graphic_representation"&gt;logarithmic scale&lt;/a&gt;, which is commonplace for this type of plot. This scale provides a better view of how we perceive sound: it is very easy to hear the difference between a 100&amp;#160;Hz tone and a 200&amp;#160;Hz tone, but the same cannot be said about 10000&amp;#160;Hz and 10100&amp;#160;Hz tones, even though the difference is still 100&amp;#160;Hz. This is because in the former case, there is a 100% increase, while in the latter case, the increase is only 1%. In other words, the human auditory system perceives &lt;em&gt;relative change&lt;/em&gt;, as opposed to absolute change &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This is consistent with other human senses, as predicted by the &lt;a href="https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law"&gt;Weber-Fechner law&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . The term &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Octave"&gt;octave&lt;/a&gt;&lt;/em&gt; is used to describe a frequency factor of two; for example, the range 2&amp;#160;kHz to 8&amp;#160;kHz is two octaves wide. The term &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Decade_%28log_scale%29"&gt;decade&lt;/a&gt;&lt;/em&gt; is also sometimes used, and describes a tenfold increase in frequency.&lt;/p&gt;
&lt;p&gt;The above plot is showing us that the signal can be decomposed into a single 1&amp;#160;kHz tone, but we already knew that. What’s more interesting is what happens when we apply the Fourier transform to the musical signal:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/piano-c5-spectrum.fingerprint-32fe3c5.png" alt="Piano C5&amp;#160;spectrum" data-svg-alternative="https://factualaudio.com/plots/piano-c5-spectrum.fingerprint-da791dd.svg"&gt;&lt;/span&gt;
&lt;p&gt;Here things become interesting. This plot is telling us that our musical example can be decomposed into a 260&amp;#160;Hz tone with high amplitude, combined with 520&amp;#160;Hz and 780&amp;#160;Hz tones of lower amplitude.&lt;/p&gt;
&lt;p&gt;Such a result is typical of a recording of an instrument playing a single note. The first tone, at 260&amp;#160;Hz, is called the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Fundamental_frequency"&gt;fundamental&lt;/a&gt;&lt;/em&gt; and indicates the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Pitch_%28music%29"&gt;pitch&lt;/a&gt;&lt;/em&gt; of the sound, in other words the note being played, &lt;a href="https://en.wikipedia.org/wiki/C_%28musical_note%29#Designation_by_octave"&gt;C5&lt;/a&gt; in this example. The 520&amp;#160;Hz and 780&amp;#160;Hz tones, because they are &lt;em&gt;multiples&lt;/em&gt; of the fundamental, are called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Harmonic"&gt;harmonics&lt;/a&gt;&lt;/em&gt;. They are interpreted by the human hearing system to help determine the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Timbre"&gt;timbre&lt;/a&gt;&lt;/em&gt; of the instrument. If the same note was being played on say, a flute or a violin, the frequency of the fundamental would be the same but the relative amplitudes of the harmonics would be different.&lt;/p&gt;
&lt;p&gt;This is interesting because we can directly relate what we see on the plot to how the sound will be &lt;em&gt;perceived&lt;/em&gt;, i.e. what the signal sounds like. Of course interpreting this data still requires some effort — most people wouldn’t be able to tell “of course that’s a piano playing a C5” just by eyeballing the above image. Furthermore, if I had used a more complex example (such as a symphonic orchestra playing in unison), the spectrum would have been just as unreadable as the waveform. Nevertheless, in practice, the spectrum often provides a much more useful view from a &lt;em&gt;perceptual&lt;/em&gt; perspective. This is why audio engineers will often ignore the time domain, instead focusing their efforts on the frequency domain.&lt;/p&gt;
&lt;p&gt;Frequency-domain data can be converted back to the time domain using the appropriately-named &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Fourier_inversion_theorem"&gt;inverse Fourier transform&lt;/a&gt;&lt;/em&gt;. One might wonder if any information gets lost during these conversions. From a purely mathematical point of view, &lt;a href="https://en.wikipedia.org/wiki/Fourier_inversion_theorem"&gt;the answer is no&lt;/a&gt;, but there is a catch. The above plots do not show the full output of the Fourier transform. In reality, the result of the Fourier transform includes &lt;em&gt;amplitude&lt;/em&gt; information (which is shown above) and &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_%28waves%29"&gt;phase&lt;/a&gt;&lt;/em&gt; information (which I omitted). Amplitude determines the strength of the constituent tones, while phase indicates which part of the waveform cycle occurs at a specific point in time. As long as phase information is not discarded, it is possible to recover the original waveform, intact, simply by applying the inverse transform. That said, aside from a few specific scenarios (such as signal summation), phase is not nearly as prominent as amplitude in practical audio discussions, which is why I won’t dig further into it in this introductory post. I will revisit this topic in a &lt;a href="https://factualaudio.com/post/phase/"&gt;later post&lt;/a&gt;.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>About Factual Audio</title>
      <link>https://factualaudio.com/page/about/</link>
      <pubDate>Tue, 21 Nov 2017 21:00:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/page/about/</guid>
      <description>
&lt;p&gt;Factual Audio is dedicated to delivering objective, trustworthy, reliable, evidence-backed information about audio, in particular audio playback at home and on the move. This includes for example “Hi-Fi” systems, “home cinema” systems, and headphones. The main goal of this website is to educate the general public about audio matters through critical thinking and a sound, rational, scientific approach to assessing audio claims and products.&lt;/p&gt;
&lt;p&gt;Factual Audio is a completely independent entity with a strong focus on integrity, transparency and full disclosure. It does not have a commercial or monetary purpose, is not affiliated with any company or organization, does not run ads, and does not accept paid promotion of any kind.&lt;/p&gt;
&lt;p&gt;While Factual Audio strives to maintain a high standard of scientific accuracy, no-one is immune from making mistakes, and new discoveries can sometimes compel us to revise our understanding. If you believe that Factual Audio has made factually incorrect claims, used outdated information, misrepresented evidence, ignored contradictory evidence, or has failed to provide appropriate citations for controversial statements, please don’t hesitate to provide feedback by posting a comment on the article or to get in touch using the contact details below.&lt;/p&gt;
&lt;h1 id="source-code-license-and-copyright"&gt;Source code, license and copyright&lt;/h1&gt;
&lt;p&gt;The source code that this website is built from is freely available in a &lt;a href="https://github.com/factualaudio/factualaudio"&gt;public repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All article text, figures and images on this website are licensed under the &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0&amp;#160;International License&lt;/a&gt;, unless explicitly mentioned otherwise. This also applies to source code.&lt;/p&gt;
&lt;p&gt;Note that most decorative or stylistic elements come from the &lt;a href="https://gohugo.io/"&gt;Hugo&lt;/a&gt;, &lt;a href="https://github.com/roryg/ghostwriter"&gt;Hugo Ghostwriter theme&lt;/a&gt; and &lt;a href="http://www.bigfootjs.com/"&gt;bigfoot.js&lt;/a&gt; projects, which come with their own licenses.&lt;/p&gt;
&lt;h1 id="contact"&gt;Contact&lt;/h1&gt;
&lt;p&gt;Owner and editor: &lt;a href="mailto:etienne@edechamps.fr"&gt;Etienne Dechamps&lt;/a&gt;&lt;/p&gt;

</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
  <channel>
    <title>Factual Audio</title>
    <link>https://factualaudio.com/</link>
    <description>Recent content on Factual Audio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>etienne@edechamps.fr (Etienne Dechamps)</managingEditor>
    <webMaster>etienne@edechamps.fr (Etienne Dechamps)</webMaster>
    <copyright>Etienne Dechamps</copyright>
    <lastBuildDate>Sun, 21 Jan 2018 23:18:15 +0000</lastBuildDate>
    
        <atom:link href="https://factualaudio.com/index.xml" rel="self" type="application/rss+xml"/>
    
    
    <item>
      <title>Reasoning about phase</title>
      <link>https://factualaudio.com/post/phase/</link>
      <pubDate>Sun, 21 Jan 2018 23:18:15 +0000</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/phase/</guid>
      <description>
&lt;p&gt;So far, when discussing the various frequency components of a signal — i.e. the sine waves that &lt;a href="https://factualaudio.com/post/anatomy/#the-frequency-domain"&gt;together make up&lt;/a&gt; the signal — I’ve mostly focused on the &lt;em&gt;&lt;a href="https://factualaudio.com/post/amplitude/"&gt;amplitude&lt;/a&gt;&lt;/em&gt; of these sine waves. In order to keep things simple and short, I’ve been purposefully evading the subject of &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_(waves)"&gt;phase&lt;/a&gt;&lt;/em&gt;, an often-overlooked property of waveforms. Now it’s time to face the music (pun intended) and fill this gap, as this concept will be useful in future posts.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz-phase.fingerprint-9e1972c.png" alt="Four sine waves with different phases" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz-phase.fingerprint-aebcfbf.svg"&gt;&lt;/span&gt;
&lt;p&gt;The above plot shows four different sine waves. Contrary to what I’ve discussed previously on this blog, these waves don’t differ in frequency nor amplitude. Instead, their cycles are offset from each other in time. In other words, they have different phase.&lt;/p&gt;
&lt;p&gt;Phase indicates what part of the wave cycle is occurring at a given point in time. When not specified, that moment is conventionally defined as the origin of time (i.e. t=0). For example, in the above plot, the phase of the solid blue sine wave is zero, because its cycle starts at the origin. In contrast, at the same instant, the other sine waves are at a quarter, a half, and three quarters of their cycles, respectively.&lt;/p&gt;
&lt;p&gt;When a sine wave reaches the end of its cycle, it’s right back when it started and a new cycle begins again. In that way, progressing through the cycles of a sine wave is a bit like running around in circles. Hence it is no surprise that the terminology used when reasoning about phase is that of turns and angles. It seems natural in that context to define a &lt;a href="https://en.wikipedia.org/wiki/Turn_(geometry)"&gt;full cycle&lt;/a&gt; as 360 &lt;a href="https://en.wikipedia.org/wiki/Degree_(angle)"&gt;degrees&lt;/a&gt; — or, alternatively, 2π &lt;a href="https://en.wikipedia.org/wiki/Radian"&gt;radians&lt;/a&gt; — and from there, the other sine waves shown above could be defined as fractions of a full cycle: 90° (½π rad), 180° (π rad) and 270° (³⁄₂π rad). &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;From a mathematical perspective, the analogy goes &lt;a href="https://commons.wikimedia.org/wiki/File:ComplexSinInATimeAxe.gif"&gt;much farther&lt;/a&gt; than this. These concepts are all variations on the same common theme.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="phase-in-the-frequency-domain"&gt;Phase in the frequency domain&lt;/h1&gt;
&lt;p&gt;We’ve &lt;a href="https://factualaudio.com/post/anatomy/#the-frequency-domain"&gt;seen previously&lt;/a&gt; that the Fourier transform can be used to decompose any signal into a number of constituent sine waves — one per discrete frequency. In addition to the amplitude of each sine wave, the output of the Fourier transform (i.e. the spectrum) also contains their phase. For example, here is the phase information that the Fourier transform produces for each of the four above signals: &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;For the sake of consistency and to avoid confusion, I cheated a bit here — the real components of the Fourier transform are cosines, not sines, so strictly speaking the output should be offset by 90°. This is purely arbitrary and doesn’t matter one bit, though.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-spectrum-1khz-phase.fingerprint-89bac21.png" alt="Four sine waves with different phases" data-svg-alternative="https://factualaudio.com/plots/sine-spectrum-1khz-phase.fingerprint-74af671.svg"&gt;&lt;/span&gt;
&lt;p&gt;We’ve also &lt;a href="https://factualaudio.com/post/distortion/#frequency-response-distortion"&gt;seen previously&lt;/a&gt; that a linear system can alter the amplitude of the frequency components that flow through it. In the same way, a linear system can also alter their phase (which is &lt;em&gt;not&lt;/em&gt; the same thing as delaying them — see below), and the extent of these alterations can be shown on a plot, called the &lt;a href="https://en.wikipedia.org/wiki/Phase_response"&gt;phase response&lt;/a&gt;. Here is the phase response of a system that is similar to the example from that previous post:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/peak-phase-response.fingerprint-bb050bb.png" alt="Four sine waves with different phases" data-svg-alternative="https://factualaudio.com/plots/peak-phase-response.fingerprint-e0eef70.svg"&gt;&lt;/span&gt;
&lt;h1 id="phase-and-delay"&gt;Phase and delay&lt;/h1&gt;
&lt;p&gt;The sine waves I’m using as examples have a frequency of 1&amp;#160;kHz, which means that a cycle completes in 1&amp;#160;millisecond. From that perspective, it is tempting to think about phase as a time offset; for example, one might say that a 1&amp;#160;kHz wave with a phase of 90° is offset by one quarter of a millisecond relative to the reference. This quantity is known as the &lt;a href="https://en.wikipedia.org/wiki/Group_delay_and_phase_delay"&gt;phase delay&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is where things get tricky and misleading, though. One subtlety that is often overlooked when dealing with such concepts is that, mathematically speaking, phase is a property of a &lt;a href="https://en.wikipedia.org/wiki/Periodic_function"&gt;periodic signal&lt;/a&gt;, which implies a &lt;a href="https://en.wikipedia.org/wiki/Continuous_wave"&gt;continuous wave&lt;/a&gt; of infinite duration with no beginning and no end. Real-world signals of course do not meet these criteria. For most intents and purposes this does not really matter, but in some cases it does, and this is one of those. Specifically, it makes it very easy to misinterpret the meaning of phase delay.&lt;/p&gt;
&lt;p&gt;Imagine that, in the above plot, the sine waves went on forever on both sides of the figure. What does “delay” even mean in that context? I could say that the 90° wave is delayed by 0.25&amp;#160;ms relative to the 0° wave, but I could just as well flip things around and say that the 0° wave is delayed by 0.75&amp;#160;ms relative to the 90° wave. Since both signals extend infinitely to the left, it makes no sense to imply that one &lt;em&gt;started&lt;/em&gt; before the other. I could even go one step further and say that the 90° wave is delayed by, say, 10.25&amp;#160;ms (10&amp;#160;full cycles, plus a quarter cycle) and it would still mean the same thing. For this reason, the word “delay” needs to be handled very carefully in this context. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;For an illustration of what can happen when people get confused about these concepts, see this &lt;a href="https://groups.google.com/d/topic/comp.soft-sys.matlab/lBSRLMUV7nE/discussion"&gt;epic trainwreck of a debate&lt;/a&gt; where 46&amp;#160;participants spend 456&amp;#160;posts fighting to the death over the deep philosophical meaning of the word “delay”. The math is easy; it’s interpreting the results that’s hard.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Following the same logic, in terms of the phase itself, 90° is equivalent to -270°, 450°, -630°, etc. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;In fact, when using &lt;a href="https://en.wikipedia.org/wiki/Complex_number#Signal_analysis"&gt;complex numbers&lt;/a&gt; to do signal analysis, these are not just equivalent: they are the exact same number, landing in the same spot on the &lt;a href="https://en.wikipedia.org/wiki/Complex_plane"&gt;complex plane&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;You might come across phase response plots where the range of values exceed 360°. This is called &lt;em&gt;unwrapped phase&lt;/em&gt; and is meant mostly as a visual aid, making the graph more readable by avoiding sudden jumps at the boundaries of the range, and making some calculations easier. The underlying data is the same.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Then again, one might still be tempted to argue that this is a mostly theoretical distinction: after all, any real-world device that changes the phase of the signal that passes through it &lt;em&gt;has&lt;/em&gt; to apply some kind of delay, right?&lt;/p&gt;
&lt;p&gt;Well, not necessarily. As a counter-example, consider the case of a very basic device that reverts the &lt;a href="https://en.wikipedia.org/wiki/Electrical_polarity"&gt;polarity&lt;/a&gt; of the signal in the &lt;a href="https://factualaudio.com/post/life/#the-analog-realm"&gt;analog realm&lt;/a&gt;. In other words, it changes the sign of the voltage, which could be as simple as swapping two wires. The opposite of a sine wave is that same sine wave shifted by 180°, so in effect, this device shifts the phase of every frequency component of the input signal by 180°. One can even say that it has a phase delay of 0.5&amp;#160;ms at 1&amp;#160;kHz (and 1&amp;#160;ms at 500&amp;#160;Hz, and 0.25&amp;#160;ms at 2&amp;#160;kHz, and so on). Yet it would be impossible to say that this device &lt;em&gt;delays&lt;/em&gt; the signal for any reasonable definition of “delay” (i.e. in terms of information or energy propagation). It physically can’t, since there is no memory (buffer) for it to hold the signal in, and energy is coming out as soon as it gets in. This device exhibits phase shift and thus phase delay, yet &lt;em&gt;there is no actual delay&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For these reasons, in general, it is not possible to know the true delay of a device by making a single phase shift measurement at a single frequency. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;A pure delay produces a phase shift at every frequency equal to frequency times delay — which, incidentally, is very effective at making a mess in phase response plots. But delay cannot be directly recovered from the phase shift at a single frequency, because, as explained above, there is loss of information — the phase shift is constrained to a 360° range, making its interpretation ambiguous.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; It is, however, often possible to get more information about delay if a number of phase shift measurements are taken at various frequencies, such as by looking at a phase response plot like the one from the previous section. More specifically, this can be used to compute the &lt;a href="https://en.wikipedia.org/wiki/Group_delay_and_phase_delay"&gt;group delay&lt;/a&gt; of the device, albeit with a number of caveats related to measurement accuracy. Some devices, especially those that exhibit amplitude frequency response distortion like the above example, have a group delay that itself varies with frequency, which makes interpretation even trickier. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;If you’re not convinced how tricky this is, consider that the example I’ve used above (which is quite mundane, really) has &lt;em&gt;negative&lt;/em&gt; group delay in the low frequencies. Let that sink in for a moment. That doesn’t seem physically possible, &lt;a href="https://www.dsprelated.com/showarticle/54.php"&gt;but it is&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Noise and distortion</title>
      <link>https://factualaudio.com/post/distortion/</link>
      <pubDate>Tue, 21 Nov 2017 21:05:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/distortion/</guid>
      <description>
&lt;p&gt;As the audio signal makes its way through the different &lt;a href="https://factualaudio.com/post/life/"&gt;realms&lt;/a&gt; of the system, it travels through various digital, analog, and acoustic components that alter the signal in various ways. Some of these alterations might or might not be audible, or might only be audible under certain conditions. In most scenarios relevant to this blog they are undesirable side effects from limitations in the components that make up the system, though in some specific cases they can be deliberately introduced in pursuit of a specific goal (e.g. &lt;a href="https://en.wikipedia.org/wiki/Equalization_(audio)"&gt;equalization&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In order to build a high quality audio system, it is necessary to keep signal degradation (i.e. unwanted alteration) under control, and this requires a good understanding of what these alterations might be, what causes them, and how to avoid or alleviate them. Ever since the advent of sound reproduction more than a century ago this topic has been the subject of great debate, some thoughtful and innovative, some misguided or downright counter-productive. Hopefully this blog will do more of the former and less of the latter — but for now, this post serves as a brief introduction to the issues at hand.&lt;/p&gt;
&lt;p&gt;Signal alterations can be divided into three broad categories: &lt;em&gt;noise&lt;/em&gt;, &lt;em&gt;frequency response distortion&lt;/em&gt;, and &lt;em&gt;non-linear distortion&lt;/em&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;When used by itself without qualification, the term “distortion” can refer to some or all of these categories, depending on context.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Real-world systems exhibit all three kinds in varying amounts. What follows is a brief overview of the issues at hand; future posts will look at each of them more closely.&lt;/p&gt;
&lt;h1 id="noise"&gt;Noise&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Noise"&gt;Noise&lt;/a&gt;&lt;/em&gt; describes an alteration of the signal in which a separate, &lt;em&gt;unrelated&lt;/em&gt; signal is added (i.e. mixed in, superposed) to the original signal. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This is the definition I’ll use throughout this blog, pursuant to &lt;a href="https://webstore.iec.ch/publication/1219"&gt;IEC 60268–2&lt;/a&gt;. In other contexts noise might be used in a more specific way (e.g. broadband noise only), or in a more general way (e.g. signal differences introduced by non-linear distortion are considered to be part of the noise signal).&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; That additional signal has its own &lt;a href="https://factualaudio.com/post/anatomy/"&gt;characteristics&lt;/a&gt; including amplitude and frequency content (spectrum), which are combined with the characteristics of the original signal. Noise is often quantified by comparing the amplitude of the noise to the amplitude of the signal, a metric known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio"&gt;signal-to-noise ratio&lt;/a&gt;&lt;/em&gt; (the higher the better).&lt;/p&gt;
&lt;p&gt;Noise can appear in all three of the audio realms. In the digital realm it can take the form of &lt;a href="https://en.wikipedia.org/wiki/Dither#Digital_audio"&gt;dithering noise&lt;/a&gt; for example, though modern digital systems provide good enough performance that noise sits comfortably below the threshold of audibility. This is not always the case in the analog realm, where noise problems are the most common, the most objectionable, and the most pernicious — often the result of complex &lt;a href="https://en.wikipedia.org/wiki/Electromagnetic_interference"&gt;electromagnetic interference&lt;/a&gt; phenomena, subtle hardware defects, or compatibility issues. Finally, the acoustic realm is rife with often overlooked sources of noise from ordinary life, from the rumbling of an air conditioning unit to the occasional car driving down the street.&lt;/p&gt;
&lt;p&gt;Depending on amplitude and frequency content, noise might or might not constitute a problem in practice. For example, low-level broadband &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Colors_of_noise"&gt;colored noise&lt;/a&gt;&lt;/em&gt; (such as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/White_noise"&gt;white noise&lt;/a&gt;&lt;/em&gt;) will often go unnoticed because its spectrum is roughly similar to typical ambient noise that we are all continuously subjected to in our daily lives. &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;See Albert Donald G., Decato Stephen N., “&lt;a href="http://www.sciencedirect.com/science/article/pii/S0003682X16306120"&gt;Acoustic and seismic ambient noise measurements in urban and rural areas&lt;/a&gt;”, Applied Acoustics, 119, 135–143, (2017) for examples of ambient noise spectra.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; The same cannot be said of narrowband noise concentrated in specific frequencies. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Unfortunately that distinction is lost when noise measurements are condensed into a single number (such as signal-to-noise ratio), discarding spectral distribution in the process.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Furthermore, narrowband noise is more likely to affect the perception of minute detail in the original signal, due to an auditory phenomenon known as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Auditory_masking"&gt;masking&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/noisy-sine-wave.fingerprint-0c9d828.png" alt="Noisy sine waveform" data-svg-alternative="https://factualaudio.com/plots/noisy-sine-wave.fingerprint-8008abb.svg"&gt;&lt;/span&gt; &lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/noisy-sine-spectrum.fingerprint-8556bc4.png" alt="Noisy sine spectrum" data-svg-alternative="https://factualaudio.com/plots/noisy-sine-spectrum.fingerprint-c7bcf3a.svg"&gt;&lt;/span&gt;
&lt;p class="legend"&gt;&lt;em&gt;Waveform and spectrum of a sine wave affected by white noise. The noise spectrum, circled in red, is often called the “noise floor”. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;From that spectrum plot you might be tempted to conclude that the signal-to-noise ratio is about 50&amp;#160;dB. That would be wrong — it’s actually much worse, around 17&amp;#160;dB. You can’t read it directly from the graph because the noise is spread across multiple frequencies. This is a very common mistake when reading spectrum plots, which I might describe in more detail in a future post.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In a real-world scenario, noise is only really noticeable when the original signal is relatively quiet, such as when there is a break in a piece of music, because all that remains is the noise itself. Conversely, noise is inaudible when a significantly loud signal is playing, again because of masking (but this time in reverse). In practice, I tend to abide by the following rule of thumb: if you can’t hear anything when playing a silent signal, then your system is probably fine as far as noise is concerned.&lt;/p&gt;
&lt;h1 id="frequency-response-distortion"&gt;Frequency response distortion&lt;/h1&gt;
&lt;p&gt;In the first post on this site, I &lt;a href="https://factualaudio.com/post/anatomy/#the-frequency-domain"&gt;explained&lt;/a&gt; that an audio signal can be decomposed into a number of constituent signals of various frequencies. One way an audio component can alter the signal is by changing the amplitude (i.e. applying &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Gain_(electronics)"&gt;gain&lt;/a&gt;&lt;/em&gt;) on some of these frequencies more than others. This relationship between frequency and gain is known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Frequency_response"&gt;frequency response&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If this relationship is constant, i.e. the same amplitude multiplier is always applied to a given frequency regardless of the shape of the signal, then we are dealing with a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Linear_time-invariant_theory"&gt;linear time-invariant system&lt;/a&gt;&lt;/em&gt;. We can plot the frequency response on a graph, known as a &lt;em&gt;frequency response graph&lt;/em&gt; (or, more technically, a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Bode_plot"&gt;Bode plot&lt;/a&gt;&lt;/em&gt;). &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;One thing that I’ve omitted to keep things simple is that a linear time-invariant system is not just allowed to change the amplitude of individual frequency components, it can also change their &lt;a href="https://en.wikipedia.org/wiki/Phase_(waves)"&gt;phase&lt;/a&gt;. This is conveyed through the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_response"&gt;phase response&lt;/a&gt;&lt;/em&gt;. Technically the term “frequency response” encompasses both &lt;em&gt;magnitude response&lt;/em&gt; and &lt;em&gt;phase response&lt;/em&gt;, though the latter is often dismissed for lack of relevance in most audio discussions. More on the phase response in the &lt;a href="https://factualaudio.com/post/phase/#phase-in-the-frequency-domain"&gt;next post&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/peak-frequency-response.fingerprint-000fac9.png" alt="Frequency response with 1&amp;#160;kHz 6&amp;#160;dB resonance" data-svg-alternative="https://factualaudio.com/plots/peak-frequency-response.fingerprint-9d34e77.svg"&gt;&lt;/span&gt;
&lt;p class="legend"&gt;&lt;em&gt;The frequency response of a system that amplifies frequencies around 1&amp;#160;kHz by about 6&amp;#160;dB. This particular type of distortion is called a &lt;a href="https://en.wikipedia.org/wiki/Resonance"&gt;resonance&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How is this useful? Well, &lt;a href="https://factualaudio.com/post/anatomy/#the-frequency-domain"&gt;remember&lt;/a&gt; that the frequency domain is often more useful than the time domain when it comes to understanding how audio signals are perceived. Frequency response tells us how a system alters the frequency components of the signal that flows through it. That makes it one the most powerful tools in the audio engineer’s toolbox.&lt;/p&gt;
&lt;p&gt;Studies show that frequency response is extremely important when it comes to assessing audio quality in real-world scenarios. For example it has been shown that the human auditory system is capable of detecting frequency response variations as tiny as 0.1&amp;#160;dB &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Toole, F. and Olive. S., “&lt;a href="http://www.aes.org/e-lib/browse.cfm?elib=5163"&gt;The Modification of Timbre by Resonances: Perception and Measurement&lt;/a&gt;”, J. Audio Eng. Soc., 36(3), 122–141, (1988). From Fig. 9: coherent sum of 0&amp;#160;dB and -40&amp;#160;dB sources is ~0.1&amp;#160;dB.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; , and that it is by far the most important criterion when it comes to assessing the quality of a loudspeaker &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Olive, Sean, “&lt;a href="http://www.aes.org/e-lib/browse.cfm?elib=12794"&gt;A Multiple Regression Model For Predicting Loudspeaker Preference Using Objective Measurements: Part 1 — Listening Test Results&lt;/a&gt;”, presented at the 116th AES Convention, Berlin, Germany, preprint 6113, (May 2004).&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; . Make no mistake: this metric is a &lt;em&gt;huge deal&lt;/em&gt;, and I expect most posts on this blog will make use of it in one way or another.&lt;/p&gt;
&lt;p&gt;The digital and analog realms typically contribute very little in the way of frequency response distortion. Virtually all of it is found in the acoustic realm, as even the best loudspeakers and headphones struggle to keep their frequency response within a few dB of the intended target &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Many examples can be found in the &lt;a href="http://www.soundstagenetwork.com/index.php?option=com_content&amp;amp;view=article&amp;amp;id=16&amp;amp;Itemid=18"&gt;SoundStage database&lt;/a&gt; (loudspeakers) and the &lt;a href="https://www.innerfidelity.com/headphone-measurements"&gt;InnerFidelity database&lt;/a&gt; (headphones).&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; . It gets worse: the acoustics of home listening rooms (&lt;a href="https://en.wikipedia.org/wiki/Room_modes"&gt;room modes&lt;/a&gt; especially) can result in low frequency alterations in the order of 10&amp;#160;dB or more! &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Literally any frequency response measurement made in a small room will show this phenomenon. One example can be found in Leduc Michel, 2009, “&lt;a href="http://www.audioholics.com/room-acoustics/listening-room-acoustics#ftn3"&gt;How Does Listening Room Acoustics Affect Sound Quality?&lt;/a&gt;“, Audioholics (graph under “Standing waves” section). A series of representative examples can be found in Toole, Floyd E., &lt;em&gt;&lt;a href="https://books.google.co.uk/books?id=sGmz0yONYFcC"&gt;Sound Reproduction: Loudspeakers and Rooms&lt;/a&gt;&lt;/em&gt;, figure 13.9.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; No wonder why these are often said to be the “weakest links” of the audio chain…&lt;/p&gt;
&lt;h1 id="non-linear-distortion"&gt;Non-linear distortion&lt;/h1&gt;
&lt;p&gt;Ideally, audio components should meet the definition of a linear system as described above; that is, they should be able to accurately track the input signal, such that the output is precisely proportional to the input. Of course that is not the case in practice. Besides noise (which we’ve already covered), consider, for example, that the driver inside a loudspeaker is made from imperfect materials that have imperfect physical properties, so its movement will not perfectly track the signal, instead giving rise to non-linear distortion. One example in the analog realm is &lt;a href="https://en.wikipedia.org/wiki/Crossover_distortion"&gt;crossover distortion&lt;/a&gt; that can occur in certain types of amplifiers.&lt;/p&gt;
&lt;p&gt;Fortunately, any reasonable audio system will be at least &lt;em&gt;approximately&lt;/em&gt; linear, so we can still use the frequency response to reason about the system. At the same time, we need to keep ourselves honest and account for any remaining non-linear behavior that might alter the signal in ways that frequency response and noise measurements will not predict. This is appropriately called &lt;em&gt;non-linear distortion&lt;/em&gt;, also known as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Amplitude_distortion"&gt;amplitude distortion&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This definition makes non-linear distortion a very open-ended category, as indeed a signal can be distorted in an infinite variety of ways. Since it is not possible to run an infinite number of tests, measuring and quantifying the non-linear behavior of a system can be quite challenging. That said, most non-linear distortion comes in well-known shapes and forms, so a few standard measurements are usually good enough to characterize the performance of an audio system.&lt;/p&gt;
&lt;p&gt;One important aspect of non-linear distortion is that, contrary to frequency response distortion, it can cause new frequencies to appear that weren’t present in the original signal. By far the most well-known type of non-linear distortion is &lt;em&gt;harmonic distortion&lt;/em&gt;, where new frequencies appear that are whole multiples (&lt;a href="https://en.wikipedia.org/wiki/Harmonic"&gt;harmonics&lt;/a&gt;) of the frequencies in the original signal. It is often summarized in a single number, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Total_harmonic_distortion"&gt;total harmonic distortion&lt;/a&gt;&lt;/em&gt; (THD), which indicates the total amplitude of the introduced harmonics relative to the original signal (more precisely, the &lt;a href="https://en.wikipedia.org/wiki/Fundamental_frequency"&gt;fundamental&lt;/a&gt;). A related phenomenon is &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Intermodulation"&gt;intermodulation distortion&lt;/a&gt;&lt;/em&gt;, where multiple frequency components in the signal interact with each other to create new frequencies in a specific pattern.&lt;/p&gt;
&lt;p&gt;At the beginning of this section I described examples of non-linear distortion that can occur during normal system operation. However, the most egregious, obvious, and audible non-linear distortion issues occur when the system is driven beyond its limits; that is, signal amplitude is pushed too far and the system is unable to keep up. When this happens the peaks of the waveform cannot be reproduced faithfully and the system “bottoms out”, a phenomenon known as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Clipping_(audio)"&gt;clipping&lt;/a&gt;&lt;/em&gt;. To mention a few examples, this can happen in the digital realm (which is otherwise immune from most other forms of non-linear distortion) due to overflowing the largest possible sample value; in the analog realm due to exceeding the power limits of an amplifier; or in the acoustic realm due to exceeding the movement limits of a driver (&lt;a href="https://en.wikipedia.org/wiki/Excursion_(audio)"&gt;excursion&lt;/a&gt;).&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/clipped-sine-wave.fingerprint-e1151d6.png" alt="Clipped sine waveform" data-svg-alternative="https://factualaudio.com/plots/clipped-sine-wave.fingerprint-ffb5f12.svg"&gt;&lt;/span&gt; &lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/clipped-sine-spectrum.fingerprint-0ed12a4.png" alt="Clipped sine spectrum" data-svg-alternative="https://factualaudio.com/plots/clipped-sine-spectrum.fingerprint-645748f.svg"&gt;&lt;/span&gt;
&lt;p class="legend"&gt;&lt;em&gt;Waveform and spectrum of a 1&amp;#160;kHz sine wave showing symptoms of “hard clipping”, which produces odd-order harmonics (circled in red). The THD in this example is about 7%. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Here’s how this number is calculated. First, take the &lt;a href="https://factualaudio.com/post/amplitude/"&gt;RMS amplitude&lt;/a&gt; of the harmonics: 3&amp;#160;kHz -26&amp;#160;dB (0.051) 5&amp;#160;kHz -32&amp;#160;dB (0.025) 7&amp;#160;kHz -46&amp;#160;dB (0.0047) 9kHz -47&amp;#160;dB (0.0043)… the &lt;a href="https://en.wikipedia.org/wiki/Root_mean_square#In_waveform_combinations"&gt;total RMS amplitude&lt;/a&gt; of the harmonics is 0.057. The RMS amplitude of the fundamental is 0.78. The THD is the ratio of these two numbers, which is 0.073, or 7.3%.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So how audible is non-linear distortion? Well, that depends. Because non-linear distortion can take many forms, there is no simple answer. For example, distortion in the lower frequencies is less audible than in higher frequencies, and it is less audible if the newly introduced frequencies are close to the fundamental (thanks, once again, to masking). &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Temme, Steve, “&lt;a href="https://www.bksv.com/media/doc/BO0385.pdf"&gt;Application Note: Audio Distortion Measurements&lt;/a&gt;“, Brüel &amp; Kjær, 1992.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; Condensing non-linear distortion measurements into a single, simplistic THD number, as is often done, certainly doesn’t help. With that in mind, real-world examples tend to suggest that a THD of 10% is likely to be audible, 1% is borderline, and 0.1% is unlikely to be audible. &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Audioholics, ”&lt;a href="http://www.audioholics.com/room-acoustics/human-hearing-distortion-audibility-part-3"&gt;Human Hearing - Distortion Audibility Part 3&lt;/a&gt;”, 2005.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt;&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Amplitude, quantified</title>
      <link>https://factualaudio.com/post/amplitude/</link>
      <pubDate>Tue, 21 Nov 2017 21:04:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/amplitude/</guid>
      <description>
&lt;p&gt;In the last series of posts I’ve been focusing on the concept of amplitude, first &lt;a href="https://factualaudio.com/post/anatomy/"&gt;defining it&lt;/a&gt; as the strength of an audio signal, then &lt;a href="https://factualaudio.com/post/life/"&gt;describing its physical manifestations&lt;/a&gt; as the signal makes its way through the audio playback chain, and lastly explaining how &lt;a href="https://factualaudio.com/post/decibel/"&gt;decibels&lt;/a&gt; are used as a way to manipulate these numbers.&lt;/p&gt;
&lt;p&gt;The attentive reader might have observed that, back in that very first post, I deliberately left out an important part for later. The issue that still needs to be addressed is exactly how to &lt;em&gt;quantify&lt;/em&gt; amplitude, i.e. how do we calculate its value for a given audio signal. Let’s go back to our original example of a pure tone (sine) waveform, but with a vertical scale this time:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-amplitude.fingerprint-bf62927.png" alt="Sine wave with amplitude scale" data-svg-alternative="https://factualaudio.com/plots/sine-wave-amplitude.fingerprint-456cf34.svg"&gt;&lt;/span&gt;
&lt;p&gt;Now let me ask you a question: what is the amplitude of that signal?&lt;/p&gt;
&lt;h1 id="peak-amplitude"&gt;Peak amplitude&lt;/h1&gt;
&lt;p&gt;You might be tempted to answer that question with “1.0” &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Well, unless you are an audio engineer and you know what’s up. But in that case, what are you doing here?&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; , because that’s the as far as the curve travels from its middle point. Or perhaps you might answer “2.0”, because that’s the total height of the waveform (-1.0&amp;#160;to 1.0).&lt;/p&gt;
&lt;p&gt;These are not the only possible answers (as we’ll see below), but they are valid answers nonetheless. The former answer (“1.0”) is called the &lt;em&gt;peak amplitude&lt;/em&gt; of the signal. The latter answer (“2.0”) is called the &lt;em&gt;peak-to-peak amplitude&lt;/em&gt; of the signal.&lt;/p&gt;
&lt;p&gt;Peak-to-peak amplitude is twice the peak amplitude. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Strictly speaking that’s a bit of an oversimplification, because it assumes that audio signals are symmetrical, but they often aren’t — see Hetrich, Wayne L., “&lt;a href="http://www.aes.org/e-lib/browse.cfm?elib=2221"&gt;Real-World Audio Wave Form Asymmetries and the Effect on the Audio Chain&lt;/a&gt;”, presented at the 55th AES Convention, New York, NY, USA, preprint 1193, (October 1976). However this rarely matters in practice.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; In practice peak amplitude is more widely used than peak-to-peak amplitude. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;One notable exception is marketing material (including manufacturer-provided specifications), where peak-to-peak amplitude is often used because the number looks bigger — don’t be fooled!&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="the-problem-with-peak-amplitude"&gt;The problem with peak amplitude&lt;/h1&gt;
&lt;p&gt;You might be wondering why we can’t simply stop there. After all, we just came up with a quantitative definition of amplitude, so our job here is done, right?&lt;/p&gt;
&lt;p&gt;Not so fast. Let’s not forget that amplitude is used in a variety of contexts for a variety of calculations and comparisons. We need to make sure that the definition we came up with (peak amplitude) is appropriate for what we’re going to use it for.&lt;/p&gt;
&lt;p&gt;Peak amplitude is appropriate in &lt;em&gt;some&lt;/em&gt; contexts. For example, if you’re trying to determine whether a digital signal is going to clip, peak amplitude is definitely the metric you should use to make that determination. But in most cases, what we’re most interested in is the amount of &lt;em&gt;energy&lt;/em&gt; that is being conveyed in that audio signal &lt;em&gt;on average&lt;/em&gt;. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;“Energy” is used here in a generic sense, as physically speaking there is no “energy” in a digital signal for example. However it does map directly to the physical definition of energy when the signal enters the analog or acoustic realms, and since the acoustic realm is really all that matters in the end, it makes sense to use that term to describe audio signals in general.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Peak amplitude fares poorly in that scenario. To understand why, let’s look at an extreme example of a signal that is very different from a sine wave:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sinh-wave-amplitude.fingerprint-24ec7ae.png" alt="Peaky wave with amplitude scale" data-svg-alternative="https://factualaudio.com/plots/sinh-wave-amplitude.fingerprint-c194792.svg"&gt;&lt;/span&gt;
&lt;p&gt;That signal has the same peak amplitude as the previous example. Yet, it’s easy to see that it conveys less energy: it’s mostly silence only interrupted by a train of narrow peaks. That makes peak amplitude ill-suited for estimating the overall strength of the signal.&lt;/p&gt;
&lt;h1 id="rms-to-the-rescue"&gt;RMS to the rescue&lt;/h1&gt;
&lt;p&gt;To solve this problem, we need a different metric. Ideally, we want to compute some kind of &lt;em&gt;average&lt;/em&gt; of the signal. We can’t use the &lt;a href="https://en.wikipedia.org/wiki/Arithmetic_mean"&gt;mean&lt;/a&gt; — that would just amount to zero, since the positive and negative parts of the signal would cancel each other out.&lt;/p&gt;
&lt;p&gt;As it turns out, there is a standard way to compute the average value of an audio signal (or any alternative signal for that matter): the &lt;a href="https://en.wikipedia.org/wiki/Root_mean_square"&gt;root mean square&lt;/a&gt; (RMS). It’s a simple formula: we square the signal values, sum the squares, divide the result by the number of values, and then finally we take the square root of that number. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;For the sake of example, I’m assuming a discrete-time signal here.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Because the values are squared, the positive and negative parts of the signal add up instead of canceling each other. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This discussion assumes that the signal is centered on zero — i.e. that there is no &lt;a href="https://en.wikipedia.org/wiki/DC_bias"&gt;DC offset&lt;/a&gt;. In audio this is almost always a safe assumption to make. Incidentally, RMS when used in this context is the same thing as &lt;a href="https://en.wikipedia.org/wiki/Standard_deviation"&gt;standard deviation&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we apply that formula to the first example, we end up with ~0.707. More generally, for a pure sine wave (and &lt;em&gt;only&lt;/em&gt; for a pure sine wave!), the math tells us RMS amplitude is equal to peak amplitude divided by the square root of two (√2). Or, when working in decibels, that’s peak amplitude minus ~3&amp;#160;dB.&lt;/p&gt;
&lt;p&gt;When applied to the second example, we end up with ~0.424. As expected, we get a lower value as the signal conveys less energy. In other words, the ratio of peak amplitude to RMS amplitude — known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Crest_factor"&gt;crest factor&lt;/a&gt;&lt;/em&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Sometimes informally — and somewhat incorrectly — referred to as &lt;a href="https://en.wikipedia.org/wiki/Dynamic_range#Music"&gt;dynamic range&lt;/a&gt; in some contexts.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;  — is different because the shape of the waveform is different.&lt;/p&gt;
&lt;h1 id="closing-thoughts"&gt;Closing thoughts&lt;/h1&gt;
&lt;p&gt;&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-amplitude-detail.fingerprint-8f3165b.png" alt="Sinewave with detailed amplitude annotations" data-svg-alternative="https://factualaudio.com/plots/sine-wave-amplitude-detail.fingerprint-f5a54c2.svg"&gt;&lt;/span&gt; &lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sinh-wave-amplitude-detail.fingerprint-fd0b016.png" alt="Peaky wave with detailed amplitude annotations" data-svg-alternative="https://factualaudio.com/plots/sinh-wave-amplitude-detail.fingerprint-d5a66b9.svg"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ve seen that there is more than one way to quantify the amplitude of a signal, and that different approaches will produce different results depending on the shape of the waveform. Depending on the context in which the numbers are used, some approaches might be more appropriate than others.&lt;/p&gt;
&lt;p&gt;In practice, the method used to calculate the amplitude is often stated near the unit. For example, “Vrms”, “Vp”, “Vpp”. Otherwise, it is often assumed that RMS was used. In particular, levels expressed in decibels (e.g. “dBV”), are virtually always RMS &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;a href="https://webstore.iec.ch/publication/94"&gt;IEC 60027–3:2002&lt;/a&gt;, &lt;em&gt;Letter symbols to be used in electrical technology — Part 3: Logarithmic and related quantities, and their units&lt;/em&gt;, §4.1&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; (with the possible exception of dBFS, which sadly is &lt;a href="https://en.wikipedia.org/wiki/DBFS"&gt;ambiguous&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;What about &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Loudness"&gt;loudness&lt;/a&gt;&lt;/em&gt;? One could see loudness as the way us humans measure the amplitude of the sounds that reach our ears. Because the human auditory system is extremely complex, it is not easy to estimate how loud a given signal will be perceived in general. Of the two approaches that I’ve described, RMS is the one that approximates loudness best, but it is still a very crude estimation. Nonetheless, in practice, RMS amplitude is often used as a poor man’s proxy for loudness due to its simplicity. More perceptually accurate ways to estimate loudness would typically involve &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Weighting"&gt;weighting&lt;/a&gt;&lt;/em&gt; or even more advanced processes such as those described in &lt;a href="https://www.itu.int/rec/R-REC-BS.1770/en"&gt;ITU-R BS.1770&lt;/a&gt;. But that’s a story for another post.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Decibels, explained</title>
      <link>https://factualaudio.com/post/decibel/</link>
      <pubDate>Tue, 21 Nov 2017 21:03:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/decibel/</guid>
      <description>
&lt;p&gt;In the &lt;a href="https://factualaudio.com/post/life/"&gt;previous post&lt;/a&gt;, I introduced a number of physical quantities that are used to describe the &lt;em&gt;&lt;a href="https://factualaudio.com/post/anatomy/"&gt;amplitude&lt;/a&gt;&lt;/em&gt; of an audio signal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the digital domain, it is a &lt;em&gt;sample value&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;In the analog domain, it is voltage in &lt;em&gt;volts&lt;/em&gt; (V);&lt;/li&gt;
&lt;li&gt;In the acoustic domain, it is a pressure difference in &lt;em&gt;pascals&lt;/em&gt; (Pa).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, in the audio literature, marketing materials and equipment specifications, these are not the units that are typically used. Instead, one often finds such quantities expressed in &lt;em&gt;decibels&lt;/em&gt; (dB) or a related unit. There are good reasons for this, and they have to do with how we humans perceive loudness.&lt;/p&gt;
&lt;h1 id="on-the-usefulness-of-ratios"&gt;On the usefulness of ratios&lt;/h1&gt;
&lt;p&gt;In audio, we care more about &lt;em&gt;ratios&lt;/em&gt; of quantities (e.g. “2×”) than absolute values (e.g. “3&amp;#160;V”). To understand why, here’s an example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The difference in loudness between 0.1&amp;#160;Pa and 0.2&amp;#160;Pa is very noticeable.&lt;/li&gt;
&lt;li&gt;The difference in loudness between 1.0&amp;#160;Pa and 1.1&amp;#160;Pa is barely noticeable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases the absolute difference is the same: 0.1&amp;#160;Pa. However, the ratio is very different; in the former case it’s 2×, in the latter case it’s 1.1×. This makes a compelling case for using ratios, not differences, when comparing amplitudes.&lt;/p&gt;
&lt;p&gt;There is another advantage to using ratios. Remember that all three quantities (sample value, voltage, and sound pressure) are &lt;em&gt;proportional&lt;/em&gt; to each other when the audio signal moves from one realm to the next. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Assuming ideal conditions, i.e. no &lt;a href="https://factualaudio.com/post/distortion/"&gt;noise or distortion&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; This means that a given ratio applies in all three domains: twice the sample value is also twice the voltage and twice the sound pressure. This is very convenient because it means that a given ratio (often called &lt;a href="https://en.wikipedia.org/wiki/Gain_%28electronics%29"&gt;gain&lt;/a&gt;) has the same meaning regardless of context.&lt;/p&gt;
&lt;p&gt;Now, if only there was a unit that made working with ratios easier…&lt;/p&gt;
&lt;h1 id="enter-the-decibel"&gt;Enter the decibel&lt;/h1&gt;
&lt;p&gt;The &lt;em&gt;decibel&lt;/em&gt; (dB) is, in its purest form, a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Dimensionless_quantity"&gt;dimensionless unit&lt;/a&gt;&lt;/em&gt; that represents a ratio between two quantities, just like “×” or “%”. What sets the decibel apart is that it is also a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Logarithmic_scale"&gt;logarithmic unit&lt;/a&gt;&lt;/em&gt;, meaning that its value is proportional to the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Logarithm"&gt;logarithm&lt;/a&gt;&lt;/em&gt; of the ratio, not the ratio itself. This will be clearer with examples:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Amplitude ratio&lt;/th&gt;
&lt;th&gt;Decibel equivalent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.01×&lt;/td&gt;
&lt;td&gt;-40&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.1×&lt;/td&gt;
&lt;td&gt;-20&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.5×&lt;/td&gt;
&lt;td&gt;-6&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1×&lt;/td&gt;
&lt;td&gt;0&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2×&lt;/td&gt;
&lt;td&gt;+6&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4×&lt;/td&gt;
&lt;td&gt;+12&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8×&lt;/td&gt;
&lt;td&gt;+18&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10×&lt;/td&gt;
&lt;td&gt;+20&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100×&lt;/td&gt;
&lt;td&gt;+40&amp;#160;dB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Because the decibel is a logarithmic unit, it behaves differently from more conventional linear units. The trick is to not fight their logarithmic nature, but embrace it. Here’s how:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The most useful ratios to keep in mind are +6&amp;#160;dB (2×) and +20&amp;#160;dB (10×).&lt;/li&gt;
&lt;li&gt;To invert the ratio, just change the sign: for example, -6&amp;#160;dB is the same as dividing by 2.&lt;/li&gt;
&lt;li&gt;When combining ratios, decibels don’t multiply; they add up. For example, 4× (2×2) is 12&amp;#160;dB (6+6), not 36. This might seem strange, but this property makes decibels very convenient to use in practice — it’s easier to add than to multiply.&lt;/li&gt;
&lt;li&gt;0&amp;#160;dB means 1×, i.e. no change in amplitude.&lt;/li&gt;
&lt;li&gt;0× is -∞ dB (negative infinity). You might have seen this as the “mute” position on some volume knobs.&lt;/li&gt;
&lt;li&gt;For less trivial cases, &lt;a href="http://www.sengpielaudio.com/calculator-db.htm"&gt;online calculators&lt;/a&gt; are available.&lt;/li&gt;
&lt;/ul&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; All of the above assumes decibels are used to express ratios of &lt;a href="https://en.wikipedia.org/wiki/Field,_power,_and_root-power_quantities"&gt;field quantities&lt;/a&gt;. Digital sample value, voltage, and sound pressure are examples of field quantities. When dealing with &lt;em&gt;power&lt;/em&gt; quantities (i.e. watts), however, there is a catch: in that case, +6&amp;#160;dB is 4×, not 2×, which is +3&amp;#160;dB. This might seem strange and confusing, but once again there is an explanation: in practice power is proportional to the &lt;em&gt;square&lt;/em&gt; of the field quantity. So, for example, when voltage is doubled, power quadruples. Or, said differently, if voltage is increased by 6&amp;#160;dB, then power increases by… 6&amp;#160;dB — which is why the rules makes sense.&lt;/p&gt;
&lt;h1 id="using-decibels-for-absolute-values"&gt;Using decibels for absolute values&lt;/h1&gt;
&lt;p&gt;In theory, one could stick with linear units when dealing with absolute values (e.g. 2&amp;#160;V), and use decibels when dealing with ratios (e.g. 2×). However, when a calculation involves both, the mental gymnastics can be challenging. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Not convinced? Try calculating 2&amp;#160;V × -11&amp;#160;dB by hand.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It would make more sense to use decibels for everything, including absolute quantities. Fortunately, that’s easy: we just need to agree on a reference, and then express in decibels the ratio between that reference and the quantity we wish to convey. The resulting decibel value is called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Level_%28logarithmic_quantity%29"&gt;level&lt;/a&gt;&lt;/em&gt;. For example, if the reference is 1&amp;#160;V, then the level of 2&amp;#160;V is +6&amp;#160;dB. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;And the calculation from the previous note becomes 6&amp;#160;dB - 11&amp;#160;dB = -5&amp;#160;dB. Much easier!&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In practice, the reference value is indicated by a suffix affixed to the unit. Here are some references commonly used in the three audio realms:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Quantity&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Equivalent level&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sound pressure&lt;/td&gt;
&lt;td&gt;20&amp;#160;µPa&lt;/td&gt;
&lt;td&gt;0 &lt;a href="https://en.wikipedia.org/wiki/Sound_pressure#Sound_pressure_level"&gt;dB SPL&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Voltage&lt;/td&gt;
&lt;td&gt;1&amp;#160;V&lt;/td&gt;
&lt;td&gt;0 &lt;a href="https://en.wikipedia.org/wiki/Decibel#Voltage"&gt;dBV&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Voltage&lt;/td&gt;
&lt;td&gt;~0.77&amp;#160;V&lt;/td&gt;
&lt;td&gt;0 &lt;a href="https://en.wikipedia.org/wiki/Decibel#Voltage"&gt;dBu&lt;/a&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Yes, there are two different references in common use for voltages. That’s sad, but that’s the way it is. To convert from one to the other, add or subtract ~2.2&amp;#160;dB.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample value&lt;/td&gt;
&lt;td&gt;Full scale&lt;/td&gt;
&lt;td&gt;0 &lt;a href="https://en.wikipedia.org/wiki/DBFS"&gt;dBFS&lt;/a&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;In other words, 0&amp;#160;dBFS is the maximum level before digital clipping (truncation) occurs. Thus valid amplitudes have negative dBFS values. Or at least that’s how it’s supposed to work. The definition of dBFS can be quite fuzzy and ambiguous, as &lt;a href="https://en.wikipedia.org/wiki/DBFS#RMS_levels"&gt;explained&lt;/a&gt; in that Wikipedia page. Caveat emptor.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Sometimes the suffix is omitted and has to be inferred from the context. For example, it is fairly common to find “dB SPL” written as simply “dB”, especially in mainstream publications. This is best avoided as it can lead to confusion between ratios and levels. Conversely, “dBr” (“relative”) is sometimes used to make it explicit that decibels are used to express a ratio, not a level.&lt;/p&gt;
&lt;p&gt;Additional suffixes and variants are often used to convey additional information. The most common examples include “peak” or “RMS”, which denote different ways of looking at the amplitude of the signal, and “dBA”, which indicates the use of &lt;a href="https://en.wikipedia.org/wiki/A-weighting"&gt;A-weighting&lt;/a&gt;. More on these in later posts.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>From bytes to your eardrum</title>
      <link>https://factualaudio.com/post/life/</link>
      <pubDate>Tue, 21 Nov 2017 21:02:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/life/</guid>
      <description>
&lt;p&gt;In the &lt;a href="https://factualaudio.com/post/anatomy/"&gt;previous post&lt;/a&gt;, I described how an audio signal is represented. Now, let’s discuss the various physical forms that audio signals take as they travel through each stage of an audio playback system.&lt;/p&gt;
&lt;p&gt;For the sake of this discussion, I am going to assume the most common and straightforward use case: playing a &lt;em&gt;digital stream&lt;/em&gt; over loudspeakers (or headphones). By “digital stream” I mean any audio signal that is processed by a computer or computer-like system; that can be anything including a MP3&amp;#160;file, online video, online music streaming, the soundtrack of a Blu-ray disc, etc. This does not include analogue media such as vinyl discs or cassette tapes.&lt;/p&gt;
&lt;p&gt;Before this digital content can reach your eardrums, it has to go through a series of steps. Between these steps, the audio signal is materialized in different ways depending on which part of the audio “pipeline” we are looking at. In this post I refer to these concrete representations as &lt;em&gt;realms&lt;/em&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;“realm” is not a widely used term — the term “domain” is normally used. However, I felt that this could create confusion with &lt;em&gt;time domain&lt;/em&gt; and &lt;em&gt;frequency domain&lt;/em&gt;, which are completely unrelated concepts.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . I am going to start at the source and then make my way through to the listener.&lt;/p&gt;
&lt;p&gt;To keep things clear and simple, the example signal I’ll use throughout this post is a monophonic (one channel) 1&amp;#160;kHz sine wave. For all intents of purposes, each additional channel can be assumed to act like a completely separate audio signal that takes a similar path through the system.&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/diagrams/realms.fingerprint-0ee5447.png" alt="Audio pipeline diagram" data-svg-alternative="https://factualaudio.com/diagrams/realms.fingerprint-346c4fa.svg"&gt;&lt;/span&gt;
&lt;h1 id="the-digital-realm"&gt;The digital realm&lt;/h1&gt;
&lt;p&gt;It all starts within the digital device, which can be any computer or computer-like gadget (PC, smartphone, Bluetooth receiver, etc.). Most devices read or receive audio data in &lt;em&gt;digitally compressed&lt;/em&gt; form. Popular digital compression algorithms include &lt;a href="https://en.wikipedia.org/wiki/MP3"&gt;MP3&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Advanced_Audio_Coding"&gt;AAC&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/FLAC"&gt;FLAC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Data_compression"&gt;Digital compression&lt;/a&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Not to be confused with &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Dynamic_range_compression"&gt;dynamic range compression&lt;/a&gt;&lt;/em&gt;, which is completely unrelated.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; is a complicated subject, which I won’t dig into further in this post. In any case, the data first goes through a &lt;em&gt;decoder&lt;/em&gt; which converts the compressed signal into uncompressed form, which looks like this:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz-digital.fingerprint-7b40d5a.png" alt="Digitally sampled 1kHz sine waveform" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz-digital.fingerprint-a06ebe2.svg"&gt;&lt;/span&gt;
&lt;p&gt;This plot shows that, in the digital realm, audio is not represented by a continuous, smoothly changing signal — instead, all we have are regularly-spaced “snapshots” that indicate what the signal amplitude is at some point in time. This is called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Discrete-time_signal"&gt;discrete signal&lt;/a&gt;&lt;/em&gt;, and the “snapshots” are called &lt;em&gt;samples&lt;/em&gt;. In this example, we have 44100&amp;#160;samples every second, or more formally, the &lt;em&gt;sample rate&lt;/em&gt; is 44.1&amp;#160;kHz. Such a sample rate is standard for music — other types of content, such as movies or games, use a slightly higher rate, 48&amp;#160;kHz, for mostly historical reasons.&lt;/p&gt;
&lt;p&gt;Because memory is not infinite, each sample value has a finite precision. In practice, each sample is typically converted to a signed integer with a precision, or &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Audio_bit_depth"&gt;bit depth&lt;/a&gt;&lt;/em&gt;, of 16&amp;#160;bits. This process is called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Quantization_%28signal_processing%29"&gt;quantization&lt;/a&gt;&lt;/em&gt;. A 16-bit signed integer can take a value from -32768&amp;#160;to +32767. Samples outside of this range cannot be represented, and will be clamped to the nearest possible value; this is called &lt;em&gt;digital &lt;a href="https://en.wikipedia.org/wiki/Clipping_%28signal_processing%29"&gt;clipping&lt;/a&gt;&lt;/em&gt;, and is best avoided as it sounds quite bad. A signal that peaks at the highest possible amplitude without clipping is called a &lt;em&gt;full-range&lt;/em&gt; or &lt;em&gt;full-scale&lt;/em&gt; signal &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;a href="https://webstore.iec.ch/publication/5664"&gt;IEC 61606–1:2009&lt;/a&gt;, &lt;em&gt;Digital audio parts — Basic measurement methods of audio characteristics — General&lt;/em&gt;, §3.1.10&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; .&lt;/p&gt;
&lt;p&gt;Finally, the signal is physically represented simply by transmitting the value of each point, or &lt;em&gt;sample&lt;/em&gt;, one after the other. For example, the above signal is transmitted as 4653, 9211, 13583, etc. in the form of binary numbers. This way of transmitting the signal is called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Pulse-code_modulation"&gt;pulse-code modulation&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This section just skirted the surface of how digital audio works. The details of how sampled signals behave in practice are often counter-intuitive; as a result, misrepresentation of digital audio phenomena is quite commonplace in the audiophile community, leading to confusion and misguided advice. Digital audio expert &lt;a href="https://en.wikipedia.org/wiki/Chris_Montgomery"&gt;Chris Montgomery&lt;/a&gt; produced a &lt;a href="https://xiph.org/video/"&gt;series of videos&lt;/a&gt; that explains these complex phenomena with very clear and straightforward examples — it is a highly recommended resource if you wish to learn more about the digital realm.&lt;/p&gt;
&lt;h1 id="the-analog-realm"&gt;The analog realm&lt;/h1&gt;
&lt;p&gt;Loudspeakers and headphones cannot receive a digital signal; it has to be converted to an &lt;em&gt;analog&lt;/em&gt; signal first. This conversion is done in an electronic circuit appropriately named the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Digital-to-analog_converter"&gt;digital-to-analog converter&lt;/a&gt;&lt;/em&gt;, or DAC. This is where computer engineering ends and electrical engineering begins. The main task of the DAC is to take each sample value and convert it to some electrical &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Voltage"&gt;voltage&lt;/a&gt;&lt;/em&gt; on its output pins. The resulting signal looks like the following:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz-analog.fingerprint-263e374.png" alt="1kHz sine wave with voltage scale" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz-analog.fingerprint-ebd32f7.svg"&gt;&lt;/span&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; In the plot above, the unit used for the vertical scale is the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Volt"&gt;volt&lt;/a&gt;&lt;/em&gt;. In other words, the amplitude of the audio signal in the analog domain is defined by its &lt;em&gt;voltage&lt;/em&gt;. It is &lt;em&gt;not&lt;/em&gt; defined by &lt;a href="https://en.wikipedia.org/wiki/Electric_current"&gt;current&lt;/a&gt; nor &lt;a href="https://en.wikipedia.org/wiki/Electric_power"&gt;power&lt;/a&gt;. Even when the signal is used as the input of a loudspeaker, it is still voltage that determines the sound that comes out; power dissipation is a &lt;em&gt;consequence&lt;/em&gt;, not a &lt;em&gt;cause&lt;/em&gt;, of the audio signal flowing through the loudspeaker. As Pat Brown &lt;a href="http://www.prosoundtraining.com/site/author/pat-brown/meaningful-metrics-the-use-and-abuse-of-loudspeaker-power-ratings/"&gt;elegantly puts it&lt;/a&gt;: “power is &lt;em&gt;drawn&lt;/em&gt;, not applied”. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Another way to state this is to say that properly engineered analog audio devices act as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Voltage_source"&gt;voltage sources&lt;/a&gt;&lt;/em&gt; (or sinks), which are connected to each other by way of &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Impedance_bridging"&gt;impedance bridging&lt;/a&gt;&lt;/em&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The DAC took our discrete signal and converted it into a continuous electrical signal, whose voltage is (hopefully) &lt;em&gt;proportional&lt;/em&gt; to the digital sample value. The central (mean) value of the signal, called the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/DC_bias"&gt;DC offset&lt;/a&gt;&lt;/em&gt;, is zero volts; the signal swings around that central value, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Alternating_current"&gt;alternating&lt;/a&gt;&lt;/em&gt; between positive and negative voltage. In this example, our full-scale digital stream was converted to an analog signal that swings between -1.41&amp;#160;V and +1.41&amp;#160;V. Depending on the specific model of DAC used, its volume control setting (if any) and the signals involved, these numbers can vary — typical peak amplitude can go as low as 0.5&amp;#160;V &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;Wikipedia, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Line_level#Nominal_levels"&gt;Nominal levels&lt;/a&gt;&lt;/em&gt;, peak amplitude for consumer audio&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; or as high as 2.8&amp;#160;V &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;a href="https://webstore.iec.ch/publication/6142"&gt;IEC 61938:2013&lt;/a&gt;, &lt;em&gt;Guide to the recommended characteristics of analogue interfaces to achieve interoperability&lt;/em&gt;, §8.2.1&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; .&lt;/p&gt;
&lt;p&gt;The amount of &lt;em&gt;current&lt;/em&gt; or &lt;em&gt;power&lt;/em&gt; transferred from the source of an analog signal (e.g. a DAC) to the equipment plugged in at the other end of the cable (the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Electrical_load"&gt;load&lt;/a&gt;&lt;/em&gt;, e.g. a loudspeaker) is determined by the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Electrical_impedance"&gt;impedance&lt;/a&gt;&lt;/em&gt; of the load, also known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Input_impedance"&gt;input impedance&lt;/a&gt;&lt;/em&gt;. According to &lt;a href="https://en.wikipedia.org/wiki/Ohm's_law"&gt;Ohm’s law&lt;/a&gt;, the lower the impedance, the more current, and therefore power, will be required to sustain a given voltage.&lt;/p&gt;
&lt;p&gt;DACs, as well as most other types of analog audio equipment (such as filters or mixers), are not designed to provide significant amounts of power. Instead, they are meant to be connected to a high-impedance load, normally 20&amp;#160;kΩ or higher &lt;strong class="inlineFootnoteDecoration"&gt;[ref]&lt;/strong&gt; &lt;span class="inlineFootnote-ref inlineFootnoteUnprocessed"&gt;&lt;a href="https://webstore.iec.ch/publication/6142"&gt;IEC 61938:2013&lt;/a&gt;, &lt;em&gt;Guide to the recommended characteristics of analogue interfaces to achieve interoperability&lt;/em&gt;, §8.2.1&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end ref]&lt;/strong&gt; . This means that the load is acting much like a &lt;a href="https://en.wikipedia.org/wiki/Voltmeter"&gt;voltmeter&lt;/a&gt; or oscilloscope — it is “peeking” at the input voltage without drawing significant power from it. Such a signal that carries some voltage but very little power is called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Line_level"&gt;line-level&lt;/a&gt;&lt;/em&gt; signal.&lt;/p&gt;
&lt;p&gt;On the other hand, loudspeakers (and headphones to a lesser extent) are low-impedance devices — often between 4&amp;#160;Ω and 8&amp;#160;Ω in the case of speakers. This is because they operate under a relatively low voltage, but require a lot of power. For example, most speakers will happily produce comfortably loud sound with as little as 6&amp;#160;V, but might consume as much as 9 &lt;a href="https://en.wikipedia.org/wiki/Watt"&gt;watts&lt;/a&gt; while doing so &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;From the numbers given a keen eye will &lt;a href="http://www.sengpielaudio.com/calculator-ohm.htm"&gt;deduce&lt;/a&gt; that this example speaker has an impedance of 4&amp;#160;Ω. One thing to note is that loudspeaker impedance is highly dependent on the frequency of the signal, making the use of one number an oversimplification. The impedance that manufacturers advertise, called the &lt;em&gt;rated impedance&lt;/em&gt;, is 1.25&amp;#160;times the &lt;em&gt;minimum&lt;/em&gt; impedance of the speaker across its rated frequency range. (see &lt;a href="https://webstore.iec.ch/publication/1223"&gt;IEC 60268–5:2003&lt;/a&gt;, &lt;em&gt;Loudspeakers&lt;/em&gt;, §16.1)&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . Line-level equipment is not designed to provide such a large amount of power.&lt;/p&gt;
&lt;p&gt;This problem is solved by using a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Audio_power_amplifier"&gt;power amplifier&lt;/a&gt;&lt;/em&gt;. This is a component that conveniently provides a high-impedance input for connecting line-level equipment, while exposing an output that is capable of providing large amounts of power, such as 10W or more, to a low-impedance load. &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;In practice, most amplifiers are also capable of increasing the voltage (amplitude) of the signal; this is called the &lt;a href="https://en.wikipedia.org/wiki/Gain_%28electronics%29"&gt;gain&lt;/a&gt; of the amplifier. This is because most loudspeakers require voltages that are somewhat higher than line level in order to play loud enough. Still, the primary goal of a power amplifier is to provide power, not to increase voltage.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; Such outputs provide so-called &lt;em&gt;speaker-level signals&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In some home audio systems, the DAC and the amplifier are integrated into one single device, which is called an &lt;em&gt;integrated amplifier&lt;/em&gt; or more commonly an &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/AV_receiver"&gt;AV receiver&lt;/a&gt;&lt;/em&gt; (AVR).&lt;/p&gt;
&lt;h1 id="the-acoustic-realm"&gt;The acoustic realm&lt;/h1&gt;
&lt;p&gt;Finally, in order to reach your ears, the analog signal must be converted to an &lt;em&gt;acoustic&lt;/em&gt; signal, that is, actual &lt;a href="https://en.wikipedia.org/wiki/Sound"&gt;sound waves&lt;/a&gt;. This is accomplished using a device called an &lt;em&gt;electroacoustic &lt;a href="https://en.wikipedia.org/wiki/Transducer"&gt;transducer&lt;/a&gt;&lt;/em&gt;, or &lt;em&gt;driver&lt;/em&gt;. The output of a driver when excited with our example signal, as measured at some point in front of it, might look like the following:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz-acoustic.fingerprint-675cab2.png" alt="1kHz sine wave with pressure scale" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz-acoustic.fingerprint-0e739d8.svg"&gt;&lt;/span&gt;
&lt;p&gt;Note the change of vertical scale. We’re not dealing with voltage anymore — amplitude takes the form of &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Sound_pressure"&gt;sound pressure&lt;/a&gt;&lt;/em&gt; instead. Indeed, sound is a physical phenomenon in which transient changes in pressure (&lt;em&gt;compression&lt;/em&gt;, &lt;em&gt;rarefaction&lt;/em&gt;) produced by the vibration of a &lt;em&gt;sound source&lt;/em&gt; propagate through the space around it. In other words, it is a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Longitudinal_wave"&gt;longitudinal wave&lt;/a&gt;&lt;/em&gt;. Sound pressure, expressed in &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Pascal_%28unit%29"&gt;Pascals&lt;/a&gt;&lt;/em&gt; (Pa), quantifies the difference between normal atmospheric pressure and some local, dynamic change in pressure, at a given point in time and space. The human ear is equipped to detect these changes, which are then — finally! — perceived as sound by the human brain.&lt;/p&gt;
&lt;p&gt;An ideal transducer will produce sound pressure &lt;em&gt;proportional&lt;/em&gt; to the voltage applied to it, like in the above waveform. However, it is difficult to design a driver that is capable of doing that across the entire range of audible frequencies. Consequently, a number of transducer types are available, which are commonly referred to as &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Subwoofer"&gt;subwoofers&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Woofer"&gt;woofers&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Mid-range_speaker"&gt;midranges&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Tweeter"&gt;tweeters&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In order to reproduce the entire range of human hearing, several of these drivers — often two or three — are assembled inside a single “box”, called the &lt;em&gt;enclosure&lt;/em&gt;. In most designs the drivers are mounted flush with one side of the box, which is called the &lt;em&gt;front baffle&lt;/em&gt;. An electrical circuit called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Audio_crossover"&gt;crossover&lt;/a&gt;&lt;/em&gt; splits the input signal into the frequency ranges appropriate for each driver. The resulting device is called a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Loudspeaker"&gt;loudspeaker&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;What I’ve described here is called a &lt;em&gt;passive&lt;/em&gt; loudspeaker, which is the most common type in consumer “Hi-Fi” systems. Sometimes the amplifier and speaker are integrated into the same device; this is called an &lt;em&gt;active&lt;/em&gt; or &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Powered_speakers"&gt;powered&lt;/a&gt;&lt;/em&gt; speaker. Examples include professional “studio monitor” speakers, which have line-level inputs. Other products, such as “Bluetooth speakers”, go one step further and throw in a DAC as well for a completely integrated solution.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Headphones"&gt;Headphones&lt;/a&gt;&lt;/em&gt; are a special case and typically only have one driver per channel, which makes them simpler. Conceptually, a headphone is akin to a miniature loudspeaker. Because of their proximity to the ear, they don’t have to produce as much sound pressure; therefore they require much less power to operate (often less than 1&amp;#160;mW).&lt;/p&gt;
&lt;p&gt;One notable aspect of the acoustic realm is that sound propagates in all three dimensions — the audio signal (sound pressure) is not the same at every point in space. In particular, speakers exhibit &lt;em&gt;radiation patterns&lt;/em&gt; that vary with angle and frequency, and the sound they emit can bounce off surfaces (&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Reflection_%28physics%29#Sound_reflection"&gt;reflection&lt;/a&gt;&lt;/em&gt;). This in turn means that they interact with their environment (the listening room, or, in the case of headphones, the listener’s head) in ways that are complex and difficult to predict but nonetheless have an enormous impact on how the radiated sound will be perceived by a human listener. This makes choosing and configuring a speaker system quite the challenge. Hopefully, future posts on this blog will provide some pointers.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Anatomy of an audio signal</title>
      <link>https://factualaudio.com/post/anatomy/</link>
      <pubDate>Tue, 21 Nov 2017 21:01:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/anatomy/</guid>
      <description>
&lt;p&gt;This inaugural Factual post starts from first principles, by laying down some of the fundamental foundations necessary to start reasoning about audio signals. I will then build on these principles in the posts to follow.&lt;/p&gt;
&lt;h1 id="the-time-domain"&gt;The time domain&lt;/h1&gt;
&lt;p&gt;An audio signal is an oscillating phenomenon: it is defined by a quantity that alternatively increases and decreases over time while keeping in the vicinity of some central value.&lt;/p&gt;
&lt;p&gt;Out of the infinity of shapes that an audio signal can take, probably the simplest is a &lt;em&gt;pure tone&lt;/em&gt;, also called a &lt;em&gt;sine wave&lt;/em&gt; from the name of the &lt;a href="https://en.wikipedia.org/wiki/Sine"&gt;mathematical function&lt;/a&gt; that it describes. Here is an example of a sine wave:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-wave-1khz.fingerprint-5b89774.png" alt="1&amp;#160;kHz pure tone waveform" data-svg-alternative="https://factualaudio.com/plots/sine-wave-1khz.fingerprint-3686aa5.svg"&gt;&lt;/span&gt;
&lt;p&gt;The horizontal axis is time, which is why it is often said that this representation shows the signal in the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Time_domain"&gt;time domain&lt;/a&gt;&lt;/em&gt; (another term is &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Waveform"&gt;waveform&lt;/a&gt;&lt;/em&gt;). The above signal oscillates around the central value represented by the horizontal line. According to the horizontal scale, this particular signal repeats once every millisecond: this is its &lt;em&gt;period&lt;/em&gt;, also known as the &lt;em&gt;wavelength&lt;/em&gt; or &lt;em&gt;cycle&lt;/em&gt;. Said differently, the signal repeats one thousand times per second: it has a &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Frequency"&gt;frequency&lt;/a&gt;&lt;/em&gt; of 1000 &lt;a href="https://en.wikipedia.org/wiki/Hertz"&gt;hertz&lt;/a&gt;. In order to be audible, the frequency of the signal must sit between 20&amp;#160;Hz and 20&amp;#160;kHz: this interval is known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Hearing_range"&gt;audible range&lt;/a&gt;&lt;/em&gt; of the human hearing system.&lt;/p&gt;
&lt;p&gt;In the image above, the height of the signal is known as the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Amplitude"&gt;amplitude&lt;/a&gt;&lt;/em&gt; (the term &lt;em&gt;magnitude&lt;/em&gt; is also used). I deliberately left out the vertical scale and unit because they depend on the context — more on this to follow in the post about &lt;a href="https://factualaudio.com/post/life/"&gt;audio realms&lt;/a&gt;. Another complication is that the “height” of the curve can be defined in multiple different ways — which are explored in a &lt;a href="https://factualaudio.com/post/amplitude/"&gt;separate post&lt;/a&gt; as well.&lt;/p&gt;
&lt;p&gt;Amplitude is related to &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Loudness"&gt;loudness&lt;/a&gt;&lt;/em&gt;, in the sense that if we take a signal and increase its amplitude (by &lt;em&gt;amplifying&lt;/em&gt; its oscillations), the human hearing system will perceive the signal to be louder. Likewise, if we decrease its amplitude (by &lt;em&gt;attenuating&lt;/em&gt;), it will be perceived as being quieter.&lt;/p&gt;
&lt;p class="caution"&gt;&lt;strong class="caution-label"&gt;Caution:&lt;/strong&gt; This relationship between amplitude and loudness does not necessarily hold when comparing signals that have differing frequencies. This is due to the fact that the human hearing system does not perceive all frequencies as being equally loud &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;The effect can be quantified using &lt;a href="https://en.wikipedia.org/wiki/Equal-loudness_contour"&gt;equal-loudness contours&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . For example, if you listen to a 30&amp;#160;Hz tone and then to a 2&amp;#160;kHz tone of equal amplitude, the latter will sound much louder than the former.&lt;/p&gt;
&lt;p&gt;Of course, most audio content is not a pure tone. In practice, an audio signal for, say, music, might look like this:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/piano-c5-wave.fingerprint-7700c23.png" alt="Piano C5&amp;#160;waveform" data-svg-alternative="https://factualaudio.com/plots/piano-c5-wave.fingerprint-ab40946.svg"&gt;&lt;/span&gt;
&lt;p&gt;As the above image shows, a musical signal is way more complex than a pure tone. And that’s not even a complicated musical piece — this is pianist Joohyun Park, solo, playing a single note &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;Specifically, this is one of the first notes played at the beginning of the &lt;em&gt;Allegro&lt;/em&gt; track from &lt;em&gt;&lt;a href="http://www.bearmccreary.com/blog/blog/battlestar-galactica-3/battlestar-galactica-solo-piano-cd/"&gt;The Music of Battlestar Galactica for Solo Piano&lt;/a&gt;&lt;/em&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . What’s really problematic, however, is that this representation doesn’t seem to relate to our perception at all — to the naked eye, it doesn’t look like a musical note played on a piano.&lt;/p&gt;
&lt;h1 id="the-frequency-domain"&gt;The frequency domain&lt;/h1&gt;
&lt;p&gt;In order to make sense of such complex signals, we need a better way to look at the data. Fortunately, the above signal can be decomposed into a number of pure tones of various frequencies and amplitudes, thanks to the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Superposition_principle"&gt;superposition principle&lt;/a&gt;&lt;/em&gt;. The mathematical tool used to do the decomposition is called the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Fourier_transform"&gt;Fourier transform&lt;/a&gt;&lt;/em&gt;. For example, if we were to apply the Fourier transform to our first pure tone example, the result could be represented as follows:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/sine-spectrum-1khz.fingerprint-f6060ac.png" alt="1&amp;#160;kHz pure tone spectrum" data-svg-alternative="https://factualaudio.com/plots/sine-spectrum-1khz.fingerprint-814fb1f.svg"&gt;&lt;/span&gt;
&lt;p&gt;The vertical axis is still amplitude, but the horizontal axis has changed — it now represents frequency. This representation shows the signal in the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Frequency_domain"&gt;frequency domain&lt;/a&gt;&lt;/em&gt;, or, in other words, it shows the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Spectral_density"&gt;spectral density&lt;/a&gt;&lt;/em&gt; (often simply called &lt;em&gt;spectrum&lt;/em&gt;) of the signal.&lt;/p&gt;
&lt;p&gt;A keen eye might have noticed that the horizontal axis is using a &lt;a href="https://en.wikipedia.org/wiki/Logarithmic_scale#Graphic_representation"&gt;logarithmic scale&lt;/a&gt;, which is commonplace for this type of plot. This scale provides a better view of how we perceive sound: it is very easy to hear the difference between a 100&amp;#160;Hz tone and a 200&amp;#160;Hz tone, but the same cannot be said about 10000&amp;#160;Hz and 10100&amp;#160;Hz tones, even though the difference is still 100&amp;#160;Hz. This is because in the former case, there is a 100% increase, while in the latter case, the increase is only 1%. In other words, the human auditory system perceives &lt;em&gt;relative change&lt;/em&gt;, as opposed to absolute change &lt;strong class="inlineFootnoteDecoration"&gt;[note]&lt;/strong&gt; &lt;span class="inlineFootnote-note inlineFootnoteUnprocessed"&gt;This is consistent with other human senses, as predicted by the &lt;a href="https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law"&gt;Weber-Fechner law&lt;/a&gt;.&lt;/span&gt; &lt;strong class="inlineFootnoteDecoration"&gt;[end note]&lt;/strong&gt; . The term &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Octave"&gt;octave&lt;/a&gt;&lt;/em&gt; is used to describe a frequency factor of two; for example, the range 2&amp;#160;kHz to 8&amp;#160;kHz is two octaves wide. The term &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Decade_%28log_scale%29"&gt;decade&lt;/a&gt;&lt;/em&gt; is also sometimes used, and describes a tenfold increase in frequency.&lt;/p&gt;
&lt;p&gt;The above plot is showing us that the signal can be decomposed into a single 1&amp;#160;kHz tone, but we already knew that. What’s more interesting is what happens when we apply the Fourier transform to the musical signal:&lt;/p&gt;
&lt;span class="figure" data-intrinsic-placeholder="unprocessed"&gt;&lt;img src="https://factualaudio.com/plots/piano-c5-spectrum.fingerprint-32fe3c5.png" alt="Piano C5&amp;#160;spectrum" data-svg-alternative="https://factualaudio.com/plots/piano-c5-spectrum.fingerprint-da791dd.svg"&gt;&lt;/span&gt;
&lt;p&gt;Here things become interesting. This plot is telling us that our musical example can be decomposed into a 260&amp;#160;Hz tone with high amplitude, combined with 520&amp;#160;Hz and 780&amp;#160;Hz tones of lower amplitude.&lt;/p&gt;
&lt;p&gt;Such a result is typical of a recording of an instrument playing a single note. The first tone, at 260&amp;#160;Hz, is called the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Fundamental_frequency"&gt;fundamental&lt;/a&gt;&lt;/em&gt; and indicates the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Pitch_%28music%29"&gt;pitch&lt;/a&gt;&lt;/em&gt; of the sound, in other words the note being played, &lt;a href="https://en.wikipedia.org/wiki/C_%28musical_note%29#Designation_by_octave"&gt;C5&lt;/a&gt; in this example. The 520&amp;#160;Hz and 780&amp;#160;Hz tones, because they are &lt;em&gt;multiples&lt;/em&gt; of the fundamental, are called &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Harmonic"&gt;harmonics&lt;/a&gt;&lt;/em&gt;. They are interpreted by the human hearing system to help determine the &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Timbre"&gt;timbre&lt;/a&gt;&lt;/em&gt; of the instrument. If the same note was being played on say, a flute or a violin, the frequency of the fundamental would be the same but the relative amplitudes of the harmonics would be different.&lt;/p&gt;
&lt;p&gt;This is interesting because we can directly relate what we see on the plot to how the sound will be &lt;em&gt;perceived&lt;/em&gt;, i.e. what the signal sounds like. Of course interpreting this data still requires some effort — most people wouldn’t be able to tell “of course that’s a piano playing a C5” just by eyeballing the above image. Furthermore, if I had used a more complex example (such as a symphonic orchestra playing in unison), the spectrum would have been just as unreadable as the waveform. Nevertheless, in practice, the spectrum often provides a much more useful view from a &lt;em&gt;perceptual&lt;/em&gt; perspective. This is why audio engineers will often ignore the time domain, instead focusing their efforts on the frequency domain.&lt;/p&gt;
&lt;p&gt;Frequency-domain data can be converted back to the time domain using the appropriately-named &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Fourier_inversion_theorem"&gt;inverse Fourier transform&lt;/a&gt;&lt;/em&gt;. One might wonder if any information gets lost during these conversions. From a purely mathematical point of view, &lt;a href="https://en.wikipedia.org/wiki/Fourier_inversion_theorem"&gt;the answer is no&lt;/a&gt;, but there is a catch. The above plots do not show the full output of the Fourier transform. In reality, the result of the Fourier transform includes &lt;em&gt;amplitude&lt;/em&gt; information (which is shown above) and &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Phase_%28waves%29"&gt;phase&lt;/a&gt;&lt;/em&gt; information (which I omitted). Amplitude determines the strength of the constituent tones, while phase indicates which part of the waveform cycle occurs at a specific point in time. As long as phase information is not discarded, it is possible to recover the original waveform, intact, simply by applying the inverse transform. That said, aside from a few specific scenarios (such as signal summation), phase is not nearly as prominent as amplitude in practical audio discussions, which is why I won’t dig further into it in this introductory post. I will revisit this topic in a &lt;a href="https://factualaudio.com/post/phase/"&gt;later post&lt;/a&gt;.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>About Factual Audio</title>
      <link>https://factualaudio.com/page/about/</link>
      <pubDate>Tue, 21 Nov 2017 21:00:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/page/about/</guid>
      <description>
&lt;p&gt;Factual Audio is dedicated to delivering objective, trustworthy, reliable, evidence-backed information about audio, in particular audio playback at home and on the move. This includes for example “Hi-Fi” systems, “home cinema” systems, and headphones. The main goal of this website is to educate the general public about audio matters through critical thinking and a sound, rational, scientific approach to assessing audio claims and products.&lt;/p&gt;
&lt;p&gt;Factual Audio is a completely independent entity with a strong focus on integrity, transparency and full disclosure. It does not have a commercial or monetary purpose, is not affiliated with any company or organization, does not run ads, and does not accept paid promotion of any kind.&lt;/p&gt;
&lt;p&gt;While Factual Audio strives to maintain a high standard of scientific accuracy, no-one is immune from making mistakes, and new discoveries can sometimes compel us to revise our understanding. If you believe that Factual Audio has made factually incorrect claims, used outdated information, misrepresented evidence, ignored contradictory evidence, or has failed to provide appropriate citations for controversial statements, please don’t hesitate to provide feedback by posting a comment on the article or to get in touch using the contact details below.&lt;/p&gt;
&lt;h1 id="source-code-license-and-copyright"&gt;Source code, license and copyright&lt;/h1&gt;
&lt;p&gt;The source code that this website is built from is freely available in a &lt;a href="https://github.com/factualaudio/factualaudio"&gt;public repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All article text, figures and images on this website are licensed under the &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0&amp;#160;International License&lt;/a&gt;, unless explicitly mentioned otherwise. This also applies to source code.&lt;/p&gt;
&lt;p&gt;Note that most decorative or stylistic elements come from the &lt;a href="https://gohugo.io/"&gt;Hugo&lt;/a&gt;, &lt;a href="https://github.com/roryg/ghostwriter"&gt;Hugo Ghostwriter theme&lt;/a&gt; and &lt;a href="http://www.bigfootjs.com/"&gt;bigfoot.js&lt;/a&gt; projects, which come with their own licenses.&lt;/p&gt;
&lt;h1 id="contact"&gt;Contact&lt;/h1&gt;
&lt;p&gt;Owner and editor: &lt;a href="mailto:etienne@edechamps.fr"&gt;Etienne Dechamps&lt;/a&gt;&lt;/p&gt;

</description>
    </item>
    
  </channel>
</rss>

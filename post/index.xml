<?xml version="1.0" encoding="utf-8" standalone="yes" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Factual Audio</title>
    <link>https://factualaudio.com/post/</link>
    <description>Recent content in Posts on Factual Audio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <managingEditor>etienne@edechamps.fr (Etienne Dechamps)</managingEditor>
    <webMaster>etienne@edechamps.fr (Etienne Dechamps)</webMaster>
    <copyright>Etienne Dechamps</copyright>
    <lastBuildDate>Tue, 21 Nov 2017 21:05:00 +0100</lastBuildDate>
    
        <atom:link href="https://factualaudio.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Noise and distortion</title>
      <link>https://factualaudio.com/post/distortion/</link>
      <pubDate>Tue, 21 Nov 2017 21:05:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/distortion/</guid>
      <description>

&lt;p&gt;As the audio signal makes its way through the different &lt;a href=&#34;https://factualaudio.com/post/life/&#34;&gt;realms&lt;/a&gt; of the system, it travels through various digital, analog, and acoustic components that alter the signal in various ways. Some of these alterations might or might not be audible, or might only be audible under certain conditions. In most scenarios relevant to this blog they are undesirable side effects from limitations in the components that make up the system, though in some specific cases they can be deliberately introduced in pursuit of a specific goal (e.g. &lt;a href=&#34;https://en.wikipedia.org/wiki/Equalization_(audio)&#34;&gt;equalization&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In order to build a high quality audio system, it is necessary to keep signal degradation (i.e. unwanted alteration) under control, and this requires a good understanding of what these alterations might be, what causes them, and how to avoid or alleviate them. Ever since the invention of the audio signal more than a century ago this has always been a huge topic on which much ink has been spilled, some thoughtful and innovative, some misguided or downright counter-productive. Hopefully this blog will do more of the former and less of the latter — but for now, this post will serve as a brief introduction to the issues at hand.&lt;/p&gt;

&lt;p&gt;Signal alterations can be divided into three broad categories: &lt;em&gt;noise&lt;/em&gt;, &lt;em&gt;linear distortion&lt;/em&gt;, and &lt;em&gt;non-linear distortion&lt;/em&gt;. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;When used by itself without qualification, the term “distortion” can refer to some or all of these categories, depending on context.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; Real-world systems typically exhibit all three kinds in varying amounts. Future posts will dive into these issues in much more detail.&lt;/p&gt;

&lt;h1 id=&#34;noise&#34;&gt;Noise&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Noise&#34;&gt;Noise&lt;/a&gt;&lt;/em&gt; describes an alteration of the signal in which a separate, &lt;em&gt;unrelated&lt;/em&gt; signal is added (i.e. mixed in, superposed) to the original signal. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;This is the definition I’ll use throughout this blog, pursuant to &lt;a href=&#34;https://webstore.iec.ch/publication/1219&#34;&gt;IEC 60268–2&lt;/a&gt;. In other contexts noise might be used in a more specific way (e.g. broadband noise only), or in a more general way (e.g. signal differences introduced by non-linear distortion are considered to be part of the noise signal).&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; That additional signal has its own &lt;a href=&#34;https://factualaudio.com/post/anatomy/&#34;&gt;characteristics&lt;/a&gt; including amplitude and frequency content (spectrum), which are combined with the characteristics of the original signal. Noise is often quantified by comparing the amplitude of the noise to the amplitude of the signal, a metric known as the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Signal-to-noise_ratio&#34;&gt;signal-to-noise ratio&lt;/a&gt;&lt;/em&gt; (the higher the better).&lt;/p&gt;

&lt;p&gt;Noise can appear in all three of the audio realms:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the digital realm it can take the form of &lt;a href=&#34;https://en.wikipedia.org/wiki/Dither#Digital_audio&#34;&gt;dithering noise&lt;/a&gt; for example, though modern digital systems typically provide good enough performance that noise sits comfortably below the threshold of audibility.&lt;/li&gt;
&lt;li&gt;The analog realm is where noise problems are the most common, the most objectionable, and the most pernicious. They are often the result of complex &lt;a href=&#34;https://en.wikipedia.org/wiki/Electromagnetic_interference&#34;&gt;electromagnetic interference&lt;/a&gt; phenomena, subtle hardware defects, or compatibility issues.&lt;/li&gt;
&lt;li&gt;The acoustic realm is rife with often overlooked sources of noise from ordinary life, from the rumbling of an air conditioning unit to the occasional car driving down the street.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Depending on amplitude and frequency content, noise might or might not constitute a problem in practice. For example, low-level broadband &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Colors_of_noise&#34;&gt;colored noise&lt;/a&gt;&lt;/em&gt; (such as &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/White_noise&#34;&gt;white noise&lt;/a&gt;&lt;/em&gt;) will often go unnoticed because its spectrum is roughly similar to typical ambient noise that we are all continuously subjected to in our daily lives. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;See Albert Donald G., Decato Stephen N., &amp;ldquo;&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/S0003682X16306120&#34;&gt;Acoustic and seismic ambient noise measurements in urban and rural areas&lt;/a&gt;&amp;rdquo;, Applied Acoustics, 119, 135–143, (2017) for examples of ambient noise spectra.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt; The same cannot be said of norrowband noise concentrated in specific frequencies. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Unfortunately that distinction is lost when noise measurements are condensed into a single number (such as signal-to-noise ratio), discarding spectral distribution in the process.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; Furthermore, narrowband noise is more likely to affect the perception of minute detail in the original signal, due to an auditory phenomenon known as &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Auditory_masking&#34;&gt;masking&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://factualaudio.com/plots/noisy-sine-wave.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;

&lt;img src=&#34;https://factualaudio.com/plots/noisy-sine-spectrum.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;

&lt;p class=&#34;legend&#34;&gt;&lt;em&gt;Waveform and spectrum of a sine wave affected by white noise. The noise spectrum, circled in red, is often called the “noise floor”. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;From that spectrum plot you might be tempted to conclude that the signal-to-noise ratio is about 50 dB. That would be wrong — it’s actually much worse, around 17 dB. You can’t read it directly from the graph because the noise is spread across multiple frequencies. This is a very common mistake when reading spectrum plots, which I might describe in more detail in a future post.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/p&gt;

&lt;p&gt;In a typical real-world scenario, noise is only really noticeable when the original signal is relatively quiet, such as when there is a break in a piece of music, because all that remains is the noise itself. Conversely, noise is typically inaudible when a significantly loud signal is playing, again because of masking (but this time in reverse). In practice, I tend to abide by the following rule of thumb: if you can’t hear anything when playing a silent signal, then your system is probably fine as far as noise is concerned.&lt;/p&gt;

&lt;h1 id=&#34;linear-distortion&#34;&gt;Linear distortion&lt;/h1&gt;

&lt;p&gt;A system might modify the signal in ways that do not depend on the amplitude of the signal. That is, all else being equal, if the amplitude of the original signal as a whole is changed, the amplitude of the resulting signal as a whole will change by the same amount. This is referred to as a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_system&#34;&gt;linear system&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So far this seems like a somewhat arbitrary distinction, but bear with me for a moment. It so happens that, mathematically, linear systems have a lot of useful properties that makes the concept useful and easy to work with. Specifically, the above means that, when a linear system is excited at a given frequency, it will always amplify (or attenuate) that frequency by the same constant ratio; in other words, it has a well-defined, constant &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Frequency_response&#34;&gt;frequency response&lt;/a&gt;&lt;/em&gt;. This means we can quantify the amount of gain that will be applied by the system at any given frequency, and plot that data on a graph, called a &lt;em&gt;frequency response graph&lt;/em&gt; (or, more technically, a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bode_plot&#34;&gt;Bode plot&lt;/a&gt;&lt;/em&gt;). Mathematically, the function that is being plotted is called the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Transfer_function&#34;&gt;transfer function&lt;/a&gt;&lt;/em&gt; of the system, and that function is sufficient to completely describe the behavior of a linear system; hence the term &lt;em&gt;linear distortion&lt;/em&gt; to describe frequency response variations. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;One thing that I’ve omitted to keep things simple is that a linear system is not just allowed to apply gain at a particular frequency, it can also apply a constant &lt;em&gt;time delay&lt;/em&gt; to specific frequencies. This is known as the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Group_delay_and_phase_delay&#34;&gt;group delay&lt;/a&gt;&lt;/em&gt;. &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Phase_response&#34;&gt;Phase response&lt;/a&gt;&lt;/em&gt; is how that information is typically conveyed. Technically the term “frequency response” encompasses both &lt;em&gt;magnitude response&lt;/em&gt; and &lt;em&gt;phase response&lt;/em&gt;, though the latter is often dismissed for lack of relevance in most audio discussions.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://factualaudio.com/plots/peak-frequency-response.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;

&lt;p class=&#34;legend&#34;&gt;&lt;em&gt;The frequency response of a system that amplifies frequencies around 1 kHz by about 6 dB. This particular type of linear distortion is called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Resonance&#34;&gt;resonance&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/p&gt;

&lt;p&gt;How is this useful? Well, &lt;a href=&#34;https://factualaudio.com/post/anatomy/#the-frequency-domain&#34;&gt;remember&lt;/a&gt; that the frequency domain is often more useful than the time domain when it comes to understanding how audio signals are perceived. Frequency response tells us how a system alters the frequency components of the signal that flows through it. That makes it one the most powerful tools in the audio engineer’s toolbox.&lt;/p&gt;

&lt;p&gt;Studies show that frequency response is extremely important when it comes to assessing audio quality in real-world scenarios. For example it has been shown that the human auditory system is capable of detecting frequency response variations as tiny as 0.1 dB &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;Toole, F. and Olive. S., &amp;ldquo;&lt;a href=&#34;http://www.aes.org/e-lib/browse.cfm?elib=5163&#34;&gt;The Modification of Timbre by Resonances: Perception and Measurement&lt;/a&gt;&amp;rdquo;, J. Audio Eng. Soc., 36(3), 122–141, (1988). From Fig. 9: coherent sum of 0 dB and -40 dB sources is ~0.1 dB.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt;, and that it is by far the most important criterion when it comes to assessing the quality of a loudspeaker &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;Olive, Sean, “&lt;a href=&#34;http://www.aes.org/e-lib/browse.cfm?elib=12794&#34;&gt;A Multiple Regression Model For Predicting Loudspeaker Preference Using Objective Measurements: Part 1 — Listening Test Results&lt;/a&gt;”, presented at the 116th AES Convention, Berlin, Germany, preprint 6113, (May 2004).&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt;. Make no mistake: this metric is a &lt;em&gt;huge deal&lt;/em&gt;, and I expect most posts on this blog will make use of it in one way or another.&lt;/p&gt;

&lt;p&gt;The digital and analog realms typically contribute very little in the way of linear distortion. Virtually all of it is found in the acoustic realm, as even the best loudspeakers and headphones struggle to keep their frequency response within a few dB of the intended target &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;Many examples can be found in the &lt;a href=&#34;http://www.soundstagenetwork.com/index.php?option=com_content&amp;amp;view=article&amp;amp;id=16&amp;amp;Itemid=18&#34;&gt;SoundStage database&lt;/a&gt; (loudspeakers) and the &lt;a href=&#34;https://www.innerfidelity.com/headphone-measurements&#34;&gt;InnerFidelity database&lt;/a&gt; (headphones).&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt;. It gets worse: the acoustics of home listening rooms (&lt;a href=&#34;https://en.wikipedia.org/wiki/Room_modes&#34;&gt;room modes&lt;/a&gt; especially) can result in low frequency alterations in the order of 10 dB or more! &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;Literally any frequency response measurement made in a small room will show this phenomenon. One typical example can be found in Leduc Michel, 2009, “&lt;a href=&#34;http://www.audioholics.com/room-acoustics/listening-room-acoustics#ftn3&#34;&gt;How Does Listening Room Acoustics Affect Sound Quality?&lt;/a&gt;“, Audioholics (graph under &amp;ldquo;Standing waves” section). A series of representative examples can be found in Toole, Floyd E., &lt;em&gt;&lt;a href=&#34;https://books.google.co.uk/books?id=sGmz0yONYFcC&#34;&gt;Sound Reproduction: Loudspeakers and Rooms&lt;/a&gt;&lt;/em&gt;, figure 13.9.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt; No wonder why these are often said to be the “weakest links” of the audio chain…&lt;/p&gt;

&lt;h1 id=&#34;non-linear-distortion&#34;&gt;Non-linear distortion&lt;/h1&gt;

&lt;p&gt;This third category includes any kind of signal alteration that is not covered by the other two categories. Specifically, it includes any scenario in which the signal is altered in a way that is contingent on its shape (thus excluding noise) and behaves differently depending on the amplitude of the signal (thus excluding linear distortion). Due to the latter, this is also known as &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Amplitude_distortion&#34;&gt;amplitude distortion&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This definition makes this a very open-ended category, as indeed a signal can be distorted in an infinite variety of ways. Since it is not possible to run an infinite number of tests, measuring and quantifying the non-linear behavior of a system can be quite challenging. That said, most non-linear distortion comes in well-known shapes and forms, so a few standard measurements are usually good enough to characterize the performance of an audio system.&lt;/p&gt;

&lt;p&gt;One important aspect of non-linear distortion is that, contrary to linear distortion, it can cause new frequencies to appear that weren’t present in the original signal. By far the most well-known type of non-linear distortion is &lt;em&gt;harmonic distortion&lt;/em&gt;, where new frequencies appear that are whole multiples (&lt;a href=&#34;https://en.wikipedia.org/wiki/Harmonic&#34;&gt;harmonics&lt;/a&gt;) of the frequencies in the original signal. It is often summarized in a single number, &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Total_harmonic_distortion&#34;&gt;total harmonic distortion&lt;/a&gt;&lt;/em&gt; (THD), which indicates the total amplitude of the introduced harmonics relative to the original signal (more precisely, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fundamental_frequency&#34;&gt;fundamental&lt;/a&gt;). A related phenomenon is &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Intermodulation&#34;&gt;intermodulation distortion&lt;/a&gt;&lt;/em&gt;, where multiple frequency components in the signal interact with each other to create new frequencies in a specific pattern.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://factualaudio.com/plots/clipped-sine-wave.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;

&lt;img src=&#34;https://factualaudio.com/plots/clipped-sine-spectrum.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;

&lt;p class=&#34;legend&#34;&gt;&lt;em&gt;Waveform and spectrum of a 1 kHz sine wave showing symptoms of “hard clipping”, which produces odd-order harmonics (circled in red). The THD in this example is about 7%. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Here’s how this number is calculated. First, take the &lt;a href=&#34;https://factualaudio.com/post/amplitude/&#34;&gt;RMS amplitude&lt;/a&gt; of the harmonics: 3 kHz -26 dB (0.051) 5 kHz -32 dB (0.025) 7 kHz -46 dB (0.0047) 9kHz -47 dB (0.0043)… the &lt;a href=&#34;https://en.wikipedia.org/wiki/Root_mean_square#In_waveform_combinations&#34;&gt;total RMS amplitude&lt;/a&gt; of the harmonics is 0.057. The RMS amplitude of the fundamental is 0.78. The THD is the ratio of these two numbers, which is 0.073, or 7.3%.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/p&gt;

&lt;p&gt;Ideally, audio components should be able to accurately track the input signal, such that the output is precisely proportional to the input. Of course that is not the case in practice; consider, for example, that the driver inside a loudspeaker is made from imperfect materials that have imperfect physical properties, so its movement will not perfectly track the signal, instead giving rise to non-linear distortion. One example in the analog realm is &lt;a href=&#34;https://en.wikipedia.org/wiki/Crossover_distortion&#34;&gt;crossover distortion&lt;/a&gt; that can occur in certain types of amplifiers.&lt;/p&gt;

&lt;p&gt;So far I’ve been describing examples of non-linear distortion that can occur during normal system operation. However, the most egregious, obvious, and audible non-linear distortion issues occur when the system is driven beyond its limits; that is, signal amplitude is pushed too far and the system is unable to keep up. When this happens the peaks of the waveform cannot be reproduced faithfully and the system “bottoms out”, a phenomenon known as &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Clipping_(audio)&#34;&gt;clipping&lt;/a&gt;&lt;/em&gt;. To mention a few examples, this can happen in the digital realm (which is otherwise immune from most other forms of non-linear distortion) due to overflowing the largest possible sample value; in the analog realm due to exceeding the power limits of an amplifier; or in the acoustic realm due to exceeding the movement limits of a driver (&lt;a href=&#34;https://en.wikipedia.org/wiki/Excursion_(audio)&#34;&gt;excursion&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;So how audible is non-linear distortion? Well, that depends. Because non-linear distortion can take many forms, there is no simple answer. For example, distortion in the lower frequencies is less audible than in higher frequencies, and it is less audible if the newly introduced frequencies are close to the fundamental (thanks, once again, to masking). &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;Temme, Steve, &amp;ldquo;&lt;a href=&#34;https://www.bksv.com/media/doc/BO0385.pdf&#34;&gt;Application Note: Audio Distortion Measurements&lt;/a&gt;“, Brüel &amp;amp; Kjær, 1992.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt; Condensing non-linear distortion measurements into a single, simplistic THD number, as is often done, certainly doesn’t help. With that in mind, real-world examples tend to suggest that a THD of 10% is likely to be audible, 1% is borderline, and 0.1% is unlikely to be audible. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;Audioholics, ”&lt;a href=&#34;http://www.audioholics.com/room-acoustics/human-hearing-distortion-audibility-part-3&#34;&gt;Human Hearing - Distortion Audibility Part 3&lt;/a&gt;&amp;rdquo;, 2005.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amplitude, quantified</title>
      <link>https://factualaudio.com/post/amplitude/</link>
      <pubDate>Tue, 21 Nov 2017 21:04:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/amplitude/</guid>
      <description>

&lt;p&gt;In the last series of posts I’ve been focusing on the concept of amplitude, first &lt;a href=&#34;https://factualaudio.com/post/anatomy/&#34;&gt;defining it&lt;/a&gt; as the strength of an audio signal, then &lt;a href=&#34;https://factualaudio.com/post/life/&#34;&gt;describing its physical manifestations&lt;/a&gt; as the signal makes its way through the audio playback chain, and lastly explaining how &lt;a href=&#34;https://factualaudio.com/post/decibel/&#34;&gt;decibels&lt;/a&gt; are used as a way to manipulate these numbers.&lt;/p&gt;

&lt;p&gt;The attentive reader might have observed that, back in that very first post, I deliberately left out an important part for later. The issue that still needs to be addressed is exactly how to &lt;em&gt;quantify&lt;/em&gt; amplitude, i.e. how do we calculate its value for a given audio signal. Let’s go back to our original example of a pure tone (sine) waveform, but with a vertical scale this time:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/sine-wave-amplitude.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;Now let me ask you a question: what is the amplitude of that signal?&lt;/p&gt;

&lt;h1 id=&#34;peak-amplitude&#34;&gt;Peak amplitude&lt;/h1&gt;

&lt;p&gt;You might be tempted to answer that question with “1.0” &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Well, unless you are an audio engineer and you know what’s up. But in that case, what are you doing here?&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;, because that’s the as far as the curve travels from its middle point. Or perhaps you might answer “2.0”, because that’s the total height of the waveform (-1.0 to 1.0).&lt;/p&gt;

&lt;p&gt;These are not the only possible answers (as we’ll see below), but they are valid answers nonetheless. The former answer (&amp;ldquo;1.0&amp;rdquo;) is called the &lt;em&gt;peak amplitude&lt;/em&gt; of the signal. The latter answer (&amp;ldquo;2.0&amp;rdquo;) is called the &lt;em&gt;peak-to-peak amplitude&lt;/em&gt; of the signal.&lt;/p&gt;

&lt;p&gt;Peak-to-peak amplitude is twice the peak amplitude. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Strictly speaking that’s a bit of an oversimplification, because it assumes that audio signals are symmetrical, but they often aren’t — see Hetrich, Wayne L., &amp;ldquo;&lt;a href=&#34;http://www.aes.org/e-lib/browse.cfm?elib=2221&#34;&gt;Real-World Audio Wave Form Asymmetries and the Effect on the Audio Chain&lt;/a&gt;&amp;rdquo;, presented at the 55th AES Convention, New York, NY, USA, preprint 1193, (October 1976). However this rarely matters in practice.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; In practice peak amplitude is more widely used than peak-to-peak amplitude. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;One notable exception is marketing material (including manufacturer-provided specifications), where peak-to-peak amplitude is often used because the number looks bigger — don’t be fooled!&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-problem-with-peak-amplitude&#34;&gt;The problem with peak amplitude&lt;/h1&gt;

&lt;p&gt;You might be wondering why we can’t simply stop there. After all, we just came up with a quantitative definition of amplitude, so our job here is done, right?&lt;/p&gt;

&lt;p&gt;Not so fast. Let’s not forget that amplitude is used in a variety of contexts for a variety of calculations and comparisons. We need to make sure that the definition we came up with (peak amplitude) is appropriate for what we’re going to use it for.&lt;/p&gt;

&lt;p&gt;Peak amplitude is appropriate in &lt;em&gt;some&lt;/em&gt; contexts. For example, if you’re trying to determine whether a digital signal is going to clip, peak amplitude is definitely the metric you should use to make that determination. But in most cases, what we’re most interested in is the amount of &lt;em&gt;energy&lt;/em&gt; that is being conveyed in that audio signal &lt;em&gt;on average&lt;/em&gt;. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;&amp;ldquo;Energy&amp;rdquo; is used here in a generic sense, as physically speaking there is no “energy” in a digital signal for example. However it does map directly to the physical definition of energy when the signal enters the analog or acoustic realms, and since the acoustic realm is really all that matters in the end, it makes sense to use that term to describe audio signals in general.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; Peak amplitude fares poorly in that scenario. To understand why, let’s look at an extreme example of a signal that is very different from a sine wave:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/sinh-wave-amplitude.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;That signal has the same peak amplitude as the previous example. Yet, it’s easy to see that it conveys less energy: it’s mostly silence only interrupted by a train of narrow peaks. That makes peak amplitude ill-suited for estimating the overall strength of the signal.&lt;/p&gt;

&lt;h1 id=&#34;rms-to-the-rescue&#34;&gt;RMS to the rescue&lt;/h1&gt;

&lt;p&gt;To solve this problem, we need a different metric. Ideally, we want to compute some kind of &lt;em&gt;average&lt;/em&gt; of the signal. We can’t use the &lt;a href=&#34;https://en.wikipedia.org/wiki/Arithmetic_mean&#34;&gt;mean&lt;/a&gt; — that would just amount to zero, since the positive and negative parts of the signal would cancel each other out.&lt;/p&gt;

&lt;p&gt;As it turns out, there is a standard way to compute the average value of an audio signal (or any alternative signal for that matter): the &lt;a href=&#34;https://en.wikipedia.org/wiki/Root_mean_square&#34;&gt;root mean square&lt;/a&gt; (RMS). It’s a simple formula: we square the signal values, sum the squares, divide the result by the number of values, and then finally we take the square root of that number. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;For the sake of example, I’m assuming a discrete-time signal here.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; Because the values are squared, the positive and negative parts of the signal add up instead of canceling each other. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;This discussion assumes that the signal is centered on zero — i.e. that there is no &lt;a href=&#34;https://en.wikipedia.org/wiki/DC_bias&#34;&gt;DC offset&lt;/a&gt;. In audio this is almost always a safe assumption to make. Incidentally, RMS when used in this context is the same thing as &lt;a href=&#34;https://en.wikipedia.org/wiki/Standard_deviation&#34;&gt;standard deviation&lt;/a&gt;.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If we apply that formula to the first example, we end up with ~0.707. More generally, for a pure sine wave (and &lt;em&gt;only&lt;/em&gt; for a pure sine wave!), the math tells us RMS amplitude is equal to peak amplitude divided by the square root of two (√2). Or, when working in decibels, that’s peak amplitude minus ~3 dB.&lt;/p&gt;

&lt;p&gt;When applied to the second example, we end up with ~0.424. As expected, we get a lower value as the signal conveys less energy. In other words, the ratio of peak amplitude to RMS amplitude — known as the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Crest_factor&#34;&gt;crest factor&lt;/a&gt;&lt;/em&gt; &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Sometimes informally — and somewhat incorrectly — referred to as &lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_range#Music&#34;&gt;dynamic range&lt;/a&gt; in some contexts.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; — is different because the shape of the waveform is different.&lt;/p&gt;

&lt;h1 id=&#34;closing-thoughts&#34;&gt;Closing thoughts&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://factualaudio.com/plots/sine-wave-amplitude-detail.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;

&lt;img src=&#34;https://factualaudio.com/plots/sinh-wave-amplitude-detail.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;
&lt;/p&gt;

&lt;p&gt;We’ve seen that there is more than one way to quantify the amplitude of a signal, and that different approaches will produce different results depending on the shape of the waveform. Depending on the context in which the numbers are used, some approaches might be more appropriate than others.&lt;/p&gt;

&lt;p&gt;In practice, the method used to calculate the amplitude is often stated near the unit. For example, “Vrms”, “Vp”, “Vpp”. Otherwise, it is typically assumed that RMS was used. In particular, levels expressed in decibels (e.g. &amp;ldquo;dBV&amp;rdquo;), are virtually always RMS &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;&lt;a href=&#34;https://webstore.iec.ch/publication/94&#34;&gt;IEC 60027–3:2002&lt;/a&gt;, &lt;em&gt;Letter symbols to be used in electrical technology — Part 3: Logarithmic and related quantities, and their units&lt;/em&gt;, §4.1&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt; (with the possible exception of dBFS, which sadly is &lt;a href=&#34;https://en.wikipedia.org/wiki/DBFS&#34;&gt;ambiguous&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;What about &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Loudness&#34;&gt;loudness&lt;/a&gt;&lt;/em&gt;? One could see loudness as the way us humans measure the amplitude of the sounds that reach our ears. Because the human auditory system is extremely complex, it is not easy to estimate how loud a given signal will be perceived in general. Of the two approaches that I’ve described, RMS is the one that approximates loudness best, but it is still a very crude estimation. Nonetheless, in practice, RMS amplitude is often used as a poor man’s proxy for loudness due to its simplicity. More perceptually accurate ways to estimate loudness would typically involve &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Weighting&#34;&gt;weighting&lt;/a&gt;&lt;/em&gt; or even more advanced processes such as those described in &lt;a href=&#34;https://www.itu.int/rec/R-REC-BS.1770/en&#34;&gt;ITU-R BS.1770&lt;/a&gt;. But that’s a story for another post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decibels, explained</title>
      <link>https://factualaudio.com/post/decibel/</link>
      <pubDate>Tue, 21 Nov 2017 21:03:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/decibel/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://factualaudio.com/post/life/&#34;&gt;previous post&lt;/a&gt;, I introduced a number of physical quantities that are used to describe the &lt;em&gt;&lt;a href=&#34;https://factualaudio.com/post/anatomy/&#34;&gt;amplitude&lt;/a&gt;&lt;/em&gt; of an audio signal:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the digital domain, it is a &lt;em&gt;sample value&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;In the analog domain, it is voltage in &lt;em&gt;volts&lt;/em&gt; (V);&lt;/li&gt;
&lt;li&gt;In the acoustic domain, it is a pressure difference in &lt;em&gt;pascals&lt;/em&gt; (Pa).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, in the audio literature, marketing materials and equipment specifications, these are not the units that are typically used. Instead, one often finds such quantities expressed in &lt;em&gt;decibels&lt;/em&gt; (dB) or a related unit. There are good reasons for this, and they have to do with how we humans perceive loudness.&lt;/p&gt;

&lt;h1 id=&#34;on-the-usefulness-of-ratios&#34;&gt;On the usefulness of ratios&lt;/h1&gt;

&lt;p&gt;In audio, we care more about &lt;em&gt;ratios&lt;/em&gt; of quantities (e.g. &amp;ldquo;2×&amp;rdquo;) than absolute values (e.g. &amp;ldquo;3 V&amp;rdquo;). To understand why, here’s an example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The difference in loudness between 0.1 Pa and 0.2 Pa is very noticeable.&lt;/li&gt;
&lt;li&gt;The difference in loudness between 1.0 Pa and 1.1 Pa is barely noticeable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In both cases the absolute difference is the same: 0.1 Pa. However, the ratio is very different; in the former case it’s 2×, in the latter case it’s 1.1×. This makes a compelling case for using ratios, not differences, when comparing amplitudes.&lt;/p&gt;

&lt;p&gt;There is another advantage to using ratios. Remember that all three quantities (sample value, voltage, and sound pressure) are (hopefully) &lt;em&gt;proportional&lt;/em&gt; to each other when the audio signal moves from one realm to the next. This means that a given ratio applies in all three domains: twice the sample value is also twice the voltage and twice the sound pressure. This is very convenient because it means that a given ratio (often called &lt;a href=&#34;https://en.wikipedia.org/wiki/Gain_%28electronics%29&#34;&gt;gain&lt;/a&gt;) has the same meaning regardless of context.&lt;/p&gt;

&lt;p&gt;Now, if only there was a unit that made working with ratios easier…&lt;/p&gt;

&lt;h1 id=&#34;enter-the-decibel&#34;&gt;Enter the decibel&lt;/h1&gt;

&lt;p&gt;The &lt;em&gt;decibel&lt;/em&gt; (dB) is, in its purest form, a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Dimensionless_quantity&#34;&gt;dimensionless unit&lt;/a&gt;&lt;/em&gt; that represents a ratio between two quantities, just like “×” or “%”. What sets the decibel apart is that it is also a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Logarithmic_scale&#34;&gt;logarithmic unit&lt;/a&gt;&lt;/em&gt;, meaning that its value is proportional to the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Logarithm&#34;&gt;logarithm&lt;/a&gt;&lt;/em&gt; of the ratio, not the ratio itself. This will be clearer with examples:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Amplitude ratio&lt;/th&gt;
&lt;th&gt;Decibel equivalent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0.01×&lt;/td&gt;
&lt;td&gt;-40 dB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.1×&lt;/td&gt;
&lt;td&gt;-20 dB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;0.5×&lt;/td&gt;
&lt;td&gt;-6 dB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1×&lt;/td&gt;
&lt;td&gt;0 dB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2×&lt;/td&gt;
&lt;td&gt;+6 dB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4×&lt;/td&gt;
&lt;td&gt;+12 dB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8×&lt;/td&gt;
&lt;td&gt;+18 dB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10×&lt;/td&gt;
&lt;td&gt;+20 dB&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;100×&lt;/td&gt;
&lt;td&gt;+40 dB&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Because the decibel is a logarithmic unit, it behaves differently from more conventional linear units. The trick is to not fight their logarithmic nature, but embrace it. Here’s how:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The most useful ratios to keep in mind are +6 dB (2×) and +20 dB (10×).&lt;/li&gt;
&lt;li&gt;To invert the ratio, just change the sign: for example, -6 dB is the same as dividing by 2.&lt;/li&gt;
&lt;li&gt;When combining ratios, decibels don’t multiply; they add up. For example, 4× (2×2) is 12 dB (6+6), not 36. This might seem strange, but this property makes decibels very convenient to use in practice — it’s easier to add than to multiply.&lt;/li&gt;
&lt;li&gt;0 dB means 1×, i.e. no change in amplitude.&lt;/li&gt;
&lt;li&gt;0× is -∞ dB (negative infinity). You might have seen this as the “mute” position on some volume knobs.&lt;/li&gt;
&lt;li&gt;For less trivial cases, &lt;a href=&#34;http://www.sengpielaudio.com/calculator-db.htm&#34;&gt;online calculators&lt;/a&gt; are available.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that all of the above assumes decibels are used to express ratios of &lt;a href=&#34;https://en.wikipedia.org/wiki/Field,_power,_and_root-power_quantities&#34;&gt;field quantities&lt;/a&gt;. Digital sample value, voltage, and sound pressure are examples of field quantities. When dealing with &lt;em&gt;power&lt;/em&gt; quantities (i.e. watts), however, there is a catch: in that case, +6 dB is 4×, not 2×, which is +3 dB. This might seem strange and confusing, but once again there is an explanation: in practice power is proportional to the &lt;em&gt;square&lt;/em&gt; of the field quantity. So, for example, when voltage is doubled, power quadruples. Or, said differently, if voltage is increased by 6 dB, then power increases by… 6 dB — which is why the rules makes sense.&lt;/p&gt;

&lt;h1 id=&#34;using-decibels-for-absolute-values&#34;&gt;Using decibels for absolute values&lt;/h1&gt;

&lt;p&gt;In theory, one could stick with linear units when dealing with absolute values (e.g. 2 V), and use decibels when dealing with ratios (e.g. 2×). However, when a calculation involves both, the mental gymnastics can be challenging. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Not convinced? Try calculating 2 V × -11 dB by hand.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It would make more sense to use decibels for everything, including absolute quantities. Fortunately, that’s easy: we just need to agree on a reference, and then express in decibels the ratio between that reference and the quantity we wish to convey. The resulting decibel value is called &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Level_%28logarithmic_quantity%29&#34;&gt;level&lt;/a&gt;&lt;/em&gt;. For example, if the reference is 1 V, then the level of 2 V is +6 dB. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;And the calculation from the previous note becomes 6 dB - 11 dB = -5 dB. Much easier!&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In practice, the reference value is indicated by a suffix affixed to the unit. Here are some references commonly used in the three audio realms:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Quantity&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Equivalent level&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sound pressure&lt;/td&gt;
&lt;td&gt;20 µPa&lt;/td&gt;
&lt;td&gt;0 &lt;a href=&#34;https://en.wikipedia.org/wiki/Sound_pressure#Sound_pressure_level&#34;&gt;dB SPL&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Voltage&lt;/td&gt;
&lt;td&gt;1 V&lt;/td&gt;
&lt;td&gt;0 &lt;a href=&#34;https://en.wikipedia.org/wiki/Decibel#Voltage&#34;&gt;dBV&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Voltage&lt;/td&gt;
&lt;td&gt;~0.77 V&lt;/td&gt;
&lt;td&gt;0 &lt;a href=&#34;https://en.wikipedia.org/wiki/Decibel#Voltage&#34;&gt;dBu&lt;/a&gt; &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Yes, there are two different references in common use for voltages. That’s sad, but that’s the way it is. To convert from one to the other, add or subtract ~2.2 dB.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Sample value&lt;/td&gt;
&lt;td&gt;Full scale&lt;/td&gt;
&lt;td&gt;0 &lt;a href=&#34;https://en.wikipedia.org/wiki/DBFS&#34;&gt;dBFS&lt;/a&gt; &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;In other words, 0 dBFS is the maximum level before digital clipping (truncation) occurs. Thus valid amplitudes have negative dBFS values. Or at least that’s how it’s supposed to work. The definition of dBFS can be quite fuzzy and ambiguous, as explained in that Wikipedia page. Caveat emptor.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Sometimes the suffix is omitted and has to be inferred from the context. For example, it is fairly common to find “dB SPL” written as simply “dB”, especially in mainstream publications. This is best avoided as it can lead to confusion between ratios and levels. Conversely, “dBr” (&amp;ldquo;relative&amp;rdquo;) is sometimes used to make it explicit that decibels are used to express a ratio, not a level.&lt;/p&gt;

&lt;p&gt;Additional suffixes and variants are often used to convey additional information. The most common examples include “peak” or “RMS”, which denote different ways of looking at the amplitude of the signal, and “dBA”, which indicates the use of &lt;a href=&#34;https://en.wikipedia.org/wiki/A-weighting&#34;&gt;A-weighting&lt;/a&gt;. More on these in later posts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Life of an audio signal</title>
      <link>https://factualaudio.com/post/life/</link>
      <pubDate>Tue, 21 Nov 2017 21:02:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/life/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;https://factualaudio.com/post/anatomy/&#34;&gt;previous post&lt;/a&gt;, I described what an audio signal is. Now, let’s discuss how this signal is concretely represented as it passes through each stage of a typical home audio playback system.&lt;/p&gt;

&lt;p&gt;For the sake of this discussion, I am going to assume the most common and straightforward use case: playing a &lt;em&gt;digital stream&lt;/em&gt; over loudspeakers (or headphones). By “digital stream” I mean any audio signal that is processed by a computer or computer-like system; that can be anything including a MP3 file, online video, online music streaming, the soundtrack of a Blu-ray disc, etc. This does not include analogue media such as vinyl discs or cassette tapes.&lt;/p&gt;

&lt;p&gt;Before this digital content can reach your eardrums, it has to go through a series of steps. Between these steps, the audio signal is materialized in different ways depending on which part of the audio “pipeline” we are looking at. In this post I will refer to these concrete representations as &lt;em&gt;realms&lt;/em&gt; &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;“realm” is not a widely used term — the term “domain” is normally used. However, I felt that this could create confusion with &lt;em&gt;time domain&lt;/em&gt; and &lt;em&gt;frequency domain&lt;/em&gt;, which are completely unrelated concepts.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;. I am going to start at the source and then make my way through to the listener.&lt;/p&gt;

&lt;p&gt;To keep things clear and simple, the example signal I’ll use throughout this post is a monophonic (one channel) 1 kHz sine wave. For all intents of purposes, each additional channel can be assumed to act like a completely separate audio signal that takes a similar path through the system.&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/diagrams/realms.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;h1 id=&#34;the-digital-realm&#34;&gt;The digital realm&lt;/h1&gt;

&lt;p&gt;It all starts within the digital device, which can be any computer or computer-like gadget (PC, smartphone, Bluetooth receiver, etc.). Most devices read or receive audio data in &lt;em&gt;digitally compressed&lt;/em&gt; form. Popular digital compression algorithms include &lt;a href=&#34;https://en.wikipedia.org/wiki/MP3&#34;&gt;MP3&lt;/a&gt;, &lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_Audio_Coding&#34;&gt;AAC&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/FLAC&#34;&gt;FLAC&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Data_compression&#34;&gt;Digital compression&lt;/a&gt; &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Not to be confused with &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_range_compression&#34;&gt;dynamic range compression&lt;/a&gt;&lt;/em&gt;, which is completely unrelated.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; is a complicated subject, which I won’t dig into further in this post. In any case, the data first goes through a &lt;em&gt;decoder&lt;/em&gt; which converts the compressed signal into uncompressed form, which looks like this:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/1khz-sine-wave-digital.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;This plot shows that, in the digital realm, audio is not represented by a continuous, smoothly changing signal — instead, all we have are regularly-spaced “snapshots” that indicate what the signal amplitude is at some point in time. This is called a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Discrete-time_signal&#34;&gt;discrete signal&lt;/a&gt;&lt;/em&gt;, and the “snapshots” are called &lt;em&gt;samples&lt;/em&gt;. In this example, we have 44100 samples every second, or more formally, the &lt;em&gt;sample rate&lt;/em&gt; is 44.1 kHz. Such a sample rate is standard for music — other types of content, such as movies or games, use a slightly higher rate, 48 kHz, for mostly historical reasons.&lt;/p&gt;

&lt;p&gt;Because memory is not infinite, each sample value has a finite precision. In practice, each sample is typically converted to a signed integer with a precision, or &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Audio_bit_depth&#34;&gt;bit depth&lt;/a&gt;&lt;/em&gt;, of 16 bits. This process is called &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Quantization_%28signal_processing%29&#34;&gt;quantization&lt;/a&gt;&lt;/em&gt;. A 16-bit signed integer can take a value from -32768 to +32767. Samples outside of this range cannot be represented, and will be clamped to the nearest possible value; this is called &lt;em&gt;digital &lt;a href=&#34;https://en.wikipedia.org/wiki/Clipping_%28signal_processing%29&#34;&gt;clipping&lt;/a&gt;&lt;/em&gt;, and is best avoided as it sounds quite bad. A signal that peaks at the highest possible amplitude without clipping is called a &lt;em&gt;full-range&lt;/em&gt; or &lt;em&gt;full-scale&lt;/em&gt; signal&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;&lt;a href=&#34;https://webstore.iec.ch/publication/5664&#34;&gt;IEC 61606–1:2009&lt;/a&gt;, &lt;em&gt;Digital audio parts — Basic measurement methods of audio characteristics — General&lt;/em&gt;, §3.1.10&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Finally, the signal is physically represented simply by transmitting the value of each point, or &lt;em&gt;sample&lt;/em&gt;, one after the other. For example, the above signal is transmitted as 4653, 9211, 13583, etc. in the form of binary numbers. This way of transmitting the signal is called &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Pulse-code_modulation&#34;&gt;pulse-code modulation&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This section just skirted the surface of how digital audio works. The details of how sampled signals behave in practice can be quite involved and are often counter-intuitive; as a result, misrepresentation of digital audio phenomena is quite commonplace in the audiophile community, leading to confusion and misguided advice. Digital audio expert &lt;a href=&#34;https://en.wikipedia.org/wiki/Chris_Montgomery&#34;&gt;Chris Montgomery&lt;/a&gt; produced a &lt;a href=&#34;https://xiph.org/video/&#34;&gt;series of videos&lt;/a&gt; that explains these complex phenomena with very clear and straightforward examples — it is a must-watch if you wish to learn more about the digital realm.&lt;/p&gt;

&lt;h1 id=&#34;the-analog-realm&#34;&gt;The analog realm&lt;/h1&gt;

&lt;p&gt;Loudspeakers and headphones cannot receive a digital signal; it has to be converted to an &lt;em&gt;analog&lt;/em&gt; signal first. This conversion is done in an electronic circuit appropriately named the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Digital-to-analog_converter&#34;&gt;digital-to-analog converter&lt;/a&gt;&lt;/em&gt;, or DAC. This is where computer engineering ends and electrical engineering begins. The main task of the DAC is to take each sample value and convert it to some electrical &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Voltage&#34;&gt;voltage&lt;/a&gt;&lt;/em&gt; on its output pins. The resulting signal looks like the following:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/1khz-sine-wave-analog.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;It is important to realize that in the plot above, the unit used for the vertical scale is the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Volt&#34;&gt;volt&lt;/a&gt;&lt;/em&gt;. In other words, the audio signal in the analog domain is defined by its &lt;em&gt;voltage&lt;/em&gt;. It is &lt;em&gt;not&lt;/em&gt; defined by &lt;a href=&#34;https://en.wikipedia.org/wiki/Electric_current&#34;&gt;current&lt;/a&gt; nor &lt;a href=&#34;https://en.wikipedia.org/wiki/Electric_power&#34;&gt;power&lt;/a&gt;. Even when the signal is used as the input of a loudspeaker, it is still voltage that determines the sound that comes out; power dissipation is a &lt;em&gt;consequence&lt;/em&gt;, not a &lt;em&gt;cause&lt;/em&gt;, of the audio signal flowing through the loudspeaker. As Pat Brown &lt;a href=&#34;http://www.prosoundtraining.com/site/author/pat-brown/meaningful-metrics-the-use-and-abuse-of-loudspeaker-power-ratings/&#34;&gt;elegantly puts it&lt;/a&gt;: &amp;ldquo;power is &lt;em&gt;drawn&lt;/em&gt;, not applied&amp;rdquo;. Another way to state this is to say that properly engineered analog audio devices act as &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Voltage_source&#34;&gt;voltage sources&lt;/a&gt;&lt;/em&gt;, which are connected to each other by way of &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Impedance_bridging&#34;&gt;impedance bridging&lt;/a&gt;&lt;/em&gt;. This is a frequent source of confusion.&lt;/p&gt;

&lt;p&gt;The DAC took our discrete signal and converted it into a continuous electrical signal, whose voltage is (hopefully) &lt;em&gt;proportional&lt;/em&gt; to the digital sample value. The central (mean) value of the signal, called the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/DC_bias&#34;&gt;DC offset&lt;/a&gt;&lt;/em&gt;, is zero volts; the signal swings around that central value, &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Alternating_current&#34;&gt;alternating&lt;/a&gt;&lt;/em&gt; between positive and negative voltage. In this example, our full-scale digital stream was converted to an analog signal that swings between -1.41 V and +1.41 V. Depending on the specific model of DAC used, its volume control setting (if any) and the signals involved, these numbers can vary — typical peak amplitude can go as low as 0.5 V &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;Wikipedia, &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Line_level#Nominal_levels&#34;&gt;Nominal levels&lt;/a&gt;&lt;/em&gt;, peak amplitude for consumer audio&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt; or as high as 2.8 V &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;&lt;a href=&#34;https://webstore.iec.ch/publication/6142&#34;&gt;IEC 61938:2013&lt;/a&gt;, &lt;em&gt;Guide to the recommended characteristics of analogue interfaces to achieve interoperability&lt;/em&gt;, §8.2.1&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt;. Low voltage signals are easier (cheaper) to implement, but are more susceptible to noise and interference.&lt;/p&gt;

&lt;p&gt;The amount of &lt;em&gt;current&lt;/em&gt; or &lt;em&gt;power&lt;/em&gt; transferred from the source of an analog signal to the equipment plugged in at the other end of the cable (the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Electrical_load&#34;&gt;load&lt;/a&gt;&lt;/em&gt;) is determined by the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Electrical_impedance&#34;&gt;impedance&lt;/a&gt;&lt;/em&gt; of the load, also known as the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Input_impedance&#34;&gt;input impedance&lt;/a&gt;&lt;/em&gt;. According to &lt;a href=&#34;https://en.wikipedia.org/wiki/Ohm&#39;s_law&#34;&gt;Ohm’s law&lt;/a&gt;, the lower the impedance, the more current, and therefore power, will be required to sustain a given voltage. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Note that, in the case of loudspeakers, power is not a good indication of loudness, because some of that power is dissipated in the form of heat, not sound. A more &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Electrical_efficiency&#34;&gt;efficient&lt;/a&gt;&lt;/em&gt; loudspeaker will produce louder sound from the same amount of power compared to a less efficient loudspeaker, even if their impedances are the same.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;DACs, as well as most other types of analog audio equipment (such as filters or mixers), are not designed to provide significant amounts of power. Instead, they are meant to be connected to a high-impedance load, typically 20 kΩ or higher &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [ref] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-ref&#34;&gt;&lt;a href=&#34;https://webstore.iec.ch/publication/6142&#34;&gt;IEC 61938:2013&lt;/a&gt;, &lt;em&gt;Guide to the recommended characteristics of analogue interfaces to achieve interoperability&lt;/em&gt;, §8.2.1&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end ref] &lt;/strong&gt;. This means that the load is acting much like a &lt;a href=&#34;https://en.wikipedia.org/wiki/Voltmeter&#34;&gt;voltmeter&lt;/a&gt; or oscilloscope — it is “peeking” at the input voltage without drawing significant power from it. Such a signal that carries some voltage but very little power is called a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Line_level&#34;&gt;line-level&lt;/a&gt;&lt;/em&gt; signal.&lt;/p&gt;

&lt;p&gt;On the other hand, loudspeakers (and headphones to a lesser extent) are low-impedance devices — typically between 4 Ω and 8 Ω in the case of speakers. This means they require a lot of power to operate, even though their input voltage is not necessarily much higher than the signal shown above. For example, most speakers will happily produce comfortably loud sound with as little as 5 V, but might consume as much as 6 &lt;a href=&#34;https://en.wikipedia.org/wiki/Watt&#34;&gt;watts&lt;/a&gt; while doing so &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;From the numbers given a keen eye will &lt;a href=&#34;http://www.sengpielaudio.com/calculator-ohm.htm&#34;&gt;deduce&lt;/a&gt; that this example speaker has an impedance of 4 Ω. One thing to note is that loudspeaker impedance is highly dependent on the frequency of the signal, making the use of one number an oversimplification. The impedance that manufacturers advertise, called the &lt;em&gt;rated impedance&lt;/em&gt;, is 1.25 times the &lt;em&gt;minimum&lt;/em&gt; impedance of the speaker across its rated frequency range. (see &lt;a href=&#34;https://webstore.iec.ch/publication/1223&#34;&gt;IEC 60268–5:2003&lt;/a&gt;, &lt;em&gt;Loudspeakers&lt;/em&gt;, §16.1)&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;, an amount of power that typical line-level equipment is not able to provide.&lt;/p&gt;

&lt;p&gt;This problem is solved by using a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Audio_power_amplifier&#34;&gt;power amplifier&lt;/a&gt;&lt;/em&gt;. This is a component that conveniently provides a high-impedance input for connecting line-level equipment, while exposing an output that is capable of providing large amounts of power, typically 10 W or more, to a low-impedance load. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;In practice, most amplifiers are also capable of increasing the voltage (amplitude) of the signal; this is called the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gain_%28electronics%29&#34;&gt;gain&lt;/a&gt; of the amplifier. This is because most loudspeakers require voltages that are somewhat higher than line level in order to play loud enough. Still, the primary goal of a power amplifier is to provide power, not to increase voltage.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; Such outputs provide so-called &lt;em&gt;speaker-level signals&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In some home audio systems, the DAC and the amplifier are integrated into one single device, which is called an &lt;em&gt;integrated amplifier&lt;/em&gt; or more commonly an &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/AV_receiver&#34;&gt;AV receiver&lt;/a&gt;&lt;/em&gt; (AVR).&lt;/p&gt;

&lt;h1 id=&#34;the-acoustic-realm&#34;&gt;The acoustic realm&lt;/h1&gt;

&lt;p&gt;Finally, in order to reach your ears, the analog signal must be converted to an &lt;em&gt;acoustic&lt;/em&gt; signal, that is, actual &lt;a href=&#34;https://en.wikipedia.org/wiki/Sound&#34;&gt;sound waves&lt;/a&gt;. This is accomplished using a device called an &lt;em&gt;electroacoustic &lt;a href=&#34;https://en.wikipedia.org/wiki/Transducer&#34;&gt;transducer&lt;/a&gt;&lt;/em&gt;, or &lt;em&gt;driver&lt;/em&gt;. The output of a driver when excited with our example signal, as measured at some point in front of it, might look like the following:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/1khz-sine-wave-acoustic.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;Note the change of vertical scale. We’re not dealing with voltage anymore — we’re dealing with &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Sound_pressure&#34;&gt;sound pressure&lt;/a&gt;&lt;/em&gt; instead. Indeed, sound is a physical phenomenon in which transient changes in pressure (&lt;em&gt;compression&lt;/em&gt;, &lt;em&gt;rarefaction&lt;/em&gt;) produced by the vibration of a &lt;em&gt;sound source&lt;/em&gt; propagate through the space around it. In other words, it is a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Longitudinal_wave&#34;&gt;longitudinal wave&lt;/a&gt;&lt;/em&gt;. Sound pressure, expressed in &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Pascal_%28unit%29&#34;&gt;Pascals&lt;/a&gt;&lt;/em&gt; (Pa), quantifies the difference between normal atmospheric pressure and some local, dynamic change in pressure, at a given point in time and space. The human ear is equipped to detect these changes, which are then — finally! — perceived as sound by the human brain.&lt;/p&gt;

&lt;p&gt;An ideal transducer will produce sound pressure &lt;em&gt;proportional&lt;/em&gt; to the voltage applied to it, like in the above waveform. However, it is difficult to design a driver that is capable of doing that across the entire range of audible frequencies. Consequently, a number of transducer types are available, which are commonly referred to as &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Subwoofer&#34;&gt;subwoofers&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Woofer&#34;&gt;woofers&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Mid-range_speaker&#34;&gt;midranges&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Tweeter&#34;&gt;tweeters&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In order to reproduce the entire range of human hearing, several of these drivers — two or three, typically — are assembled inside a single “box”, called the &lt;em&gt;enclosure&lt;/em&gt;. The drivers are typically mounted flush with one side of the box, which is called the &lt;em&gt;front baffle&lt;/em&gt;. An electrical circuit called a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Audio_crossover&#34;&gt;crossover&lt;/a&gt;&lt;/em&gt; splits the input signal into the frequency ranges appropriate for each driver. The resulting device is called a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Loudspeaker&#34;&gt;loudspeaker&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;What I’ve described here is called a &lt;em&gt;passive&lt;/em&gt; loudspeaker, which is the most common type in consumer “Hi-Fi” systems. Sometimes the amplifier and speaker are integrated into the same device; this is called an &lt;em&gt;active&lt;/em&gt; or &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Powered_speakers&#34;&gt;powered&lt;/a&gt;&lt;/em&gt; speaker. Examples include professional “studio monitor” speakers, which have line-level inputs. Other products, such as “Bluetooth speakers”, go one step further and throw in a DAC as well for a completely integrated solution.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Headphones&#34;&gt;Headphones&lt;/a&gt;&lt;/em&gt; are a special case and typically only have one driver per channel, which makes them simpler. Conceptually, a headphone is akin to a miniature loudspeaker. Because of their proximity to the ear, they don’t have to produce as much sound pressure; therefore they require much less power to operate (often less than 1 mW).&lt;/p&gt;

&lt;p&gt;One very important aspect of the acoustic realm is that sound propagates in all three dimensions — the audio signal (sound pressure) is not the same at every point in space. In particular, speakers exhibit &lt;em&gt;radiation patterns&lt;/em&gt; that vary with angle and frequency, and the sound they emit can bounce off surfaces (&lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Reflection_%28physics%29#Sound_reflection&#34;&gt;reflection&lt;/a&gt;&lt;/em&gt;). This in turn means that they interact with their environment (the listening room, or, in the case of headphones, the listener’s head) in ways that are complicated and difficult to predict but nonetheless have an enormous impact on how the radiated sound will be perceived by a human listener. This makes choosing and configuring a speaker system quite the challenge. Hopefully, future posts on this blog will be able to help.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Anatomy of an audio signal</title>
      <link>https://factualaudio.com/post/anatomy/</link>
      <pubDate>Tue, 21 Nov 2017 21:01:00 +0100</pubDate>
      <author>etienne@edechamps.fr (Etienne Dechamps)</author>
      <guid>https://factualaudio.com/post/anatomy/</guid>
      <description>

&lt;p&gt;This inaugural Factual post will start from first principles, by laying down some of the fundamental foundations necessary to start reasoning about audio signals. I will then build on these principles in the posts to follow.&lt;/p&gt;

&lt;h1 id=&#34;the-time-domain&#34;&gt;The time domain&lt;/h1&gt;

&lt;p&gt;An audio signal is an oscillating phenomenon: it is defined by a quantity that alternatively increases and decreases over time while keeping in the vicinity of some central value.&lt;/p&gt;

&lt;p&gt;Out of the infinity of shapes that an audio signal can take, probably the simplest is a &lt;em&gt;pure tone&lt;/em&gt;, also called a &lt;em&gt;sine wave&lt;/em&gt; from the name of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sine&#34;&gt;mathematical function&lt;/a&gt; that it describes. Here is an example of a sine wave:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/1khz-sine-wave.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;The horizontal axis is time, which is why it is often said that this representation shows the signal in the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Time_domain&#34;&gt;time domain&lt;/a&gt;&lt;/em&gt; (another term is &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Waveform&#34;&gt;waveform&lt;/a&gt;&lt;/em&gt;). The above signal oscillates around the central value represented by the horizontal line. According to the horizontal scale, this particular signal repeats one thousand times per second: it has a &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Frequency&#34;&gt;frequency&lt;/a&gt;&lt;/em&gt; of 1000 &lt;a href=&#34;https://en.wikipedia.org/wiki/Hertz&#34;&gt;hertz&lt;/a&gt;. In order to be audible, the frequency of the signal must sit between &lt;strong&gt;20 Hz&lt;/strong&gt; and &lt;strong&gt;20 kHz&lt;/strong&gt;: this interval is known as the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hearing_range&#34;&gt;audible range&lt;/a&gt;&lt;/em&gt; of the human hearing system.&lt;/p&gt;

&lt;p&gt;In the image above, the height of the signal is known as the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Amplitude&#34;&gt;amplitude&lt;/a&gt;&lt;/em&gt; (the term &lt;em&gt;magnitude&lt;/em&gt; is also used). I deliberately left out the vertical scale and unit because they depend on the context — more on this to follow in the post about &lt;a href=&#34;https://factualaudio.com/post/life/&#34;&gt;audio realms&lt;/a&gt;. Another complication is that the “height” of the curve can be defined in multiple different ways — which are explored in a &lt;a href=&#34;https://factualaudio.com/post/amplitude/&#34;&gt;separate post&lt;/a&gt; as well.&lt;/p&gt;

&lt;p&gt;Amplitude is related to &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Loudness&#34;&gt;loudness&lt;/a&gt;&lt;/em&gt;, in the sense that if we take a signal and increase its amplitude (by &lt;em&gt;amplifying&lt;/em&gt; its oscillations), the human hearing system will perceive the signal to be louder. Likewise, if we decrease its amplitude (by &lt;em&gt;attenuating&lt;/em&gt;), it will be perceived as being quieter. Be careful however, because this relationship between amplitude and loudness does &lt;em&gt;not&lt;/em&gt; necessarily hold when comparing signals that have differing frequencies. This is due to the fact that the human hearing system does not perceive all frequencies as being equally loud &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;The effect can be quantified using &lt;a href=&#34;https://en.wikipedia.org/wiki/Equal-loudness_contour&#34;&gt;equal-loudness contours&lt;/a&gt;.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;. For example, if you listen to a 30 Hz tone and then to a 2 kHz tone of equal amplitude, the latter will sound much louder than the former. Be sure to keep this in mind when reasoning about loudness.&lt;/p&gt;

&lt;p&gt;Of course, most audio content is not a pure tone. In practice, a typical audio signal for, say, music, might look like this:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/piano-c5-wave.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;As the above image shows, a musical signal is way more complex than a pure tone. And that’s not even a complicated musical piece — this is pianist Joohyun Park, solo, playing a single note &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Specifically, this is one of the first notes played at the beginning of the &lt;em&gt;Allegro&lt;/em&gt; track from &lt;em&gt;&lt;a href=&#34;http://www.bearmccreary.com/blog/blog/battlestar-galactica-3/battlestar-galactica-solo-piano-cd/&#34;&gt;The Music of Battlestar Galactica for Solo Piano&lt;/a&gt;&lt;/em&gt;.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;. What’s really problematic, however, is that this representation doesn’t seem to relate to our perception at all — to the naked eye, it doesn’t look like a musical note played on a piano, it just looks like random data.&lt;/p&gt;

&lt;h1 id=&#34;the-frequency-domain&#34;&gt;The frequency domain&lt;/h1&gt;

&lt;p&gt;In order to make sense of such complex signals, we need a better way to look at the data. Fortunately, the above signal can be decomposed into a number of pure tones of various frequencies and amplitudes, thanks to the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Superposition_principle&#34;&gt;superposition principle&lt;/a&gt;&lt;/em&gt;. The mathematical tool used to do the decomposition is called the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Fourier_transform&#34;&gt;Fourier transform&lt;/a&gt;&lt;/em&gt;. For example, if we were to apply the Fourier transform to our first pure tone example, the result could be represented as follows:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/1khz-sine-spectrum.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;The vertical axis is still amplitude, but the horizontal axis has changed — it now represents frequency. This representation shows the signal in the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Frequency_domain&#34;&gt;frequency domain&lt;/a&gt;&lt;/em&gt;, or, in other words, it shows the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Spectral_density&#34;&gt;spectral density&lt;/a&gt;&lt;/em&gt; (often simply called &lt;em&gt;spectrum&lt;/em&gt;) of the signal.&lt;/p&gt;

&lt;p&gt;A keen eye might have noticed that the horizontal axis is using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Logarithmic_scale#Graphic_representation&#34;&gt;logarithmic scale&lt;/a&gt;, which is typical for this type of plot. This scale provides a better view of how we perceive sound: it is very easy to hear the difference between a 100 Hz tone and a 200 Hz tone, but the same cannot be said about 10000 Hz and 10100 Hz tones, even though the difference is still 100 Hz. This is because in the former case, there is a 100% increase, while in the latter case, the increase is only 1%. In other words, the human auditory system perceives &lt;em&gt;relative change&lt;/em&gt;, as opposed to absolute change &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;This is consistent with other human senses, as predicted by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law&#34;&gt;Weber-Fechner law&lt;/a&gt;.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt;. The term &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Octave&#34;&gt;octave&lt;/a&gt;&lt;/em&gt; is used to describe a frequency factor of &lt;em&gt;two&lt;/em&gt;; for example, the range 2 kHz to 8 kHz is two octaves wide. The term &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Decade_%28log_scale%29&#34;&gt;decade&lt;/a&gt;&lt;/em&gt; is also sometimes used, and describes a tenfold increase in frequency.&lt;/p&gt;

&lt;p&gt;The above plot is showing us that the signal can be decomposed into a single 1 kHz tone, but we already knew that. What’s more interesting is what happens when we apply the Fourier transform to the musical signal:&lt;/p&gt;

&lt;img src=&#34;https://factualaudio.com/plots/piano-c5-spectrum.png&#34; width=&#34;100%&#34; class=&#34;figure replacewithsvg&#34;&gt;


&lt;p&gt;Here things become interesting. This plot is telling us that our musical example can be decomposed into a 260 Hz tone with high amplitude, combined with 520 Hz and 780 Hz tones of lower amplitude.&lt;/p&gt;

&lt;p&gt;Such a result is typical for a recording of a single instrument playing a single note. The first tone, at 260 Hz, is called the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Fundamental_frequency&#34;&gt;fundamental&lt;/a&gt;&lt;/em&gt; and indicates the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Pitch_%28music%29&#34;&gt;pitch&lt;/a&gt;&lt;/em&gt; of the sound, in other words the note being played, &lt;a href=&#34;https://en.wikipedia.org/wiki/C_%28musical_note%29#Designation_by_octave&#34;&gt;C5&lt;/a&gt; in this example. The 520 Hz and 780 Hz tones, because they are &lt;em&gt;multiples&lt;/em&gt; of the fundamental, are called &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Harmonic&#34;&gt;harmonics&lt;/a&gt;&lt;/em&gt;. They are interpreted by the human hearing system to determine the &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Timbre&#34;&gt;timbre&lt;/a&gt;&lt;/em&gt; of the instrument. If the same note was being played on say, a flute or a violin, the frequency of the fundamental would be the same but the relative amplitudes of the harmonics would be different.&lt;/p&gt;

&lt;p&gt;This is interesting because we can directly relate what we see on the plot to how the sound will be &lt;em&gt;perceived&lt;/em&gt;, i.e. what the signal sounds like. Of course interpreting this data still requires some effort — most people wouldn’t be able to tell “of course that’s a piano playing a C5” just by eyeballing the above image. Furthermore, if I had used a more complex example (such as a symphonic orchestra playing in unison), the spectrum would have been just as unreadable as the waveform. Nevertheless, in practice, the spectrum often provides a much more useful view from a &lt;em&gt;perceptual&lt;/em&gt; perspective. This is why audio engineers will often ignore the time domain, instead focusing their efforts on the frequency domain.&lt;/p&gt;

&lt;p&gt;Frequency-domain data can be converted back to the time domain using the appropriately-named &lt;em&gt;inverse Fourier transform&lt;/em&gt;. One might wonder if any information gets lost during these conversions. From a purely mathematical point of view, the answer is no, but there is a catch. The above plots do not show the full output of the Fourier transform. In reality, the result of the Fourier transform includes &lt;em&gt;amplitude&lt;/em&gt; information (which is shown above) and &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Phase_%28waves%29&#34;&gt;phase&lt;/a&gt;&lt;/em&gt; information (which I omitted). Amplitude determines the levels of the constituent tones, while phase determines their &lt;em&gt;timing&lt;/em&gt;, or in other words, how they are offset from each other in time. I opted not to mention phase until now because it is rarely useful when discussing audio equipment. &lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [note] &lt;/strong&gt;&lt;span class=&#34;inlineFootnote inlineFootnote-note&#34;&gt;Audibility of &lt;a href=&#34;https://en.wikipedia.org/wiki/Phase_distortion&#34;&gt;phase distortion&lt;/a&gt; is a subject of &lt;a href=&#34;http://www.silcom.com/~aludwig/Phase_audibility.htm&#34;&gt;intense debate&lt;/a&gt;. That said, obsessing about phase is unlikely to be a good use of time considering that even a single acoustic reflection (either at recording or at playback) will severely disrupt the phase response anyway. A notable exception is where there is a phase difference between two &lt;em&gt;channels&lt;/em&gt; (e.g. left and right channels in a stereo system), in which case the effect is extremely audible — as any one who wired a speaker with the wrong polarity can attest.&lt;/span&gt;&lt;strong class=&#34;inlineFootnoteDecoration&#34;&gt; [end note] &lt;/strong&gt; As long as phase information is not discarded, it is possible to recover the original waveform, intact, simply by applying the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fourier_inversion_theorem&#34;&gt;inverse Fourier transform&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
